Hyperparameters are iterations: 10000 and learning rate 0
Iteration 0: Achieved Loss=> 812.8666666667
Gradients: w=>-806.8000000000, b=>-53.7333333333
Iteration 1: Achieved Loss=> 302.5769561564
Gradients: w=>-452.3830755556, b=>-33.1869333333
Iteration 2: Achieved Loss=> 141.9840903267
Gradients: w=>-253.5795060480, b=>-21.6601882193
Iteration 3: Achieved Loss=> 91.4213766211
Gradients: w=>-142.0643894332, b=>-15.1928536896
Iteration 4: Achieved Loss=> 75.4790576522
Gradients: w=>-79.5121269904, b=>-11.5635034499
Iteration 5: Achieved Loss=> 70.4298834852
Gradients: w=>-44.4246781320, b=>-9.5260692259
Iteration 6: Achieved Loss=> 68.8082102709
Gradients: w=>-24.7431118944, b=>-8.3815919081
Iteration 7: Achieved Loss=> 68.2650157314
Gradients: w=>-13.7031924677, b=>-7.7380032230
Iteration 8: Achieved Loss=> 68.0611933727
Gradients: w=>-7.5106460289, b=>-7.3753796740
Iteration 9: Achieved Loss=> 67.9641875162
Gradients: w=>-4.0371443749, b=>-7.1703592153
Iteration 10: Achieved Loss=> 67.9008258073
Gradients: w=>-2.0888408963, b=>-7.0537441727
Iteration 11: Achieved Loss=> 67.8480849450
Gradients: w=>-0.9960692677, b=>-6.9867193816
Iteration 12: Achieved Loss=> 67.7987207420
Gradients: w=>-0.3831924652, b=>-6.9475121881
Iteration 13: Achieved Loss=> 67.7504538190
Gradients: w=>-0.0395038003, b=>-6.9239096213
Iteration 14: Achieved Loss=> 67.7025669451
Gradients: w=>0.1531887731, b=>-6.9090610391
Iteration 15: Achieved Loss=> 67.6548344110
Gradients: w=>0.2611829123, b=>-6.8991236993
Iteration 16: Achieved Loss=> 67.6071851614
Gradients: w=>0.3216670703, b=>-6.8919420856
Iteration 17: Achieved Loss=> 67.5595968023
Gradients: w=>0.3555014265, b=>-6.8863071006
Iteration 18: Achieved Loss=> 67.5120622506
Gradients: w=>0.3743871155, b=>-6.8815405225
Iteration 19: Achieved Loss=> 67.4645792410
Gradients: w=>0.3848876736, b=>-6.8772619151
Iteration 20: Achieved Loss=> 67.4171470238
Gradients: w=>0.3906847962, b=>-6.8732578790
Iteration 21: Achieved Loss=> 67.3697653264
Gradients: w=>0.3938436623, b=>-6.8694087114
Iteration 22: Achieved Loss=> 67.3224340264
Gradients: w=>0.3955226964, b=>-6.8656472667
Iteration 23: Achieved Loss=> 67.2751530484
Gradients: w=>0.3963716952, b=>-6.8619358805
Iteration 24: Achieved Loss=> 67.2279223323
Gradients: w=>0.3967551503, b=>-6.8582534250
Iteration 25: Achieved Loss=> 67.1807418223
Gradients: w=>0.3968775167, b=>-6.8545880486
Iteration 26: Achieved Loss=> 67.1336114646
Gradients: w=>0.3968534793, b=>-6.8509331030
Iteration 27: Achieved Loss=> 67.0865312055
Gradients: w=>0.3967473687, b=>-6.8472848582
Iteration 28: Achieved Loss=> 67.0395009918
Gradients: w=>0.3965952699, b=>-6.8436412219
Iteration 29: Achieved Loss=> 66.9925207702
Gradients: w=>0.3964174241, b=>-6.8400010196
Iteration 30: Achieved Loss=> 66.9455904876
Gradients: w=>0.3962251851, b=>-6.8363635923
Iteration 31: Achieved Loss=> 66.8987100909
Gradients: w=>0.3960249218, b=>-6.8327285698
Iteration 32: Achieved Loss=> 66.8518795270
Gradients: w=>0.3958202064, b=>-6.8290957440
Iteration 33: Achieved Loss=> 66.8050987431
Gradients: w=>0.3956130430, b=>-6.8254649978
Iteration 34: Achieved Loss=> 66.7583676861
Gradients: w=>0.3954045553, b=>-6.8218362648
Iteration 35: Achieved Loss=> 66.7116863032
Gradients: w=>0.3951953739, b=>-6.8182095077
Iteration 36: Achieved Loss=> 66.6650545417
Gradients: w=>0.3949858525, b=>-6.8145847048
Iteration 37: Achieved Loss=> 66.6184723487
Gradients: w=>0.3947761893, b=>-6.8109618437
Iteration 38: Achieved Loss=> 66.5719396716
Gradients: w=>0.3945664956, b=>-6.8073409168
Iteration 39: Achieved Loss=> 66.5254564578
Gradients: w=>0.3943568337, b=>-6.8037219195
Iteration 40: Achieved Loss=> 66.4790226546
Gradients: w=>0.3941472385, b=>-6.8001048488
Iteration 41: Achieved Loss=> 66.4326382096
Gradients: w=>0.3939377298, b=>-6.7964897025
Iteration 42: Achieved Loss=> 66.3863030702
Gradients: w=>0.3937283184, b=>-6.7928764789
Iteration 43: Achieved Loss=> 66.3400171842
Gradients: w=>0.3935190104, b=>-6.7892651767
Iteration 44: Achieved Loss=> 66.2937804990
Gradients: w=>0.3933098092, b=>-6.7856557946
Iteration 45: Achieved Loss=> 66.2475929625
Gradients: w=>0.3931007169, b=>-6.7820483315
Iteration 46: Achieved Loss=> 66.2014545223
Gradients: w=>0.3928917342, b=>-6.7784427863
Iteration 47: Achieved Loss=> 66.1553651263
Gradients: w=>0.3926828619, b=>-6.7748391580
Iteration 48: Achieved Loss=> 66.1093247223
Gradients: w=>0.3924741002, b=>-6.7712374455
Iteration 49: Achieved Loss=> 66.0633332584
Gradients: w=>0.3922654492, b=>-6.7676376479
Iteration 50: Achieved Loss=> 66.0173906823
Gradients: w=>0.3920569091, b=>-6.7640397639
Iteration 51: Achieved Loss=> 65.9714969423
Gradients: w=>0.3918484797, b=>-6.7604437928
Iteration 52: Achieved Loss=> 65.9256519863
Gradients: w=>0.3916401611, b=>-6.7568497333
Iteration 53: Achieved Loss=> 65.8798557626
Gradients: w=>0.3914319532, b=>-6.7532575846
Iteration 54: Achieved Loss=> 65.8341082192
Gradients: w=>0.3912238559, b=>-6.7496673456
Iteration 55: Achieved Loss=> 65.7884093045
Gradients: w=>0.3910158693, b=>-6.7460790153
Iteration 56: Achieved Loss=> 65.7427589668
Gradients: w=>0.3908079933, b=>-6.7424925926
Iteration 57: Achieved Loss=> 65.6971571545
Gradients: w=>0.3906002278, b=>-6.7389080766
Iteration 58: Achieved Loss=> 65.6516038159
Gradients: w=>0.3903925727, b=>-6.7353254662
Iteration 59: Achieved Loss=> 65.6060988995
Gradients: w=>0.3901850280, b=>-6.7317447604
Iteration 60: Achieved Loss=> 65.5606423539
Gradients: w=>0.3899775937, b=>-6.7281659583
Iteration 61: Achieved Loss=> 65.5152341276
Gradients: w=>0.3897702696, b=>-6.7245890587
Iteration 62: Achieved Loss=> 65.4698741693
Gradients: w=>0.3895630558, b=>-6.7210140608
Iteration 63: Achieved Loss=> 65.4245624277
Gradients: w=>0.3893559521, b=>-6.7174409634
Iteration 64: Achieved Loss=> 65.3792988515
Gradients: w=>0.3891489585, b=>-6.7138697656
Iteration 65: Achieved Loss=> 65.3340833895
Gradients: w=>0.3889420750, b=>-6.7103004664
Iteration 66: Achieved Loss=> 65.2889159906
Gradients: w=>0.3887353015, b=>-6.7067330647
Iteration 67: Achieved Loss=> 65.2437966037
Gradients: w=>0.3885286379, b=>-6.7031675595
Iteration 68: Achieved Loss=> 65.1987251777
Gradients: w=>0.3883220841, b=>-6.6996039499
Iteration 69: Achieved Loss=> 65.1537016617
Gradients: w=>0.3881156402, b=>-6.6960422348
Iteration 70: Achieved Loss=> 65.1087260047
Gradients: w=>0.3879093060, b=>-6.6924824132
Iteration 71: Achieved Loss=> 65.0637981559
Gradients: w=>0.3877030815, b=>-6.6889244841
Iteration 72: Achieved Loss=> 65.0189180645
Gradients: w=>0.3874969666, b=>-6.6853684465
Iteration 73: Achieved Loss=> 64.9740856796
Gradients: w=>0.3872909613, b=>-6.6818142995
Iteration 74: Achieved Loss=> 64.9293009507
Gradients: w=>0.3870850656, b=>-6.6782620419
Iteration 75: Achieved Loss=> 64.8845638269
Gradients: w=>0.3868792793, b=>-6.6747116728
Iteration 76: Achieved Loss=> 64.8398742577
Gradients: w=>0.3866736024, b=>-6.6711631912
Iteration 77: Achieved Loss=> 64.7952321926
Gradients: w=>0.3864680348, b=>-6.6676165961
Iteration 78: Achieved Loss=> 64.7506375811
Gradients: w=>0.3862625765, b=>-6.6640718864
Iteration 79: Achieved Loss=> 64.7060903727
Gradients: w=>0.3860572275, b=>-6.6605290613
Iteration 80: Achieved Loss=> 64.6615905170
Gradients: w=>0.3858519876, b=>-6.6569881196
Iteration 81: Achieved Loss=> 64.6171379637
Gradients: w=>0.3856468569, b=>-6.6534490603
Iteration 82: Achieved Loss=> 64.5727326626
Gradients: w=>0.3854418352, b=>-6.6499118826
Iteration 83: Achieved Loss=> 64.5283745633
Gradients: w=>0.3852369225, b=>-6.6463765853
Iteration 84: Achieved Loss=> 64.4840636158
Gradients: w=>0.3850321187, b=>-6.6428431675
Iteration 85: Achieved Loss=> 64.4397997699
Gradients: w=>0.3848274238, b=>-6.6393116282
Iteration 86: Achieved Loss=> 64.3955829754
Gradients: w=>0.3846228377, b=>-6.6357819663
Iteration 87: Achieved Loss=> 64.3514131825
Gradients: w=>0.3844183604, b=>-6.6322541810
Iteration 88: Achieved Loss=> 64.3072903412
Gradients: w=>0.3842139918, b=>-6.6287282711
Iteration 89: Achieved Loss=> 64.2632144015
Gradients: w=>0.3840097319, b=>-6.6252042357
Iteration 90: Achieved Loss=> 64.2191853136
Gradients: w=>0.3838055805, b=>-6.6216820737
Iteration 91: Achieved Loss=> 64.1752030276
Gradients: w=>0.3836015377, b=>-6.6181617843
Iteration 92: Achieved Loss=> 64.1312674939
Gradients: w=>0.3833976033, b=>-6.6146433663
Iteration 93: Achieved Loss=> 64.0873786628
Gradients: w=>0.3831937774, b=>-6.6111268189
Iteration 94: Achieved Loss=> 64.0435364845
Gradients: w=>0.3829900598, b=>-6.6076121409
Iteration 95: Achieved Loss=> 63.9997409096
Gradients: w=>0.3827864505, b=>-6.6040993315
Iteration 96: Achieved Loss=> 63.9559918884
Gradients: w=>0.3825829495, b=>-6.6005883896
Iteration 97: Achieved Loss=> 63.9122893714
Gradients: w=>0.3823795567, b=>-6.5970793142
Iteration 98: Achieved Loss=> 63.8686333094
Gradients: w=>0.3821762720, b=>-6.5935721043
Iteration 99: Achieved Loss=> 63.8250236528
Gradients: w=>0.3819730953, b=>-6.5900667590
Iteration 100: Achieved Loss=> 63.7814603523
Gradients: w=>0.3817700267, b=>-6.5865632773
Iteration 101: Achieved Loss=> 63.7379433587
Gradients: w=>0.3815670660, b=>-6.5830616580
Iteration 102: Achieved Loss=> 63.6944726228
Gradients: w=>0.3813642133, b=>-6.5795619004
Iteration 103: Achieved Loss=> 63.6510480953
Gradients: w=>0.3811614684, b=>-6.5760640033
Iteration 104: Achieved Loss=> 63.6076697271
Gradients: w=>0.3809588312, b=>-6.5725679659
Iteration 105: Achieved Loss=> 63.5643374693
Gradients: w=>0.3807563018, b=>-6.5690737870
Iteration 106: Achieved Loss=> 63.5210512727
Gradients: w=>0.3805538801, b=>-6.5655814657
Iteration 107: Achieved Loss=> 63.4778110884
Gradients: w=>0.3803515660, b=>-6.5620910011
Iteration 108: Achieved Loss=> 63.4346168675
Gradients: w=>0.3801493594, b=>-6.5586023921
Iteration 109: Achieved Loss=> 63.3914685612
Gradients: w=>0.3799472603, b=>-6.5551156377
Iteration 110: Achieved Loss=> 63.3483661206
Gradients: w=>0.3797452687, b=>-6.5516307371
Iteration 111: Achieved Loss=> 63.3053094970
Gradients: w=>0.3795433845, b=>-6.5481476891
Iteration 112: Achieved Loss=> 63.2622986416
Gradients: w=>0.3793416075, b=>-6.5446664928
Iteration 113: Achieved Loss=> 63.2193335059
Gradients: w=>0.3791399379, b=>-6.5411871472
Iteration 114: Achieved Loss=> 63.1764140412
Gradients: w=>0.3789383755, b=>-6.5377096513
Iteration 115: Achieved Loss=> 63.1335401990
Gradients: w=>0.3787369202, b=>-6.5342340042
Iteration 116: Achieved Loss=> 63.0907119307
Gradients: w=>0.3785355720, b=>-6.5307602048
Iteration 117: Achieved Loss=> 63.0479291880
Gradients: w=>0.3783343309, b=>-6.5272882522
Iteration 118: Achieved Loss=> 63.0051919225
Gradients: w=>0.3781331968, b=>-6.5238181454
Iteration 119: Achieved Loss=> 62.9625000857
Gradients: w=>0.3779321695, b=>-6.5203498835
Iteration 120: Achieved Loss=> 62.9198536295
Gradients: w=>0.3777312492, b=>-6.5168834653
Iteration 121: Achieved Loss=> 62.8772525055
Gradients: w=>0.3775304357, b=>-6.5134188900
Iteration 122: Achieved Loss=> 62.8346966657
Gradients: w=>0.3773297289, b=>-6.5099561566
Iteration 123: Achieved Loss=> 62.7921860618
Gradients: w=>0.3771291288, b=>-6.5064952641
Iteration 124: Achieved Loss=> 62.7497206457
Gradients: w=>0.3769286354, b=>-6.5030362115
Iteration 125: Achieved Loss=> 62.7073003695
Gradients: w=>0.3767282486, b=>-6.4995789979
Iteration 126: Achieved Loss=> 62.6649251852
Gradients: w=>0.3765279683, b=>-6.4961236222
Iteration 127: Achieved Loss=> 62.6225950447
Gradients: w=>0.3763277945, b=>-6.4926700834
Iteration 128: Achieved Loss=> 62.5803099003
Gradients: w=>0.3761277270, b=>-6.4892183807
Iteration 129: Achieved Loss=> 62.5380697042
Gradients: w=>0.3759277660, b=>-6.4857685131
Iteration 130: Achieved Loss=> 62.4958744084
Gradients: w=>0.3757279113, b=>-6.4823204794
Iteration 131: Achieved Loss=> 62.4537239654
Gradients: w=>0.3755281628, b=>-6.4788742789
Iteration 132: Achieved Loss=> 62.4116183273
Gradients: w=>0.3753285205, b=>-6.4754299105
Iteration 133: Achieved Loss=> 62.3695574467
Gradients: w=>0.3751289843, b=>-6.4719873732
Iteration 134: Achieved Loss=> 62.3275412759
Gradients: w=>0.3749295542, b=>-6.4685466660
Iteration 135: Achieved Loss=> 62.2855697674
Gradients: w=>0.3747302302, b=>-6.4651077881
Iteration 136: Achieved Loss=> 62.2436428737
Gradients: w=>0.3745310121, b=>-6.4616707383
Iteration 137: Achieved Loss=> 62.2017605474
Gradients: w=>0.3743318999, b=>-6.4582355158
Iteration 138: Achieved Loss=> 62.1599227412
Gradients: w=>0.3741328936, b=>-6.4548021196
Iteration 139: Achieved Loss=> 62.1181294076
Gradients: w=>0.3739339930, b=>-6.4513705486
Iteration 140: Achieved Loss=> 62.0763804995
Gradients: w=>0.3737351983, b=>-6.4479408020
Iteration 141: Achieved Loss=> 62.0346759695
Gradients: w=>0.3735365091, b=>-6.4445128788
Iteration 142: Achieved Loss=> 61.9930157706
Gradients: w=>0.3733379257, b=>-6.4410867779
Iteration 143: Achieved Loss=> 61.9513998556
Gradients: w=>0.3731394478, b=>-6.4376624985
Iteration 144: Achieved Loss=> 61.9098281775
Gradients: w=>0.3729410754, b=>-6.4342400395
Iteration 145: Achieved Loss=> 61.8683006891
Gradients: w=>0.3727428085, b=>-6.4308194000
Iteration 146: Achieved Loss=> 61.8268173436
Gradients: w=>0.3725446469, b=>-6.4274005790
Iteration 147: Achieved Loss=> 61.7853780940
Gradients: w=>0.3723465908, b=>-6.4239835756
Iteration 148: Achieved Loss=> 61.7439828935
Gradients: w=>0.3721486399, b=>-6.4205683887
Iteration 149: Achieved Loss=> 61.7026316951
Gradients: w=>0.3719507943, b=>-6.4171550175
Iteration 150: Achieved Loss=> 61.6613244522
Gradients: w=>0.3717530538, b=>-6.4137434609
Iteration 151: Achieved Loss=> 61.6200611181
Gradients: w=>0.3715554185, b=>-6.4103337180
Iteration 152: Achieved Loss=> 61.5788416459
Gradients: w=>0.3713578882, b=>-6.4069257879
Iteration 153: Achieved Loss=> 61.5376659892
Gradients: w=>0.3711604630, b=>-6.4035196694
Iteration 154: Achieved Loss=> 61.4965341014
Gradients: w=>0.3709631427, b=>-6.4001153618
Iteration 155: Achieved Loss=> 61.4554459358
Gradients: w=>0.3707659273, b=>-6.3967128641
Iteration 156: Achieved Loss=> 61.4144014462
Gradients: w=>0.3705688167, b=>-6.3933121752
Iteration 157: Achieved Loss=> 61.3734005859
Gradients: w=>0.3703718110, b=>-6.3899132942
Iteration 158: Achieved Loss=> 61.3324433087
Gradients: w=>0.3701749099, b=>-6.3865162201
Iteration 159: Achieved Loss=> 61.2915295682
Gradients: w=>0.3699781136, b=>-6.3831209521
Iteration 160: Achieved Loss=> 61.2506593181
Gradients: w=>0.3697814219, b=>-6.3797274890
Iteration 161: Achieved Loss=> 61.2098325123
Gradients: w=>0.3695848347, b=>-6.3763358301
Iteration 162: Achieved Loss=> 61.1690491044
Gradients: w=>0.3693883521, b=>-6.3729459742
Iteration 163: Achieved Loss=> 61.1283090485
Gradients: w=>0.3691919739, b=>-6.3695579205
Iteration 164: Achieved Loss=> 61.0876122984
Gradients: w=>0.3689957001, b=>-6.3661716680
Iteration 165: Achieved Loss=> 61.0469588080
Gradients: w=>0.3687995307, b=>-6.3627872158
Iteration 166: Achieved Loss=> 61.0063485315
Gradients: w=>0.3686034655, b=>-6.3594045628
Iteration 167: Achieved Loss=> 60.9657814228
Gradients: w=>0.3684075046, b=>-6.3560237081
Iteration 168: Achieved Loss=> 60.9252574361
Gradients: w=>0.3682116479, b=>-6.3526446508
Iteration 169: Achieved Loss=> 60.8847765255
Gradients: w=>0.3680158952, b=>-6.3492673899
Iteration 170: Achieved Loss=> 60.8443386453
Gradients: w=>0.3678202467, b=>-6.3458919245
Iteration 171: Achieved Loss=> 60.8039437497
Gradients: w=>0.3676247022, b=>-6.3425182536
Iteration 172: Achieved Loss=> 60.7635917930
Gradients: w=>0.3674292616, b=>-6.3391463762
Iteration 173: Achieved Loss=> 60.7232827296
Gradients: w=>0.3672339249, b=>-6.3357762914
Iteration 174: Achieved Loss=> 60.6830165138
Gradients: w=>0.3670386921, b=>-6.3324079982
Iteration 175: Achieved Loss=> 60.6427931002
Gradients: w=>0.3668435631, b=>-6.3290414958
Iteration 176: Achieved Loss=> 60.6026124432
Gradients: w=>0.3666485378, b=>-6.3256767830
Iteration 177: Achieved Loss=> 60.5624744975
Gradients: w=>0.3664536161, b=>-6.3223138591
Iteration 178: Achieved Loss=> 60.5223792174
Gradients: w=>0.3662587981, b=>-6.3189527230
Iteration 179: Achieved Loss=> 60.4823265579
Gradients: w=>0.3660640837, b=>-6.3155933738
Iteration 180: Achieved Loss=> 60.4423164734
Gradients: w=>0.3658694728, b=>-6.3122358105
Iteration 181: Achieved Loss=> 60.4023489188
Gradients: w=>0.3656749654, b=>-6.3088800322
Iteration 182: Achieved Loss=> 60.3624238489
Gradients: w=>0.3654805613, b=>-6.3055260379
Iteration 183: Achieved Loss=> 60.3225412185
Gradients: w=>0.3652862607, b=>-6.3021738267
Iteration 184: Achieved Loss=> 60.2827009825
Gradients: w=>0.3650920633, b=>-6.2988233976
Iteration 185: Achieved Loss=> 60.2429030957
Gradients: w=>0.3648979691, b=>-6.2954747498
Iteration 186: Achieved Loss=> 60.2031475133
Gradients: w=>0.3647039782, b=>-6.2921278822
Iteration 187: Achieved Loss=> 60.1634341903
Gradients: w=>0.3645100903, b=>-6.2887827938
Iteration 188: Achieved Loss=> 60.1237630816
Gradients: w=>0.3643163056, b=>-6.2854394839
Iteration 189: Achieved Loss=> 60.0841341425
Gradients: w=>0.3641226239, b=>-6.2820979513
Iteration 190: Achieved Loss=> 60.0445473282
Gradients: w=>0.3639290451, b=>-6.2787581952
Iteration 191: Achieved Loss=> 60.0050025937
Gradients: w=>0.3637355692, b=>-6.2754202146
Iteration 192: Achieved Loss=> 59.9654998945
Gradients: w=>0.3635421962, b=>-6.2720840086
Iteration 193: Achieved Loss=> 59.9260391858
Gradients: w=>0.3633489261, b=>-6.2687495763
Iteration 194: Achieved Loss=> 59.8866204230
Gradients: w=>0.3631557586, b=>-6.2654169166
Iteration 195: Achieved Loss=> 59.8472435616
Gradients: w=>0.3629626939, b=>-6.2620860286
Iteration 196: Achieved Loss=> 59.8079085568
Gradients: w=>0.3627697318, b=>-6.2587569115
Iteration 197: Achieved Loss=> 59.7686153644
Gradients: w=>0.3625768722, b=>-6.2554295642
Iteration 198: Achieved Loss=> 59.7293639398
Gradients: w=>0.3623841152, b=>-6.2521039858
Iteration 199: Achieved Loss=> 59.6901542386
Gradients: w=>0.3621914607, b=>-6.2487801754
Iteration 200: Achieved Loss=> 59.6509862165
Gradients: w=>0.3619989086, b=>-6.2454581321
Iteration 201: Achieved Loss=> 59.6118598292
Gradients: w=>0.3618064589, b=>-6.2421378548
Iteration 202: Achieved Loss=> 59.5727750323
Gradients: w=>0.3616141115, b=>-6.2388193428
Iteration 203: Achieved Loss=> 59.5337317818
Gradients: w=>0.3614218663, b=>-6.2355025949
Iteration 204: Achieved Loss=> 59.4947300335
Gradients: w=>0.3612297233, b=>-6.2321876103
Iteration 205: Achieved Loss=> 59.4557697432
Gradients: w=>0.3610376825, b=>-6.2288743881
Iteration 206: Achieved Loss=> 59.4168508668
Gradients: w=>0.3608457438, b=>-6.2255629273
Iteration 207: Achieved Loss=> 59.3779733604
Gradients: w=>0.3606539072, b=>-6.2222532269
Iteration 208: Achieved Loss=> 59.3391371799
Gradients: w=>0.3604621725, b=>-6.2189452861
Iteration 209: Achieved Loss=> 59.3003422815
Gradients: w=>0.3602705397, b=>-6.2156391039
Iteration 210: Achieved Loss=> 59.2615886212
Gradients: w=>0.3600790088, b=>-6.2123346794
Iteration 211: Achieved Loss=> 59.2228761552
Gradients: w=>0.3598875798, b=>-6.2090320116
Iteration 212: Achieved Loss=> 59.1842048398
Gradients: w=>0.3596962525, b=>-6.2057310996
Iteration 213: Achieved Loss=> 59.1455746312
Gradients: w=>0.3595050269, b=>-6.2024319424
Iteration 214: Achieved Loss=> 59.1069854856
Gradients: w=>0.3593139030, b=>-6.1991345392
Iteration 215: Achieved Loss=> 59.0684373595
Gradients: w=>0.3591228807, b=>-6.1958388890
Iteration 216: Achieved Loss=> 59.0299302092
Gradients: w=>0.3589319600, b=>-6.1925449909
Iteration 217: Achieved Loss=> 58.9914639913
Gradients: w=>0.3587411407, b=>-6.1892528439
Iteration 218: Achieved Loss=> 58.9530386620
Gradients: w=>0.3585504229, b=>-6.1859624471
Iteration 219: Achieved Loss=> 58.9146541781
Gradients: w=>0.3583598065, b=>-6.1826737996
Iteration 220: Achieved Loss=> 58.8763104961
Gradients: w=>0.3581692914, b=>-6.1793869004
Iteration 221: Achieved Loss=> 58.8380075726
Gradients: w=>0.3579788776, b=>-6.1761017487
Iteration 222: Achieved Loss=> 58.7997453643
Gradients: w=>0.3577885651, b=>-6.1728183434
Iteration 223: Achieved Loss=> 58.7615238279
Gradients: w=>0.3575983537, b=>-6.1695366837
Iteration 224: Achieved Loss=> 58.7233429202
Gradients: w=>0.3574082434, b=>-6.1662567686
Iteration 225: Achieved Loss=> 58.6852025980
Gradients: w=>0.3572182343, b=>-6.1629785973
Iteration 226: Achieved Loss=> 58.6471028182
Gradients: w=>0.3570283261, b=>-6.1597021687
Iteration 227: Achieved Loss=> 58.6090435376
Gradients: w=>0.3568385189, b=>-6.1564274819
Iteration 228: Achieved Loss=> 58.5710247132
Gradients: w=>0.3566488125, b=>-6.1531545361
Iteration 229: Achieved Loss=> 58.5330463020
Gradients: w=>0.3564592071, b=>-6.1498833303
Iteration 230: Achieved Loss=> 58.4951082610
Gradients: w=>0.3562697024, b=>-6.1466138635
Iteration 231: Achieved Loss=> 58.4572105474
Gradients: w=>0.3560802985, b=>-6.1433461350
Iteration 232: Achieved Loss=> 58.4193531183
Gradients: w=>0.3558909953, b=>-6.1400801436
Iteration 233: Achieved Loss=> 58.3815359308
Gradients: w=>0.3557017927, b=>-6.1368158885
Iteration 234: Achieved Loss=> 58.3437589421
Gradients: w=>0.3555126907, b=>-6.1335533688
Iteration 235: Achieved Loss=> 58.3060221096
Gradients: w=>0.3553236893, b=>-6.1302925836
Iteration 236: Achieved Loss=> 58.2683253906
Gradients: w=>0.3551347883, b=>-6.1270335319
Iteration 237: Achieved Loss=> 58.2306687423
Gradients: w=>0.3549459877, b=>-6.1237762128
Iteration 238: Achieved Loss=> 58.1930521223
Gradients: w=>0.3547572876, b=>-6.1205206254
Iteration 239: Achieved Loss=> 58.1554754879
Gradients: w=>0.3545686877, b=>-6.1172667687
Iteration 240: Achieved Loss=> 58.1179387967
Gradients: w=>0.3543801881, b=>-6.1140146420
Iteration 241: Achieved Loss=> 58.0804420063
Gradients: w=>0.3541917887, b=>-6.1107642441
Iteration 242: Achieved Loss=> 58.0429850741
Gradients: w=>0.3540034895, b=>-6.1075155743
Iteration 243: Achieved Loss=> 58.0055679578
Gradients: w=>0.3538152904, b=>-6.1042686315
Iteration 244: Achieved Loss=> 57.9681906151
Gradients: w=>0.3536271913, b=>-6.1010234149
Iteration 245: Achieved Loss=> 57.9308530038
Gradients: w=>0.3534391922, b=>-6.0977799236
Iteration 246: Achieved Loss=> 57.8935550815
Gradients: w=>0.3532512931, b=>-6.0945381566
Iteration 247: Achieved Loss=> 57.8562968061
Gradients: w=>0.3530634939, b=>-6.0912981131
Iteration 248: Achieved Loss=> 57.8190781355
Gradients: w=>0.3528757945, b=>-6.0880597920
Iteration 249: Achieved Loss=> 57.7818990276
Gradients: w=>0.3526881949, b=>-6.0848231926
Iteration 250: Achieved Loss=> 57.7447594402
Gradients: w=>0.3525006950, b=>-6.0815883138
Iteration 251: Achieved Loss=> 57.7076593315
Gradients: w=>0.3523132948, b=>-6.0783551548
Iteration 252: Achieved Loss=> 57.6705986593
Gradients: w=>0.3521259943, b=>-6.0751237146
Iteration 253: Achieved Loss=> 57.6335773819
Gradients: w=>0.3519387933, b=>-6.0718939924
Iteration 254: Achieved Loss=> 57.5965954573
Gradients: w=>0.3517516918, b=>-6.0686659871
Iteration 255: Achieved Loss=> 57.5596528437
Gradients: w=>0.3515646898, b=>-6.0654396980
Iteration 256: Achieved Loss=> 57.5227494993
Gradients: w=>0.3513777872, b=>-6.0622151241
Iteration 257: Achieved Loss=> 57.4858853824
Gradients: w=>0.3511909840, b=>-6.0589922645
Iteration 258: Achieved Loss=> 57.4490604513
Gradients: w=>0.3510042801, b=>-6.0557711182
Iteration 259: Achieved Loss=> 57.4122746643
Gradients: w=>0.3508176755, b=>-6.0525516844
Iteration 260: Achieved Loss=> 57.3755279798
Gradients: w=>0.3506311700, b=>-6.0493339621
Iteration 261: Achieved Loss=> 57.3388203562
Gradients: w=>0.3504447637, b=>-6.0461179505
Iteration 262: Achieved Loss=> 57.3021517521
Gradients: w=>0.3502584565, b=>-6.0429036486
Iteration 263: Achieved Loss=> 57.2655221259
Gradients: w=>0.3500722484, b=>-6.0396910556
Iteration 264: Achieved Loss=> 57.2289314362
Gradients: w=>0.3498861392, b=>-6.0364801704
Iteration 265: Achieved Loss=> 57.1923796417
Gradients: w=>0.3497001290, b=>-6.0332709923
Iteration 266: Achieved Loss=> 57.1558667009
Gradients: w=>0.3495142177, b=>-6.0300635202
Iteration 267: Achieved Loss=> 57.1193925726
Gradients: w=>0.3493284052, b=>-6.0268577534
Iteration 268: Achieved Loss=> 57.0829572156
Gradients: w=>0.3491426915, b=>-6.0236536908
Iteration 269: Achieved Loss=> 57.0465605885
Gradients: w=>0.3489570765, b=>-6.0204513316
Iteration 270: Achieved Loss=> 57.0102026503
Gradients: w=>0.3487715602, b=>-6.0172506749
Iteration 271: Achieved Loss=> 56.9738833598
Gradients: w=>0.3485861426, b=>-6.0140517197
Iteration 272: Achieved Loss=> 56.9376026760
Gradients: w=>0.3484008235, b=>-6.0108544652
Iteration 273: Achieved Loss=> 56.9013605577
Gradients: w=>0.3482156029, b=>-6.0076589105
Iteration 274: Achieved Loss=> 56.8651569640
Gradients: w=>0.3480304808, b=>-6.0044650546
Iteration 275: Achieved Loss=> 56.8289918540
Gradients: w=>0.3478454571, b=>-6.0012728967
Iteration 276: Achieved Loss=> 56.7928651868
Gradients: w=>0.3476605318, b=>-5.9980824358
Iteration 277: Achieved Loss=> 56.7567769214
Gradients: w=>0.3474757048, b=>-5.9948936711
Iteration 278: Achieved Loss=> 56.7207270170
Gradients: w=>0.3472909760, b=>-5.9917066016
Iteration 279: Achieved Loss=> 56.6847154330
Gradients: w=>0.3471063455, b=>-5.9885212264
Iteration 280: Achieved Loss=> 56.6487421285
Gradients: w=>0.3469218131, b=>-5.9853375447
Iteration 281: Achieved Loss=> 56.6128070629
Gradients: w=>0.3467373788, b=>-5.9821555556
Iteration 282: Achieved Loss=> 56.5769101954
Gradients: w=>0.3465530426, b=>-5.9789752581
Iteration 283: Achieved Loss=> 56.5410514856
Gradients: w=>0.3463688044, b=>-5.9757966513
Iteration 284: Achieved Loss=> 56.5052308929
Gradients: w=>0.3461846641, b=>-5.9726197344
Iteration 285: Achieved Loss=> 56.4694483766
Gradients: w=>0.3460006217, b=>-5.9694445064
Iteration 286: Achieved Loss=> 56.4337038964
Gradients: w=>0.3458166771, b=>-5.9662709665
Iteration 287: Achieved Loss=> 56.3979974119
Gradients: w=>0.3456328304, b=>-5.9630991137
Iteration 288: Achieved Loss=> 56.3623288825
Gradients: w=>0.3454490813, b=>-5.9599289472
Iteration 289: Achieved Loss=> 56.3266982681
Gradients: w=>0.3452654300, b=>-5.9567604660
Iteration 290: Achieved Loss=> 56.2911055282
Gradients: w=>0.3450818763, b=>-5.9535936693
Iteration 291: Achieved Loss=> 56.2555506227
Gradients: w=>0.3448984202, b=>-5.9504285561
Iteration 292: Achieved Loss=> 56.2200335113
Gradients: w=>0.3447150616, b=>-5.9472651257
Iteration 293: Achieved Loss=> 56.1845541538
Gradients: w=>0.3445318005, b=>-5.9441033770
Iteration 294: Achieved Loss=> 56.1491125101
Gradients: w=>0.3443486368, b=>-5.9409433092
Iteration 295: Achieved Loss=> 56.1137085402
Gradients: w=>0.3441655705, b=>-5.9377849214
Iteration 296: Achieved Loss=> 56.0783422039
Gradients: w=>0.3439826015, b=>-5.9346282126
Iteration 297: Achieved Loss=> 56.0430134612
Gradients: w=>0.3437997298, b=>-5.9314731821
Iteration 298: Achieved Loss=> 56.0077222723
Gradients: w=>0.3436169553, b=>-5.9283198289
Iteration 299: Achieved Loss=> 55.9724685972
Gradients: w=>0.3434342780, b=>-5.9251681521
Iteration 300: Achieved Loss=> 55.9372523959
Gradients: w=>0.3432516978, b=>-5.9220181509
Iteration 301: Achieved Loss=> 55.9020736288
Gradients: w=>0.3430692147, b=>-5.9188698242
Iteration 302: Achieved Loss=> 55.8669322559
Gradients: w=>0.3428868285, b=>-5.9157231714
Iteration 303: Achieved Loss=> 55.8318282375
Gradients: w=>0.3427045394, b=>-5.9125781913
Iteration 304: Achieved Loss=> 55.7967615340
Gradients: w=>0.3425223471, b=>-5.9094348833
Iteration 305: Achieved Loss=> 55.7617321056
Gradients: w=>0.3423402517, b=>-5.9062932463
Iteration 306: Achieved Loss=> 55.7267399128
Gradients: w=>0.3421582531, b=>-5.9031532795
Iteration 307: Achieved Loss=> 55.6917849159
Gradients: w=>0.3419763513, b=>-5.9000149820
Iteration 308: Achieved Loss=> 55.6568670755
Gradients: w=>0.3417945462, b=>-5.8968783530
Iteration 309: Achieved Loss=> 55.6219863520
Gradients: w=>0.3416128377, b=>-5.8937433914
Iteration 310: Achieved Loss=> 55.5871427059
Gradients: w=>0.3414312259, b=>-5.8906100966
Iteration 311: Achieved Loss=> 55.5523360979
Gradients: w=>0.3412497105, b=>-5.8874784674
Iteration 312: Achieved Loss=> 55.5175664886
Gradients: w=>0.3410682917, b=>-5.8843485031
Iteration 313: Achieved Loss=> 55.4828338386
Gradients: w=>0.3408869693, b=>-5.8812202029
Iteration 314: Achieved Loss=> 55.4481381087
Gradients: w=>0.3407057434, b=>-5.8780935657
Iteration 315: Achieved Loss=> 55.4134792597
Gradients: w=>0.3405246137, b=>-5.8749685907
Iteration 316: Achieved Loss=> 55.3788572522
Gradients: w=>0.3403435804, b=>-5.8718452771
Iteration 317: Achieved Loss=> 55.3442720472
Gradients: w=>0.3401626433, b=>-5.8687236239
Iteration 318: Achieved Loss=> 55.3097236056
Gradients: w=>0.3399818024, b=>-5.8656036303
Iteration 319: Achieved Loss=> 55.2752118882
Gradients: w=>0.3398010576, b=>-5.8624852953
Iteration 320: Achieved Loss=> 55.2407368560
Gradients: w=>0.3396204090, b=>-5.8593686182
Iteration 321: Achieved Loss=> 55.2062984701
Gradients: w=>0.3394398563, b=>-5.8562535980
Iteration 322: Achieved Loss=> 55.1718966914
Gradients: w=>0.3392593997, b=>-5.8531402338
Iteration 323: Achieved Loss=> 55.1375314811
Gradients: w=>0.3390790390, b=>-5.8500285248
Iteration 324: Achieved Loss=> 55.1032028003
Gradients: w=>0.3388987741, b=>-5.8469184701
Iteration 325: Achieved Loss=> 55.0689106101
Gradients: w=>0.3387186052, b=>-5.8438100688
Iteration 326: Achieved Loss=> 55.0346548718
Gradients: w=>0.3385385319, b=>-5.8407033200
Iteration 327: Achieved Loss=> 55.0004355467
Gradients: w=>0.3383585545, b=>-5.8375982228
Iteration 328: Achieved Loss=> 54.9662525959
Gradients: w=>0.3381786727, b=>-5.8344947764
Iteration 329: Achieved Loss=> 54.9321059809
Gradients: w=>0.3379988865, b=>-5.8313929799
Iteration 330: Achieved Loss=> 54.8979956630
Gradients: w=>0.3378191959, b=>-5.8282928324
Iteration 331: Achieved Loss=> 54.8639216037
Gradients: w=>0.3376396009, b=>-5.8251943330
Iteration 332: Achieved Loss=> 54.8298837644
Gradients: w=>0.3374601013, b=>-5.8220974809
Iteration 333: Achieved Loss=> 54.7958821065
Gradients: w=>0.3372806972, b=>-5.8190022752
Iteration 334: Achieved Loss=> 54.7619165918
Gradients: w=>0.3371013884, b=>-5.8159087150
Iteration 335: Achieved Loss=> 54.7279871816
Gradients: w=>0.3369221749, b=>-5.8128167994
Iteration 336: Achieved Loss=> 54.6940938377
Gradients: w=>0.3367430568, b=>-5.8097265275
Iteration 337: Achieved Loss=> 54.6602365216
Gradients: w=>0.3365640338, b=>-5.8066378986
Iteration 338: Achieved Loss=> 54.6264151952
Gradients: w=>0.3363851061, b=>-5.8035509116
Iteration 339: Achieved Loss=> 54.5926298202
Gradients: w=>0.3362062734, b=>-5.8004655658
Iteration 340: Achieved Loss=> 54.5588803582
Gradients: w=>0.3360275358, b=>-5.7973818603
Iteration 341: Achieved Loss=> 54.5251667713
Gradients: w=>0.3358488933, b=>-5.7942997942
Iteration 342: Achieved Loss=> 54.4914890211
Gradients: w=>0.3356703457, b=>-5.7912193665
Iteration 343: Achieved Loss=> 54.4578470697
Gradients: w=>0.3354918930, b=>-5.7881405766
Iteration 344: Achieved Loss=> 54.4242408790
Gradients: w=>0.3353135353, b=>-5.7850634234
Iteration 345: Achieved Loss=> 54.3906704109
Gradients: w=>0.3351352723, b=>-5.7819879061
Iteration 346: Achieved Loss=> 54.3571356276
Gradients: w=>0.3349571041, b=>-5.7789140238
Iteration 347: Achieved Loss=> 54.3236364910
Gradients: w=>0.3347790306, b=>-5.7758417757
Iteration 348: Achieved Loss=> 54.2901729632
Gradients: w=>0.3346010518, b=>-5.7727711610
Iteration 349: Achieved Loss=> 54.2567450065
Gradients: w=>0.3344231676, b=>-5.7697021786
Iteration 350: Achieved Loss=> 54.2233525830
Gradients: w=>0.3342453780, b=>-5.7666348279
Iteration 351: Achieved Loss=> 54.1899956549
Gradients: w=>0.3340676829, b=>-5.7635691078
Iteration 352: Achieved Loss=> 54.1566741846
Gradients: w=>0.3338900823, b=>-5.7605050175
Iteration 353: Achieved Loss=> 54.1233881342
Gradients: w=>0.3337125760, b=>-5.7574425562
Iteration 354: Achieved Loss=> 54.0901374662
Gradients: w=>0.3335351642, b=>-5.7543817231
Iteration 355: Achieved Loss=> 54.0569221430
Gradients: w=>0.3333578467, b=>-5.7513225171
Iteration 356: Achieved Loss=> 54.0237421270
Gradients: w=>0.3331806234, b=>-5.7482649375
Iteration 357: Achieved Loss=> 53.9905973806
Gradients: w=>0.3330034943, b=>-5.7452089834
Iteration 358: Achieved Loss=> 53.9574878663
Gradients: w=>0.3328264595, b=>-5.7421546540
Iteration 359: Achieved Loss=> 53.9244135468
Gradients: w=>0.3326495187, b=>-5.7391019483
Iteration 360: Achieved Loss=> 53.8913743846
Gradients: w=>0.3324726720, b=>-5.7360508656
Iteration 361: Achieved Loss=> 53.8583703423
Gradients: w=>0.3322959193, b=>-5.7330014049
Iteration 362: Achieved Loss=> 53.8254013826
Gradients: w=>0.3321192606, b=>-5.7299535653
Iteration 363: Achieved Loss=> 53.7924674682
Gradients: w=>0.3319426958, b=>-5.7269073461
Iteration 364: Achieved Loss=> 53.7595685618
Gradients: w=>0.3317662249, b=>-5.7238627464
Iteration 365: Achieved Loss=> 53.7267046263
Gradients: w=>0.3315898478, b=>-5.7208197653
Iteration 366: Achieved Loss=> 53.6938756245
Gradients: w=>0.3314135645, b=>-5.7177784019
Iteration 367: Achieved Loss=> 53.6610815192
Gradients: w=>0.3312373748, b=>-5.7147386554
Iteration 368: Achieved Loss=> 53.6283222733
Gradients: w=>0.3310612789, b=>-5.7117005249
Iteration 369: Achieved Loss=> 53.5955978498
Gradients: w=>0.3308852765, b=>-5.7086640096
Iteration 370: Achieved Loss=> 53.5629082117
Gradients: w=>0.3307093678, b=>-5.7056291086
Iteration 371: Achieved Loss=> 53.5302533220
Gradients: w=>0.3305335525, b=>-5.7025958210
Iteration 372: Achieved Loss=> 53.4976331437
Gradients: w=>0.3303578307, b=>-5.6995641460
Iteration 373: Achieved Loss=> 53.4650476400
Gradients: w=>0.3301822024, b=>-5.6965340828
Iteration 374: Achieved Loss=> 53.4324967739
Gradients: w=>0.3300066674, b=>-5.6935056304
Iteration 375: Achieved Loss=> 53.3999805088
Gradients: w=>0.3298312257, b=>-5.6904787881
Iteration 376: Achieved Loss=> 53.3674988078
Gradients: w=>0.3296558773, b=>-5.6874535549
Iteration 377: Achieved Loss=> 53.3350516341
Gradients: w=>0.3294806221, b=>-5.6844299300
Iteration 378: Achieved Loss=> 53.3026389510
Gradients: w=>0.3293054601, b=>-5.6814079125
Iteration 379: Achieved Loss=> 53.2702607220
Gradients: w=>0.3291303912, b=>-5.6783875017
Iteration 380: Achieved Loss=> 53.2379169103
Gradients: w=>0.3289554154, b=>-5.6753686966
Iteration 381: Achieved Loss=> 53.2056074795
Gradients: w=>0.3287805326, b=>-5.6723514964
Iteration 382: Achieved Loss=> 53.1733323928
Gradients: w=>0.3286057428, b=>-5.6693359003
Iteration 383: Achieved Loss=> 53.1410916139
Gradients: w=>0.3284310459, b=>-5.6663219073
Iteration 384: Achieved Loss=> 53.1088851063
Gradients: w=>0.3282564419, b=>-5.6633095166
Iteration 385: Achieved Loss=> 53.0767128334
Gradients: w=>0.3280819307, b=>-5.6602987274
Iteration 386: Achieved Loss=> 53.0445747591
Gradients: w=>0.3279075122, b=>-5.6572895389
Iteration 387: Achieved Loss=> 53.0124708468
Gradients: w=>0.3277331865, b=>-5.6542819501
Iteration 388: Achieved Loss=> 52.9804010603
Gradients: w=>0.3275589535, b=>-5.6512759603
Iteration 389: Achieved Loss=> 52.9483653633
Gradients: w=>0.3273848131, b=>-5.6482715685
Iteration 390: Achieved Loss=> 52.9163637195
Gradients: w=>0.3272107653, b=>-5.6452687740
Iteration 391: Achieved Loss=> 52.8843960929
Gradients: w=>0.3270368100, b=>-5.6422675758
Iteration 392: Achieved Loss=> 52.8524624471
Gradients: w=>0.3268629472, b=>-5.6392679732
Iteration 393: Achieved Loss=> 52.8205627461
Gradients: w=>0.3266891768, b=>-5.6362699652
Iteration 394: Achieved Loss=> 52.7886969539
Gradients: w=>0.3265154988, b=>-5.6332735511
Iteration 395: Achieved Loss=> 52.7568650342
Gradients: w=>0.3263419132, b=>-5.6302787300
Iteration 396: Achieved Loss=> 52.7250669513
Gradients: w=>0.3261684198, b=>-5.6272855010
Iteration 397: Achieved Loss=> 52.6933026690
Gradients: w=>0.3259950187, b=>-5.6242938633
Iteration 398: Achieved Loss=> 52.6615721515
Gradients: w=>0.3258217097, b=>-5.6213038160
Iteration 399: Achieved Loss=> 52.6298753629
Gradients: w=>0.3256484929, b=>-5.6183153584
Iteration 400: Achieved Loss=> 52.5982122672
Gradients: w=>0.3254753681, b=>-5.6153284895
Iteration 401: Achieved Loss=> 52.5665828288
Gradients: w=>0.3253023354, b=>-5.6123432085
Iteration 402: Achieved Loss=> 52.5349870118
Gradients: w=>0.3251293947, b=>-5.6093595146
Iteration 403: Achieved Loss=> 52.5034247805
Gradients: w=>0.3249565460, b=>-5.6063774069
Iteration 404: Achieved Loss=> 52.4718960991
Gradients: w=>0.3247837891, b=>-5.6033968846
Iteration 405: Achieved Loss=> 52.4404009321
Gradients: w=>0.3246111241, b=>-5.6004179468
Iteration 406: Achieved Loss=> 52.4089392437
Gradients: w=>0.3244385508, b=>-5.5974405927
Iteration 407: Achieved Loss=> 52.3775109985
Gradients: w=>0.3242660693, b=>-5.5944648215
Iteration 408: Achieved Loss=> 52.3461161608
Gradients: w=>0.3240936795, b=>-5.5914906323
Iteration 409: Achieved Loss=> 52.3147546952
Gradients: w=>0.3239213814, b=>-5.5885180242
Iteration 410: Achieved Loss=> 52.2834265661
Gradients: w=>0.3237491748, b=>-5.5855469965
Iteration 411: Achieved Loss=> 52.2521317382
Gradients: w=>0.3235770598, b=>-5.5825775483
Iteration 412: Achieved Loss=> 52.2208701760
Gradients: w=>0.3234050363, b=>-5.5796096787
Iteration 413: Achieved Loss=> 52.1896418442
Gradients: w=>0.3232331043, b=>-5.5766433869
Iteration 414: Achieved Loss=> 52.1584467075
Gradients: w=>0.3230612637, b=>-5.5736786721
Iteration 415: Achieved Loss=> 52.1272847305
Gradients: w=>0.3228895144, b=>-5.5707155334
Iteration 416: Achieved Loss=> 52.0961558780
Gradients: w=>0.3227178564, b=>-5.5677539701
Iteration 417: Achieved Loss=> 52.0650601149
Gradients: w=>0.3225462897, b=>-5.5647939812
Iteration 418: Achieved Loss=> 52.0339974059
Gradients: w=>0.3223748142, b=>-5.5618355659
Iteration 419: Achieved Loss=> 52.0029677158
Gradients: w=>0.3222034298, b=>-5.5588787234
Iteration 420: Achieved Loss=> 51.9719710097
Gradients: w=>0.3220321366, b=>-5.5559234528
Iteration 421: Achieved Loss=> 51.9410072524
Gradients: w=>0.3218609344, b=>-5.5529697534
Iteration 422: Achieved Loss=> 51.9100764090
Gradients: w=>0.3216898233, b=>-5.5500176242
Iteration 423: Achieved Loss=> 51.8791784443
Gradients: w=>0.3215188031, b=>-5.5470670645
Iteration 424: Achieved Loss=> 51.8483133236
Gradients: w=>0.3213478738, b=>-5.5441180734
Iteration 425: Achieved Loss=> 51.8174810118
Gradients: w=>0.3211770354, b=>-5.5411706500
Iteration 426: Achieved Loss=> 51.7866814741
Gradients: w=>0.3210062879, b=>-5.5382247936
Iteration 427: Achieved Loss=> 51.7559146757
Gradients: w=>0.3208356311, b=>-5.5352805033
Iteration 428: Achieved Loss=> 51.7251805817
Gradients: w=>0.3206650650, b=>-5.5323377783
Iteration 429: Achieved Loss=> 51.6944791574
Gradients: w=>0.3204945896, b=>-5.5293966177
Iteration 430: Achieved Loss=> 51.6638103680
Gradients: w=>0.3203242048, b=>-5.5264570208
Iteration 431: Achieved Loss=> 51.6331741790
Gradients: w=>0.3201539107, b=>-5.5235189866
Iteration 432: Achieved Loss=> 51.6025705555
Gradients: w=>0.3199837070, b=>-5.5205825143
Iteration 433: Achieved Loss=> 51.5719994631
Gradients: w=>0.3198135939, b=>-5.5176476032
Iteration 434: Achieved Loss=> 51.5414608671
Gradients: w=>0.3196435711, b=>-5.5147142524
Iteration 435: Achieved Loss=> 51.5109547329
Gradients: w=>0.3194736388, b=>-5.5117824610
Iteration 436: Achieved Loss=> 51.4804810261
Gradients: w=>0.3193037968, b=>-5.5088522283
Iteration 437: Achieved Loss=> 51.4500397123
Gradients: w=>0.3191340451, b=>-5.5059235533
Iteration 438: Achieved Loss=> 51.4196307569
Gradients: w=>0.3189643837, b=>-5.5029964354
Iteration 439: Achieved Loss=> 51.3892541256
Gradients: w=>0.3187948124, b=>-5.5000708736
Iteration 440: Achieved Loss=> 51.3589097839
Gradients: w=>0.3186253313, b=>-5.4971468671
Iteration 441: Achieved Loss=> 51.3285976977
Gradients: w=>0.3184559403, b=>-5.4942244151
Iteration 442: Achieved Loss=> 51.2983178326
Gradients: w=>0.3182866393, b=>-5.4913035167
Iteration 443: Achieved Loss=> 51.2680701543
Gradients: w=>0.3181174284, b=>-5.4883841712
Iteration 444: Achieved Loss=> 51.2378546287
Gradients: w=>0.3179483074, b=>-5.4854663777
Iteration 445: Achieved Loss=> 51.2076712216
Gradients: w=>0.3177792763, b=>-5.4825501354
Iteration 446: Achieved Loss=> 51.1775198987
Gradients: w=>0.3176103351, b=>-5.4796354435
Iteration 447: Achieved Loss=> 51.1474006261
Gradients: w=>0.3174414837, b=>-5.4767223011
Iteration 448: Achieved Loss=> 51.1173133696
Gradients: w=>0.3172727221, b=>-5.4738107074
Iteration 449: Achieved Loss=> 51.0872580953
Gradients: w=>0.3171040501, b=>-5.4709006616
Iteration 450: Achieved Loss=> 51.0572347690
Gradients: w=>0.3169354679, b=>-5.4679921629
Iteration 451: Achieved Loss=> 51.0272433569
Gradients: w=>0.3167669753, b=>-5.4650852104
Iteration 452: Achieved Loss=> 50.9972838251
Gradients: w=>0.3165985722, b=>-5.4621798034
Iteration 453: Achieved Loss=> 50.9673561396
Gradients: w=>0.3164302587, b=>-5.4592759409
Iteration 454: Achieved Loss=> 50.9374602666
Gradients: w=>0.3162620347, b=>-5.4563736223
Iteration 455: Achieved Loss=> 50.9075961722
Gradients: w=>0.3160939001, b=>-5.4534728466
Iteration 456: Achieved Loss=> 50.8777638228
Gradients: w=>0.3159258548, b=>-5.4505736130
Iteration 457: Achieved Loss=> 50.8479631845
Gradients: w=>0.3157578990, b=>-5.4476759208
Iteration 458: Achieved Loss=> 50.8181942237
Gradients: w=>0.3155900324, b=>-5.4447797690
Iteration 459: Achieved Loss=> 50.7884569066
Gradients: w=>0.3154222550, b=>-5.4418851570
Iteration 460: Achieved Loss=> 50.7587511997
Gradients: w=>0.3152545669, b=>-5.4389920838
Iteration 461: Achieved Loss=> 50.7290770693
Gradients: w=>0.3150869679, b=>-5.4361005487
Iteration 462: Achieved Loss=> 50.6994344819
Gradients: w=>0.3149194580, b=>-5.4332105507
Iteration 463: Achieved Loss=> 50.6698234039
Gradients: w=>0.3147520371, b=>-5.4303220892
Iteration 464: Achieved Loss=> 50.6402438018
Gradients: w=>0.3145847053, b=>-5.4274351633
Iteration 465: Achieved Loss=> 50.6106956423
Gradients: w=>0.3144174624, b=>-5.4245497722
Iteration 466: Achieved Loss=> 50.5811788918
Gradients: w=>0.3142503084, b=>-5.4216659150
Iteration 467: Achieved Loss=> 50.5516935170
Gradients: w=>0.3140832433, b=>-5.4187835910
Iteration 468: Achieved Loss=> 50.5222394845
Gradients: w=>0.3139162670, b=>-5.4159027993
Iteration 469: Achieved Loss=> 50.4928167611
Gradients: w=>0.3137493795, b=>-5.4130235392
Iteration 470: Achieved Loss=> 50.4634253133
Gradients: w=>0.3135825807, b=>-5.4101458097
Iteration 471: Achieved Loss=> 50.4340651081
Gradients: w=>0.3134158706, b=>-5.4072696101
Iteration 472: Achieved Loss=> 50.4047361121
Gradients: w=>0.3132492491, b=>-5.4043949396
Iteration 473: Achieved Loss=> 50.3754382922
Gradients: w=>0.3130827161, b=>-5.4015217974
Iteration 474: Achieved Loss=> 50.3461716153
Gradients: w=>0.3129162718, b=>-5.3986501826
Iteration 475: Achieved Loss=> 50.3169360483
Gradients: w=>0.3127499159, b=>-5.3957800945
Iteration 476: Achieved Loss=> 50.2877315580
Gradients: w=>0.3125836484, b=>-5.3929115321
Iteration 477: Achieved Loss=> 50.2585581115
Gradients: w=>0.3124174693, b=>-5.3900444948
Iteration 478: Achieved Loss=> 50.2294156757
Gradients: w=>0.3122513786, b=>-5.3871789817
Iteration 479: Achieved Loss=> 50.2003042177
Gradients: w=>0.3120853762, b=>-5.3843149920
Iteration 480: Achieved Loss=> 50.1712237046
Gradients: w=>0.3119194620, b=>-5.3814525249
Iteration 481: Achieved Loss=> 50.1421741035
Gradients: w=>0.3117536361, b=>-5.3785915796
Iteration 482: Achieved Loss=> 50.1131553814
Gradients: w=>0.3115878983, b=>-5.3757321552
Iteration 483: Achieved Loss=> 50.0841675057
Gradients: w=>0.3114222486, b=>-5.3728742510
Iteration 484: Achieved Loss=> 50.0552104434
Gradients: w=>0.3112566870, b=>-5.3700178661
Iteration 485: Achieved Loss=> 50.0262841619
Gradients: w=>0.3110912134, b=>-5.3671629998
Iteration 486: Achieved Loss=> 49.9973886284
Gradients: w=>0.3109258277, b=>-5.3643096512
Iteration 487: Achieved Loss=> 49.9685238103
Gradients: w=>0.3107605300, b=>-5.3614578195
Iteration 488: Achieved Loss=> 49.9396896748
Gradients: w=>0.3105953202, b=>-5.3586075040
Iteration 489: Achieved Loss=> 49.9108861894
Gradients: w=>0.3104301982, b=>-5.3557587037
Iteration 490: Achieved Loss=> 49.8821133215
Gradients: w=>0.3102651639, b=>-5.3529114180
Iteration 491: Achieved Loss=> 49.8533710386
Gradients: w=>0.3101002175, b=>-5.3500656460
Iteration 492: Achieved Loss=> 49.8246593081
Gradients: w=>0.3099353587, b=>-5.3472213869
Iteration 493: Achieved Loss=> 49.7959780975
Gradients: w=>0.3097705875, b=>-5.3443786399
Iteration 494: Achieved Loss=> 49.7673273745
Gradients: w=>0.3096059040, b=>-5.3415374041
Iteration 495: Achieved Loss=> 49.7387071065
Gradients: w=>0.3094413080, b=>-5.3386976789
Iteration 496: Achieved Loss=> 49.7101172614
Gradients: w=>0.3092767995, b=>-5.3358594633
Iteration 497: Achieved Loss=> 49.6815578066
Gradients: w=>0.3091123784, b=>-5.3330227567
Iteration 498: Achieved Loss=> 49.6530287099
Gradients: w=>0.3089480448, b=>-5.3301875581
Iteration 499: Achieved Loss=> 49.6245299390
Gradients: w=>0.3087837985, b=>-5.3273538668
Iteration 500: Achieved Loss=> 49.5960614617
Gradients: w=>0.3086196396, b=>-5.3245216819
Iteration 501: Achieved Loss=> 49.5676232458
Gradients: w=>0.3084555679, b=>-5.3216910028
Iteration 502: Achieved Loss=> 49.5392152591
Gradients: w=>0.3082915835, b=>-5.3188618285
Iteration 503: Achieved Loss=> 49.5108374695
Gradients: w=>0.3081276862, b=>-5.3160341583
Iteration 504: Achieved Loss=> 49.4824898449
Gradients: w=>0.3079638761, b=>-5.3132079913
Iteration 505: Achieved Loss=> 49.4541723532
Gradients: w=>0.3078001530, b=>-5.3103833269
Iteration 506: Achieved Loss=> 49.4258849625
Gradients: w=>0.3076365170, b=>-5.3075601641
Iteration 507: Achieved Loss=> 49.3976276406
Gradients: w=>0.3074729680, b=>-5.3047385022
Iteration 508: Achieved Loss=> 49.3694003556
Gradients: w=>0.3073095059, b=>-5.3019183404
Iteration 509: Achieved Loss=> 49.3412030756
Gradients: w=>0.3071461308, b=>-5.2990996779
Iteration 510: Achieved Loss=> 49.3130357688
Gradients: w=>0.3069828424, b=>-5.2962825138
Iteration 511: Achieved Loss=> 49.2848984032
Gradients: w=>0.3068196409, b=>-5.2934668475
Iteration 512: Achieved Loss=> 49.2567909470
Gradients: w=>0.3066565262, b=>-5.2906526780
Iteration 513: Achieved Loss=> 49.2287133685
Gradients: w=>0.3064934982, b=>-5.2878400046
Iteration 514: Achieved Loss=> 49.2006656358
Gradients: w=>0.3063305568, b=>-5.2850288266
Iteration 515: Achieved Loss=> 49.1726477172
Gradients: w=>0.3061677021, b=>-5.2822191430
Iteration 516: Achieved Loss=> 49.1446595812
Gradients: w=>0.3060049340, b=>-5.2794109532
Iteration 517: Achieved Loss=> 49.1167011959
Gradients: w=>0.3058422523, b=>-5.2766042563
Iteration 518: Achieved Loss=> 49.0887725298
Gradients: w=>0.3056796572, b=>-5.2737990515
Iteration 519: Achieved Loss=> 49.0608735512
Gradients: w=>0.3055171485, b=>-5.2709953380
Iteration 520: Achieved Loss=> 49.0330042287
Gradients: w=>0.3053547262, b=>-5.2681931151
Iteration 521: Achieved Loss=> 49.0051645307
Gradients: w=>0.3051923903, b=>-5.2653923820
Iteration 522: Achieved Loss=> 48.9773544257
Gradients: w=>0.3050301406, b=>-5.2625931378
Iteration 523: Achieved Loss=> 48.9495738822
Gradients: w=>0.3048679773, b=>-5.2597953817
Iteration 524: Achieved Loss=> 48.9218228689
Gradients: w=>0.3047059001, b=>-5.2569991130
Iteration 525: Achieved Loss=> 48.8941013543
Gradients: w=>0.3045439091, b=>-5.2542043309
Iteration 526: Achieved Loss=> 48.8664093071
Gradients: w=>0.3043820042, b=>-5.2514110347
Iteration 527: Achieved Loss=> 48.8387466959
Gradients: w=>0.3042201854, b=>-5.2486192234
Iteration 528: Achieved Loss=> 48.8111134895
Gradients: w=>0.3040584526, b=>-5.2458288963
Iteration 529: Achieved Loss=> 48.7835096566
Gradients: w=>0.3038968058, b=>-5.2430400526
Iteration 530: Achieved Loss=> 48.7559351660
Gradients: w=>0.3037352449, b=>-5.2402526916
Iteration 531: Achieved Loss=> 48.7283899866
Gradients: w=>0.3035737699, b=>-5.2374668124
Iteration 532: Achieved Loss=> 48.7008740870
Gradients: w=>0.3034123808, b=>-5.2346824143
Iteration 533: Achieved Loss=> 48.6733874363
Gradients: w=>0.3032510774, b=>-5.2318994964
Iteration 534: Achieved Loss=> 48.6459300033
Gradients: w=>0.3030898599, b=>-5.2291180581
Iteration 535: Achieved Loss=> 48.6185017569
Gradients: w=>0.3029287280, b=>-5.2263380984
Iteration 536: Achieved Loss=> 48.5911026662
Gradients: w=>0.3027676818, b=>-5.2235596166
Iteration 537: Achieved Loss=> 48.5637327002
Gradients: w=>0.3026067212, b=>-5.2207826120
Iteration 538: Achieved Loss=> 48.5363918278
Gradients: w=>0.3024458462, b=>-5.2180070837
Iteration 539: Achieved Loss=> 48.5090800182
Gradients: w=>0.3022850567, b=>-5.2152330310
Iteration 540: Achieved Loss=> 48.4817972405
Gradients: w=>0.3021243527, b=>-5.2124604530
Iteration 541: Achieved Loss=> 48.4545434638
Gradients: w=>0.3019637341, b=>-5.2096893491
Iteration 542: Achieved Loss=> 48.4273186573
Gradients: w=>0.3018032009, b=>-5.2069197183
Iteration 543: Achieved Loss=> 48.4001227902
Gradients: w=>0.3016427530, b=>-5.2041515600
Iteration 544: Achieved Loss=> 48.3729558317
Gradients: w=>0.3014823905, b=>-5.2013848732
Iteration 545: Achieved Loss=> 48.3458177511
Gradients: w=>0.3013221132, b=>-5.1986196574
Iteration 546: Achieved Loss=> 48.3187085177
Gradients: w=>0.3011619211, b=>-5.1958559116
Iteration 547: Achieved Loss=> 48.2916281008
Gradients: w=>0.3010018142, b=>-5.1930936351
Iteration 548: Achieved Loss=> 48.2645764699
Gradients: w=>0.3008417924, b=>-5.1903328271
Iteration 549: Achieved Loss=> 48.2375535943
Gradients: w=>0.3006818557, b=>-5.1875734869
Iteration 550: Achieved Loss=> 48.2105594434
Gradients: w=>0.3005220040, b=>-5.1848156136
Iteration 551: Achieved Loss=> 48.1835939867
Gradients: w=>0.3003622372, b=>-5.1820592065
Iteration 552: Achieved Loss=> 48.1566571937
Gradients: w=>0.3002025555, b=>-5.1793042647
Iteration 553: Achieved Loss=> 48.1297490340
Gradients: w=>0.3000429586, b=>-5.1765507876
Iteration 554: Achieved Loss=> 48.1028694770
Gradients: w=>0.2998834465, b=>-5.1737987743
Iteration 555: Achieved Loss=> 48.0760184925
Gradients: w=>0.2997240193, b=>-5.1710482241
Iteration 556: Achieved Loss=> 48.0491960500
Gradients: w=>0.2995646768, b=>-5.1682991361
Iteration 557: Achieved Loss=> 48.0224021192
Gradients: w=>0.2994054190, b=>-5.1655515097
Iteration 558: Achieved Loss=> 47.9956366697
Gradients: w=>0.2992462459, b=>-5.1628053439
Iteration 559: Achieved Loss=> 47.9688996714
Gradients: w=>0.2990871574, b=>-5.1600606381
Iteration 560: Achieved Loss=> 47.9421910939
Gradients: w=>0.2989281535, b=>-5.1573173915
Iteration 561: Achieved Loss=> 47.9155109071
Gradients: w=>0.2987692341, b=>-5.1545756033
Iteration 562: Achieved Loss=> 47.8888590807
Gradients: w=>0.2986103992, b=>-5.1518352727
Iteration 563: Achieved Loss=> 47.8622355847
Gradients: w=>0.2984516488, b=>-5.1490963989
Iteration 564: Achieved Loss=> 47.8356403889
Gradients: w=>0.2982929827, b=>-5.1463589812
Iteration 565: Achieved Loss=> 47.8090734632
Gradients: w=>0.2981344010, b=>-5.1436230188
Iteration 566: Achieved Loss=> 47.7825347776
Gradients: w=>0.2979759036, b=>-5.1408885109
Iteration 567: Achieved Loss=> 47.7560243020
Gradients: w=>0.2978174905, b=>-5.1381554568
Iteration 568: Achieved Loss=> 47.7295420065
Gradients: w=>0.2976591615, b=>-5.1354238556
Iteration 569: Achieved Loss=> 47.7030878611
Gradients: w=>0.2975009168, b=>-5.1326937067
Iteration 570: Achieved Loss=> 47.6766618360
Gradients: w=>0.2973427562, b=>-5.1299650092
Iteration 571: Achieved Loss=> 47.6502639011
Gradients: w=>0.2971846796, b=>-5.1272377623
Iteration 572: Achieved Loss=> 47.6238940267
Gradients: w=>0.2970266871, b=>-5.1245119653
Iteration 573: Achieved Loss=> 47.5975521829
Gradients: w=>0.2968687786, b=>-5.1217876175
Iteration 574: Achieved Loss=> 47.5712383399
Gradients: w=>0.2967109541, b=>-5.1190647180
Iteration 575: Achieved Loss=> 47.5449524680
Gradients: w=>0.2965532134, b=>-5.1163432660
Iteration 576: Achieved Loss=> 47.5186945373
Gradients: w=>0.2963955566, b=>-5.1136232609
Iteration 577: Achieved Loss=> 47.4924645184
Gradients: w=>0.2962379837, b=>-5.1109047018
Iteration 578: Achieved Loss=> 47.4662623813
Gradients: w=>0.2960804945, b=>-5.1081875880
Iteration 579: Achieved Loss=> 47.4400880966
Gradients: w=>0.2959230890, b=>-5.1054719187
Iteration 580: Achieved Loss=> 47.4139416346
Gradients: w=>0.2957657672, b=>-5.1027576931
Iteration 581: Achieved Loss=> 47.3878229657
Gradients: w=>0.2956085290, b=>-5.1000449105
Iteration 582: Achieved Loss=> 47.3617320604
Gradients: w=>0.2954513745, b=>-5.0973335701
Iteration 583: Achieved Loss=> 47.3356688892
Gradients: w=>0.2952943034, b=>-5.0946236711
Iteration 584: Achieved Loss=> 47.3096334226
Gradients: w=>0.2951373159, b=>-5.0919152128
Iteration 585: Achieved Loss=> 47.2836256312
Gradients: w=>0.2949804119, b=>-5.0892081943
Iteration 586: Achieved Loss=> 47.2576454854
Gradients: w=>0.2948235912, b=>-5.0865026151
Iteration 587: Achieved Loss=> 47.2316929561
Gradients: w=>0.2946668540, b=>-5.0837984741
Iteration 588: Achieved Loss=> 47.2057680136
Gradients: w=>0.2945102000, b=>-5.0810957708
Iteration 589: Achieved Loss=> 47.1798706289
Gradients: w=>0.2943536293, b=>-5.0783945043
Iteration 590: Achieved Loss=> 47.1540007725
Gradients: w=>0.2941971419, b=>-5.0756946739
Iteration 591: Achieved Loss=> 47.1281584153
Gradients: w=>0.2940407377, b=>-5.0729962789
Iteration 592: Achieved Loss=> 47.1023435279
Gradients: w=>0.2938844166, b=>-5.0702993183
Iteration 593: Achieved Loss=> 47.0765560812
Gradients: w=>0.2937281786, b=>-5.0676037916
Iteration 594: Achieved Loss=> 47.0507960460
Gradients: w=>0.2935720237, b=>-5.0649096979
Iteration 595: Achieved Loss=> 47.0250633932
Gradients: w=>0.2934159518, b=>-5.0622170364
Iteration 596: Achieved Loss=> 46.9993580936
Gradients: w=>0.2932599629, b=>-5.0595258064
Iteration 597: Achieved Loss=> 46.9736801182
Gradients: w=>0.2931040569, b=>-5.0568360072
Iteration 598: Achieved Loss=> 46.9480294380
Gradients: w=>0.2929482338, b=>-5.0541476380
Iteration 599: Achieved Loss=> 46.9224060239
Gradients: w=>0.2927924935, b=>-5.0514606980
Iteration 600: Achieved Loss=> 46.8968098469
Gradients: w=>0.2926368360, b=>-5.0487751864
Iteration 601: Achieved Loss=> 46.8712408782
Gradients: w=>0.2924812613, b=>-5.0460911025
Iteration 602: Achieved Loss=> 46.8456990887
Gradients: w=>0.2923257693, b=>-5.0434084456
Iteration 603: Achieved Loss=> 46.8201844495
Gradients: w=>0.2921703599, b=>-5.0407272149
Iteration 604: Achieved Loss=> 46.7946969319
Gradients: w=>0.2920150332, b=>-5.0380474096
Iteration 605: Achieved Loss=> 46.7692365070
Gradients: w=>0.2918597891, b=>-5.0353690289
Iteration 606: Achieved Loss=> 46.7438031460
Gradients: w=>0.2917046274, b=>-5.0326920722
Iteration 607: Achieved Loss=> 46.7183968201
Gradients: w=>0.2915495483, b=>-5.0300165386
Iteration 608: Achieved Loss=> 46.6930175006
Gradients: w=>0.2913945516, b=>-5.0273424274
Iteration 609: Achieved Loss=> 46.6676651588
Gradients: w=>0.2912396373, b=>-5.0246697379
Iteration 610: Achieved Loss=> 46.6423397659
Gradients: w=>0.2910848054, b=>-5.0219984692
Iteration 611: Achieved Loss=> 46.6170412935
Gradients: w=>0.2909300558, b=>-5.0193286207
Iteration 612: Achieved Loss=> 46.5917697127
Gradients: w=>0.2907753884, b=>-5.0166601915
Iteration 613: Achieved Loss=> 46.5665249951
Gradients: w=>0.2906208033, b=>-5.0139931810
Iteration 614: Achieved Loss=> 46.5413071121
Gradients: w=>0.2904663004, b=>-5.0113275883
Iteration 615: Achieved Loss=> 46.5161160352
Gradients: w=>0.2903118796, b=>-5.0086634127
Iteration 616: Achieved Loss=> 46.4909517359
Gradients: w=>0.2901575409, b=>-5.0060006535
Iteration 617: Achieved Loss=> 46.4658141857
Gradients: w=>0.2900032842, b=>-5.0033393099
Iteration 618: Achieved Loss=> 46.4407033561
Gradients: w=>0.2898491096, b=>-5.0006793811
Iteration 619: Achieved Loss=> 46.4156192189
Gradients: w=>0.2896950169, b=>-4.9980208665
Iteration 620: Achieved Loss=> 46.3905617455
Gradients: w=>0.2895410061, b=>-4.9953637652
Iteration 621: Achieved Loss=> 46.3655309077
Gradients: w=>0.2893870772, b=>-4.9927080765
Iteration 622: Achieved Loss=> 46.3405266772
Gradients: w=>0.2892332302, b=>-4.9900537996
Iteration 623: Achieved Loss=> 46.3155490256
Gradients: w=>0.2890794649, b=>-4.9874009338
Iteration 624: Achieved Loss=> 46.2905979248
Gradients: w=>0.2889257814, b=>-4.9847494784
Iteration 625: Achieved Loss=> 46.2656733464
Gradients: w=>0.2887721796, b=>-4.9820994326
Iteration 626: Achieved Loss=> 46.2407752623
Gradients: w=>0.2886186594, b=>-4.9794507956
Iteration 627: Achieved Loss=> 46.2159036444
Gradients: w=>0.2884652209, b=>-4.9768035667
Iteration 628: Achieved Loss=> 46.1910584644
Gradients: w=>0.2883118639, b=>-4.9741577452
Iteration 629: Achieved Loss=> 46.1662396944
Gradients: w=>0.2881585885, b=>-4.9715133303
Iteration 630: Achieved Loss=> 46.1414473062
Gradients: w=>0.2880053945, b=>-4.9688703212
Iteration 631: Achieved Loss=> 46.1166812717
Gradients: w=>0.2878522820, b=>-4.9662287172
Iteration 632: Achieved Loss=> 46.0919415631
Gradients: w=>0.2876992509, b=>-4.9635885176
Iteration 633: Achieved Loss=> 46.0672281522
Gradients: w=>0.2875463012, b=>-4.9609497216
Iteration 634: Achieved Loss=> 46.0425410112
Gradients: w=>0.2873934327, b=>-4.9583123284
Iteration 635: Achieved Loss=> 46.0178801120
Gradients: w=>0.2872406455, b=>-4.9556763374
Iteration 636: Achieved Loss=> 45.9932454269
Gradients: w=>0.2870879396, b=>-4.9530417477
Iteration 637: Achieved Loss=> 45.9686369279
Gradients: w=>0.2869353148, b=>-4.9504085587
Iteration 638: Achieved Loss=> 45.9440545872
Gradients: w=>0.2867827712, b=>-4.9477767696
Iteration 639: Achieved Loss=> 45.9194983771
Gradients: w=>0.2866303087, b=>-4.9451463796
Iteration 640: Achieved Loss=> 45.8949682696
Gradients: w=>0.2864779272, b=>-4.9425173879
Iteration 641: Achieved Loss=> 45.8704642372
Gradients: w=>0.2863256268, b=>-4.9398897940
Iteration 642: Achieved Loss=> 45.8459862520
Gradients: w=>0.2861734073, b=>-4.9372635970
Iteration 643: Achieved Loss=> 45.8215342864
Gradients: w=>0.2860212687, b=>-4.9346387961
Iteration 644: Achieved Loss=> 45.7971083127
Gradients: w=>0.2858692110, b=>-4.9320153906
Iteration 645: Achieved Loss=> 45.7727083033
Gradients: w=>0.2857172342, b=>-4.9293933799
Iteration 646: Achieved Loss=> 45.7483342306
Gradients: w=>0.2855653381, b=>-4.9267727630
Iteration 647: Achieved Loss=> 45.7239860670
Gradients: w=>0.2854135228, b=>-4.9241535394
Iteration 648: Achieved Loss=> 45.6996637850
Gradients: w=>0.2852617882, b=>-4.9215357082
Iteration 649: Achieved Loss=> 45.6753673571
Gradients: w=>0.2851101343, b=>-4.9189192688
Iteration 650: Achieved Loss=> 45.6510967557
Gradients: w=>0.2849585610, b=>-4.9163042203
Iteration 651: Achieved Loss=> 45.6268519535
Gradients: w=>0.2848070683, b=>-4.9136905621
Iteration 652: Achieved Loss=> 45.6026329230
Gradients: w=>0.2846556561, b=>-4.9110782934
Iteration 653: Achieved Loss=> 45.5784396368
Gradients: w=>0.2845043244, b=>-4.9084674134
Iteration 654: Achieved Loss=> 45.5542720676
Gradients: w=>0.2843530732, b=>-4.9058579215
Iteration 655: Achieved Loss=> 45.5301301879
Gradients: w=>0.2842019024, b=>-4.9032498168
Iteration 656: Achieved Loss=> 45.5060139706
Gradients: w=>0.2840508119, b=>-4.9006430987
Iteration 657: Achieved Loss=> 45.4819233883
Gradients: w=>0.2838998018, b=>-4.8980377664
Iteration 658: Achieved Loss=> 45.4578584138
Gradients: w=>0.2837488719, b=>-4.8954338192
Iteration 659: Achieved Loss=> 45.4338190198
Gradients: w=>0.2835980223, b=>-4.8928312563
Iteration 660: Achieved Loss=> 45.4098051792
Gradients: w=>0.2834472529, b=>-4.8902300770
Iteration 661: Achieved Loss=> 45.3858168648
Gradients: w=>0.2832965637, b=>-4.8876302806
Iteration 662: Achieved Loss=> 45.3618540495
Gradients: w=>0.2831459545, b=>-4.8850318663
Iteration 663: Achieved Loss=> 45.3379167062
Gradients: w=>0.2829954254, b=>-4.8824348334
Iteration 664: Achieved Loss=> 45.3140048077
Gradients: w=>0.2828449764, b=>-4.8798391812
Iteration 665: Achieved Loss=> 45.2901183270
Gradients: w=>0.2826946073, b=>-4.8772449089
Iteration 666: Achieved Loss=> 45.2662572372
Gradients: w=>0.2825443182, b=>-4.8746520158
Iteration 667: Achieved Loss=> 45.2424215113
Gradients: w=>0.2823941089, b=>-4.8720605012
Iteration 668: Achieved Loss=> 45.2186111221
Gradients: w=>0.2822439796, b=>-4.8694703643
Iteration 669: Achieved Loss=> 45.1948260430
Gradients: w=>0.2820939300, b=>-4.8668816044
Iteration 670: Achieved Loss=> 45.1710662468
Gradients: w=>0.2819439602, b=>-4.8642942207
Iteration 671: Achieved Loss=> 45.1473317069
Gradients: w=>0.2817940701, b=>-4.8617082126
Iteration 672: Achieved Loss=> 45.1236223962
Gradients: w=>0.2816442598, b=>-4.8591235793
Iteration 673: Achieved Loss=> 45.0999382880
Gradients: w=>0.2814945290, b=>-4.8565403200
Iteration 674: Achieved Loss=> 45.0762793555
Gradients: w=>0.2813448779, b=>-4.8539584341
Iteration 675: Achieved Loss=> 45.0526455720
Gradients: w=>0.2811953063, b=>-4.8513779208
Iteration 676: Achieved Loss=> 45.0290369107
Gradients: w=>0.2810458143, b=>-4.8487987794
Iteration 677: Achieved Loss=> 45.0054533449
Gradients: w=>0.2808964017, b=>-4.8462210092
Iteration 678: Achieved Loss=> 44.9818948479
Gradients: w=>0.2807470685, b=>-4.8436446093
Iteration 679: Achieved Loss=> 44.9583613931
Gradients: w=>0.2805978148, b=>-4.8410695792
Iteration 680: Achieved Loss=> 44.9348529539
Gradients: w=>0.2804486404, b=>-4.8384959180
Iteration 681: Achieved Loss=> 44.9113695036
Gradients: w=>0.2802995453, b=>-4.8359236250
Iteration 682: Achieved Loss=> 44.8879110157
Gradients: w=>0.2801505294, b=>-4.8333526996
Iteration 683: Achieved Loss=> 44.8644774638
Gradients: w=>0.2800015928, b=>-4.8307831409
Iteration 684: Achieved Loss=> 44.8410688211
Gradients: w=>0.2798527354, b=>-4.8282149483
Iteration 685: Achieved Loss=> 44.8176850614
Gradients: w=>0.2797039571, b=>-4.8256481211
Iteration 686: Achieved Loss=> 44.7943261581
Gradients: w=>0.2795552579, b=>-4.8230826584
Iteration 687: Achieved Loss=> 44.7709920849
Gradients: w=>0.2794066377, b=>-4.8205185596
Iteration 688: Achieved Loss=> 44.7476828152
Gradients: w=>0.2792580966, b=>-4.8179558240
Iteration 689: Achieved Loss=> 44.7243983228
Gradients: w=>0.2791096344, b=>-4.8153944508
Iteration 690: Achieved Loss=> 44.7011385814
Gradients: w=>0.2789612511, b=>-4.8128344393
Iteration 691: Achieved Loss=> 44.6779035645
Gradients: w=>0.2788129468, b=>-4.8102757888
Iteration 692: Achieved Loss=> 44.6546932460
Gradients: w=>0.2786647212, b=>-4.8077184985
Iteration 693: Achieved Loss=> 44.6315075995
Gradients: w=>0.2785165745, b=>-4.8051625678
Iteration 694: Achieved Loss=> 44.6083465989
Gradients: w=>0.2783685066, b=>-4.8026079959
Iteration 695: Achieved Loss=> 44.5852102180
Gradients: w=>0.2782205173, b=>-4.8000547821
Iteration 696: Achieved Loss=> 44.5620984305
Gradients: w=>0.2780726068, b=>-4.7975029256
Iteration 697: Achieved Loss=> 44.5390112104
Gradients: w=>0.2779247748, b=>-4.7949524258
Iteration 698: Achieved Loss=> 44.5159485315
Gradients: w=>0.2777770215, b=>-4.7924032819
Iteration 699: Achieved Loss=> 44.4929103677
Gradients: w=>0.2776293467, b=>-4.7898554932
Iteration 700: Achieved Loss=> 44.4698966930
Gradients: w=>0.2774817504, b=>-4.7873090590
Iteration 701: Achieved Loss=> 44.4469074814
Gradients: w=>0.2773342326, b=>-4.7847639786
Iteration 702: Achieved Loss=> 44.4239427068
Gradients: w=>0.2771867932, b=>-4.7822202512
Iteration 703: Achieved Loss=> 44.4010023433
Gradients: w=>0.2770394322, b=>-4.7796778761
Iteration 704: Achieved Loss=> 44.3780863649
Gradients: w=>0.2768921495, b=>-4.7771368526
Iteration 705: Achieved Loss=> 44.3551947457
Gradients: w=>0.2767449452, b=>-4.7745971800
Iteration 706: Achieved Loss=> 44.3323274598
Gradients: w=>0.2765978191, b=>-4.7720588576
Iteration 707: Achieved Loss=> 44.3094844813
Gradients: w=>0.2764507712, b=>-4.7695218847
Iteration 708: Achieved Loss=> 44.2866657844
Gradients: w=>0.2763038014, b=>-4.7669862604
Iteration 709: Achieved Loss=> 44.2638713434
Gradients: w=>0.2761569099, b=>-4.7644519842
Iteration 710: Achieved Loss=> 44.2411011323
Gradients: w=>0.2760100964, b=>-4.7619190553
Iteration 711: Achieved Loss=> 44.2183551255
Gradients: w=>0.2758633609, b=>-4.7593874730
Iteration 712: Achieved Loss=> 44.1956332972
Gradients: w=>0.2757167035, b=>-4.7568572365
Iteration 713: Achieved Loss=> 44.1729356217
Gradients: w=>0.2755701240, b=>-4.7543283452
Iteration 714: Achieved Loss=> 44.1502620734
Gradients: w=>0.2754236225, b=>-4.7518007983
Iteration 715: Achieved Loss=> 44.1276126266
Gradients: w=>0.2752771989, b=>-4.7492745951
Iteration 716: Achieved Loss=> 44.1049872556
Gradients: w=>0.2751308530, b=>-4.7467497350
Iteration 717: Achieved Loss=> 44.0823859349
Gradients: w=>0.2749845850, b=>-4.7442262171
Iteration 718: Achieved Loss=> 44.0598086390
Gradients: w=>0.2748383948, b=>-4.7417040408
Iteration 719: Achieved Loss=> 44.0372553423
Gradients: w=>0.2746922823, b=>-4.7391832054
Iteration 720: Achieved Loss=> 44.0147260192
Gradients: w=>0.2745462474, b=>-4.7366637102
Iteration 721: Achieved Loss=> 43.9922206443
Gradients: w=>0.2744002902, b=>-4.7341455543
Iteration 722: Achieved Loss=> 43.9697391922
Gradients: w=>0.2742544106, b=>-4.7316287373
Iteration 723: Achieved Loss=> 43.9472816373
Gradients: w=>0.2741086085, b=>-4.7291132582
Iteration 724: Achieved Loss=> 43.9248479544
Gradients: w=>0.2739628840, b=>-4.7265991164
Iteration 725: Achieved Loss=> 43.9024381180
Gradients: w=>0.2738172369, b=>-4.7240863112
Iteration 726: Achieved Loss=> 43.8800521028
Gradients: w=>0.2736716672, b=>-4.7215748420
Iteration 727: Achieved Loss=> 43.8576898834
Gradients: w=>0.2735261750, b=>-4.7190647078
Iteration 728: Achieved Loss=> 43.8353514346
Gradients: w=>0.2733807601, b=>-4.7165559082
Iteration 729: Achieved Loss=> 43.8130367311
Gradients: w=>0.2732354225, b=>-4.7140484423
Iteration 730: Achieved Loss=> 43.7907457476
Gradients: w=>0.2730901621, b=>-4.7115423094
Iteration 731: Achieved Loss=> 43.7684784590
Gradients: w=>0.2729449790, b=>-4.7090375089
Iteration 732: Achieved Loss=> 43.7462348401
Gradients: w=>0.2727998731, b=>-4.7065340401
Iteration 733: Achieved Loss=> 43.7240148656
Gradients: w=>0.2726548443, b=>-4.7040319021
Iteration 734: Achieved Loss=> 43.7018185105
Gradients: w=>0.2725098926, b=>-4.7015310943
Iteration 735: Achieved Loss=> 43.6796457497
Gradients: w=>0.2723650180, b=>-4.6990316161
Iteration 736: Achieved Loss=> 43.6574965580
Gradients: w=>0.2722202204, b=>-4.6965334667
Iteration 737: Achieved Loss=> 43.6353709104
Gradients: w=>0.2720754998, b=>-4.6940366453
Iteration 738: Achieved Loss=> 43.6132687820
Gradients: w=>0.2719308561, b=>-4.6915411513
Iteration 739: Achieved Loss=> 43.5911901476
Gradients: w=>0.2717862893, b=>-4.6890469841
Iteration 740: Achieved Loss=> 43.5691349824
Gradients: w=>0.2716417994, b=>-4.6865541428
Iteration 741: Achieved Loss=> 43.5471032613
Gradients: w=>0.2714973862, b=>-4.6840626267
Iteration 742: Achieved Loss=> 43.5250949595
Gradients: w=>0.2713530499, b=>-4.6815724353
Iteration 743: Achieved Loss=> 43.5031100520
Gradients: w=>0.2712087903, b=>-4.6790835676
Iteration 744: Achieved Loss=> 43.4811485140
Gradients: w=>0.2710646074, b=>-4.6765960232
Iteration 745: Achieved Loss=> 43.4592103207
Gradients: w=>0.2709205011, b=>-4.6741098012
Iteration 746: Achieved Loss=> 43.4372954472
Gradients: w=>0.2707764715, b=>-4.6716249010
Iteration 747: Achieved Loss=> 43.4154038688
Gradients: w=>0.2706325184, b=>-4.6691413218
Iteration 748: Achieved Loss=> 43.3935355606
Gradients: w=>0.2704886418, b=>-4.6666590629
Iteration 749: Achieved Loss=> 43.3716904980
Gradients: w=>0.2703448418, b=>-4.6641781237
Iteration 750: Achieved Loss=> 43.3498686562
Gradients: w=>0.2702011181, b=>-4.6616985035
Iteration 751: Achieved Loss=> 43.3280700106
Gradients: w=>0.2700574709, b=>-4.6592202015
Iteration 752: Achieved Loss=> 43.3062945364
Gradients: w=>0.2699139001, b=>-4.6567432170
Iteration 753: Achieved Loss=> 43.2845422092
Gradients: w=>0.2697704056, b=>-4.6542675494
Iteration 754: Achieved Loss=> 43.2628130042
Gradients: w=>0.2696269873, b=>-4.6517931979
Iteration 755: Achieved Loss=> 43.2411068969
Gradients: w=>0.2694836454, b=>-4.6493201618
Iteration 756: Achieved Loss=> 43.2194238627
Gradients: w=>0.2693403796, b=>-4.6468484405
Iteration 757: Achieved Loss=> 43.1977638771
Gradients: w=>0.2691971900, b=>-4.6443780332
Iteration 758: Achieved Loss=> 43.1761269156
Gradients: w=>0.2690540765, b=>-4.6419089393
Iteration 759: Achieved Loss=> 43.1545129538
Gradients: w=>0.2689110391, b=>-4.6394411580
Iteration 760: Achieved Loss=> 43.1329219671
Gradients: w=>0.2687680777, b=>-4.6369746887
Iteration 761: Achieved Loss=> 43.1113539312
Gradients: w=>0.2686251924, b=>-4.6345095306
Iteration 762: Achieved Loss=> 43.0898088217
Gradients: w=>0.2684823830, b=>-4.6320456831
Iteration 763: Achieved Loss=> 43.0682866141
Gradients: w=>0.2683396495, b=>-4.6295831455
Iteration 764: Achieved Loss=> 43.0467872842
Gradients: w=>0.2681969919, b=>-4.6271219170
Iteration 765: Achieved Loss=> 43.0253108077
Gradients: w=>0.2680544101, b=>-4.6246619969
Iteration 766: Achieved Loss=> 43.0038571601
Gradients: w=>0.2679119042, b=>-4.6222033846
Iteration 767: Achieved Loss=> 42.9824263174
Gradients: w=>0.2677694740, b=>-4.6197460794
Iteration 768: Achieved Loss=> 42.9610182551
Gradients: w=>0.2676271195, b=>-4.6172900806
Iteration 769: Achieved Loss=> 42.9396329492
Gradients: w=>0.2674848407, b=>-4.6148353875
Iteration 770: Achieved Loss=> 42.9182703755
Gradients: w=>0.2673426376, b=>-4.6123819994
Iteration 771: Achieved Loss=> 42.8969305096
Gradients: w=>0.2672005100, b=>-4.6099299155
Iteration 772: Achieved Loss=> 42.8756133276
Gradients: w=>0.2670584580, b=>-4.6074791353
Iteration 773: Achieved Loss=> 42.8543188053
Gradients: w=>0.2669164816, b=>-4.6050296579
Iteration 774: Achieved Loss=> 42.8330469187
Gradients: w=>0.2667745806, b=>-4.6025814828
Iteration 775: Achieved Loss=> 42.8117976436
Gradients: w=>0.2666327550, b=>-4.6001346092
Iteration 776: Achieved Loss=> 42.7905709560
Gradients: w=>0.2664910048, b=>-4.5976890365
Iteration 777: Achieved Loss=> 42.7693668320
Gradients: w=>0.2663493301, b=>-4.5952447638
Iteration 778: Achieved Loss=> 42.7481852475
Gradients: w=>0.2662077306, b=>-4.5928017907
Iteration 779: Achieved Loss=> 42.7270261785
Gradients: w=>0.2660662064, b=>-4.5903601163
Iteration 780: Achieved Loss=> 42.7058896012
Gradients: w=>0.2659247574, b=>-4.5879197399
Iteration 781: Achieved Loss=> 42.6847754916
Gradients: w=>0.2657833836, b=>-4.5854806610
Iteration 782: Achieved Loss=> 42.6636838259
Gradients: w=>0.2656420850, b=>-4.5830428787
Iteration 783: Achieved Loss=> 42.6426145802
Gradients: w=>0.2655008616, b=>-4.5806063924
Iteration 784: Achieved Loss=> 42.6215677306
Gradients: w=>0.2653597131, b=>-4.5781712015
Iteration 785: Achieved Loss=> 42.6005432534
Gradients: w=>0.2652186398, b=>-4.5757373051
Iteration 786: Achieved Loss=> 42.5795411248
Gradients: w=>0.2650776414, b=>-4.5733047027
Iteration 787: Achieved Loss=> 42.5585613209
Gradients: w=>0.2649367180, b=>-4.5708733936
Iteration 788: Achieved Loss=> 42.5376038182
Gradients: w=>0.2647958695, b=>-4.5684433770
Iteration 789: Achieved Loss=> 42.5166685929
Gradients: w=>0.2646550959, b=>-4.5660146523
Iteration 790: Achieved Loss=> 42.4957556212
Gradients: w=>0.2645143971, b=>-4.5635872187
Iteration 791: Achieved Loss=> 42.4748648796
Gradients: w=>0.2643737731, b=>-4.5611610757
Iteration 792: Achieved Loss=> 42.4539963444
Gradients: w=>0.2642332239, b=>-4.5587362224
Iteration 793: Achieved Loss=> 42.4331499920
Gradients: w=>0.2640927494, b=>-4.5563126583
Iteration 794: Achieved Loss=> 42.4123257988
Gradients: w=>0.2639523496, b=>-4.5538903827
Iteration 795: Achieved Loss=> 42.3915237414
Gradients: w=>0.2638120245, b=>-4.5514693948
Iteration 796: Achieved Loss=> 42.3707437960
Gradients: w=>0.2636717739, b=>-4.5490496939
Iteration 797: Achieved Loss=> 42.3499859393
Gradients: w=>0.2635315979, b=>-4.5466312795
Iteration 798: Achieved Loss=> 42.3292501478
Gradients: w=>0.2633914964, b=>-4.5442141507
Iteration 799: Achieved Loss=> 42.3085363979
Gradients: w=>0.2632514694, b=>-4.5417983070
Iteration 800: Achieved Loss=> 42.2878446664
Gradients: w=>0.2631115168, b=>-4.5393837476
Iteration 801: Achieved Loss=> 42.2671749297
Gradients: w=>0.2629716387, b=>-4.5369704719
Iteration 802: Achieved Loss=> 42.2465271645
Gradients: w=>0.2628318349, b=>-4.5345584791
Iteration 803: Achieved Loss=> 42.2259013475
Gradients: w=>0.2626921054, b=>-4.5321477686
Iteration 804: Achieved Loss=> 42.2052974552
Gradients: w=>0.2625524502, b=>-4.5297383398
Iteration 805: Achieved Loss=> 42.1847154645
Gradients: w=>0.2624128693, b=>-4.5273301918
Iteration 806: Achieved Loss=> 42.1641553519
Gradients: w=>0.2622733625, b=>-4.5249233241
Iteration 807: Achieved Loss=> 42.1436170943
Gradients: w=>0.2621339300, b=>-4.5225177360
Iteration 808: Achieved Loss=> 42.1231006685
Gradients: w=>0.2619945715, b=>-4.5201134268
Iteration 809: Achieved Loss=> 42.1026060512
Gradients: w=>0.2618552872, b=>-4.5177103957
Iteration 810: Achieved Loss=> 42.0821332192
Gradients: w=>0.2617160769, b=>-4.5153086422
Iteration 811: Achieved Loss=> 42.0616821494
Gradients: w=>0.2615769406, b=>-4.5129081655
Iteration 812: Achieved Loss=> 42.0412528187
Gradients: w=>0.2614378782, b=>-4.5105089650
Iteration 813: Achieved Loss=> 42.0208452039
Gradients: w=>0.2612988898, b=>-4.5081110400
Iteration 814: Achieved Loss=> 42.0004592820
Gradients: w=>0.2611599753, b=>-4.5057143898
Iteration 815: Achieved Loss=> 41.9800950299
Gradients: w=>0.2610211347, b=>-4.5033190137
Iteration 816: Achieved Loss=> 41.9597524245
Gradients: w=>0.2608823678, b=>-4.5009249111
Iteration 817: Achieved Loss=> 41.9394314429
Gradients: w=>0.2607436747, b=>-4.4985320813
Iteration 818: Achieved Loss=> 41.9191320621
Gradients: w=>0.2606050554, b=>-4.4961405235
Iteration 819: Achieved Loss=> 41.8988542591
Gradients: w=>0.2604665098, b=>-4.4937502372
Iteration 820: Achieved Loss=> 41.8785980109
Gradients: w=>0.2603280378, b=>-4.4913612217
Iteration 821: Achieved Loss=> 41.8583632947
Gradients: w=>0.2601896394, b=>-4.4889734762
Iteration 822: Achieved Loss=> 41.8381500876
Gradients: w=>0.2600513146, b=>-4.4865870001
Iteration 823: Achieved Loss=> 41.8179583667
Gradients: w=>0.2599130633, b=>-4.4842017927
Iteration 824: Achieved Loss=> 41.7977881092
Gradients: w=>0.2597748856, b=>-4.4818178534
Iteration 825: Achieved Loss=> 41.7776392922
Gradients: w=>0.2596367813, b=>-4.4794351815
Iteration 826: Achieved Loss=> 41.7575118930
Gradients: w=>0.2594987504, b=>-4.4770537762
Iteration 827: Achieved Loss=> 41.7374058888
Gradients: w=>0.2593607929, b=>-4.4746736370
Iteration 828: Achieved Loss=> 41.7173212568
Gradients: w=>0.2592229087, b=>-4.4722947632
Iteration 829: Achieved Loss=> 41.6972579744
Gradients: w=>0.2590850979, b=>-4.4699171540
Iteration 830: Achieved Loss=> 41.6772160189
Gradients: w=>0.2589473603, b=>-4.4675408088
Iteration 831: Achieved Loss=> 41.6571953674
Gradients: w=>0.2588096959, b=>-4.4651657270
Iteration 832: Achieved Loss=> 41.6371959976
Gradients: w=>0.2586721047, b=>-4.4627919079
Iteration 833: Achieved Loss=> 41.6172178866
Gradients: w=>0.2585345867, b=>-4.4604193507
Iteration 834: Achieved Loss=> 41.5972610119
Gradients: w=>0.2583971418, b=>-4.4580480549
Iteration 835: Achieved Loss=> 41.5773253510
Gradients: w=>0.2582597699, b=>-4.4556780197
Iteration 836: Achieved Loss=> 41.5574108813
Gradients: w=>0.2581224711, b=>-4.4533092445
Iteration 837: Achieved Loss=> 41.5375175802
Gradients: w=>0.2579852453, b=>-4.4509417286
Iteration 838: Achieved Loss=> 41.5176454252
Gradients: w=>0.2578480924, b=>-4.4485754713
Iteration 839: Achieved Loss=> 41.4977943940
Gradients: w=>0.2577110124, b=>-4.4462104721
Iteration 840: Achieved Loss=> 41.4779644640
Gradients: w=>0.2575740054, b=>-4.4438467301
Iteration 841: Achieved Loss=> 41.4581556127
Gradients: w=>0.2574370711, b=>-4.4414842448
Iteration 842: Achieved Loss=> 41.4383678179
Gradients: w=>0.2573002097, b=>-4.4391230154
Iteration 843: Achieved Loss=> 41.4186010571
Gradients: w=>0.2571634210, b=>-4.4367630414
Iteration 844: Achieved Loss=> 41.3988553079
Gradients: w=>0.2570267050, b=>-4.4344043220
Iteration 845: Achieved Loss=> 41.3791305480
Gradients: w=>0.2568900617, b=>-4.4320468565
Iteration 846: Achieved Loss=> 41.3594267551
Gradients: w=>0.2567534911, b=>-4.4296906444
Iteration 847: Achieved Loss=> 41.3397439069
Gradients: w=>0.2566169930, b=>-4.4273356849
Iteration 848: Achieved Loss=> 41.3200819812
Gradients: w=>0.2564805676, b=>-4.4249819773
Iteration 849: Achieved Loss=> 41.3004409557
Gradients: w=>0.2563442146, b=>-4.4226295211
Iteration 850: Achieved Loss=> 41.2808208082
Gradients: w=>0.2562079342, b=>-4.4202783155
Iteration 851: Achieved Loss=> 41.2612215165
Gradients: w=>0.2560717262, b=>-4.4179283598
Iteration 852: Achieved Loss=> 41.2416430584
Gradients: w=>0.2559355906, b=>-4.4155796535
Iteration 853: Achieved Loss=> 41.2220854118
Gradients: w=>0.2557995274, b=>-4.4132321958
Iteration 854: Achieved Loss=> 41.2025485546
Gradients: w=>0.2556635365, b=>-4.4108859861
Iteration 855: Achieved Loss=> 41.1830324646
Gradients: w=>0.2555276179, b=>-4.4085410237
Iteration 856: Achieved Loss=> 41.1635371198
Gradients: w=>0.2553917716, b=>-4.4061973080
Iteration 857: Achieved Loss=> 41.1440624982
Gradients: w=>0.2552559975, b=>-4.4038548383
Iteration 858: Achieved Loss=> 41.1246085776
Gradients: w=>0.2551202955, b=>-4.4015136139
Iteration 859: Achieved Loss=> 41.1051753362
Gradients: w=>0.2549846658, b=>-4.3991736341
Iteration 860: Achieved Loss=> 41.0857627519
Gradients: w=>0.2548491081, b=>-4.3968348984
Iteration 861: Achieved Loss=> 41.0663708028
Gradients: w=>0.2547136225, b=>-4.3944974060
Iteration 862: Achieved Loss=> 41.0469994668
Gradients: w=>0.2545782089, b=>-4.3921611563
Iteration 863: Achieved Loss=> 41.0276487222
Gradients: w=>0.2544428673, b=>-4.3898261486
Iteration 864: Achieved Loss=> 41.0083185471
Gradients: w=>0.2543075977, b=>-4.3874923823
Iteration 865: Achieved Loss=> 40.9890089195
Gradients: w=>0.2541724000, b=>-4.3851598567
Iteration 866: Achieved Loss=> 40.9697198176
Gradients: w=>0.2540372741, b=>-4.3828285711
Iteration 867: Achieved Loss=> 40.9504512196
Gradients: w=>0.2539022201, b=>-4.3804985249
Iteration 868: Achieved Loss=> 40.9312031038
Gradients: w=>0.2537672379, b=>-4.3781697174
Iteration 869: Achieved Loss=> 40.9119754483
Gradients: w=>0.2536323274, b=>-4.3758421480
Iteration 870: Achieved Loss=> 40.8927682313
Gradients: w=>0.2534974887, b=>-4.3735158160
Iteration 871: Achieved Loss=> 40.8735814313
Gradients: w=>0.2533627216, b=>-4.3711907207
Iteration 872: Achieved Loss=> 40.8544150264
Gradients: w=>0.2532280262, b=>-4.3688668616
Iteration 873: Achieved Loss=> 40.8352689949
Gradients: w=>0.2530934025, b=>-4.3665442379
Iteration 874: Achieved Loss=> 40.8161433153
Gradients: w=>0.2529588502, b=>-4.3642228489
Iteration 875: Achieved Loss=> 40.7970379659
Gradients: w=>0.2528243695, b=>-4.3619026941
Iteration 876: Achieved Loss=> 40.7779529251
Gradients: w=>0.2526899603, b=>-4.3595837727
Iteration 877: Achieved Loss=> 40.7588881713
Gradients: w=>0.2525556226, b=>-4.3572660842
Iteration 878: Achieved Loss=> 40.7398436828
Gradients: w=>0.2524213563, b=>-4.3549496278
Iteration 879: Achieved Loss=> 40.7208194383
Gradients: w=>0.2522871613, b=>-4.3526344029
Iteration 880: Achieved Loss=> 40.7018154162
Gradients: w=>0.2521530377, b=>-4.3503204088
Iteration 881: Achieved Loss=> 40.6828315949
Gradients: w=>0.2520189855, b=>-4.3480076450
Iteration 882: Achieved Loss=> 40.6638679530
Gradients: w=>0.2518850044, b=>-4.3456961106
Iteration 883: Achieved Loss=> 40.6449244691
Gradients: w=>0.2517510946, b=>-4.3433858052
Iteration 884: Achieved Loss=> 40.6260011217
Gradients: w=>0.2516172560, b=>-4.3410767280
Iteration 885: Achieved Loss=> 40.6070978894
Gradients: w=>0.2514834886, b=>-4.3387688784
Iteration 886: Achieved Loss=> 40.5882147508
Gradients: w=>0.2513497922, b=>-4.3364622556
Iteration 887: Achieved Loss=> 40.5693516846
Gradients: w=>0.2512161670, b=>-4.3341568592
Iteration 888: Achieved Loss=> 40.5505086695
Gradients: w=>0.2510826127, b=>-4.3318526884
Iteration 889: Achieved Loss=> 40.5316856840
Gradients: w=>0.2509491295, b=>-4.3295497425
Iteration 890: Achieved Loss=> 40.5128827071
Gradients: w=>0.2508157172, b=>-4.3272480210
Iteration 891: Achieved Loss=> 40.4940997172
Gradients: w=>0.2506823759, b=>-4.3249475231
Iteration 892: Achieved Loss=> 40.4753366933
Gradients: w=>0.2505491055, b=>-4.3226482483
Iteration 893: Achieved Loss=> 40.4565936141
Gradients: w=>0.2504159059, b=>-4.3203501958
Iteration 894: Achieved Loss=> 40.4378704584
Gradients: w=>0.2502827771, b=>-4.3180533650
Iteration 895: Achieved Loss=> 40.4191672050
Gradients: w=>0.2501497191, b=>-4.3157577553
Iteration 896: Achieved Loss=> 40.4004838328
Gradients: w=>0.2500167318, b=>-4.3134633660
Iteration 897: Achieved Loss=> 40.3818203206
Gradients: w=>0.2498838152, b=>-4.3111701965
Iteration 898: Achieved Loss=> 40.3631766473
Gradients: w=>0.2497509693, b=>-4.3088782461
Iteration 899: Achieved Loss=> 40.3445527918
Gradients: w=>0.2496181941, b=>-4.3065875141
Iteration 900: Achieved Loss=> 40.3259487331
Gradients: w=>0.2494854894, b=>-4.3042980000
Iteration 901: Achieved Loss=> 40.3073644501
Gradients: w=>0.2493528552, b=>-4.3020097031
Iteration 902: Achieved Loss=> 40.2887999218
Gradients: w=>0.2492202916, b=>-4.2997226227
Iteration 903: Achieved Loss=> 40.2702551273
Gradients: w=>0.2490877984, b=>-4.2974367581
Iteration 904: Achieved Loss=> 40.2517300454
Gradients: w=>0.2489553757, b=>-4.2951521088
Iteration 905: Achieved Loss=> 40.2332246553
Gradients: w=>0.2488230234, b=>-4.2928686741
Iteration 906: Achieved Loss=> 40.2147389360
Gradients: w=>0.2486907414, b=>-4.2905864534
Iteration 907: Achieved Loss=> 40.1962728667
Gradients: w=>0.2485585298, b=>-4.2883054459
Iteration 908: Achieved Loss=> 40.1778264264
Gradients: w=>0.2484263885, b=>-4.2860256511
Iteration 909: Achieved Loss=> 40.1593995943
Gradients: w=>0.2482943174, b=>-4.2837470683
Iteration 910: Achieved Loss=> 40.1409923495
Gradients: w=>0.2481623165, b=>-4.2814696969
Iteration 911: Achieved Loss=> 40.1226046713
Gradients: w=>0.2480303858, b=>-4.2791935362
Iteration 912: Achieved Loss=> 40.1042365387
Gradients: w=>0.2478985252, b=>-4.2769185856
Iteration 913: Achieved Loss=> 40.0858879311
Gradients: w=>0.2477667347, b=>-4.2746448444
Iteration 914: Achieved Loss=> 40.0675588276
Gradients: w=>0.2476350143, b=>-4.2723723120
Iteration 915: Achieved Loss=> 40.0492492076
Gradients: w=>0.2475033640, b=>-4.2701009877
Iteration 916: Achieved Loss=> 40.0309590504
Gradients: w=>0.2473717836, b=>-4.2678308709
Iteration 917: Achieved Loss=> 40.0126883351
Gradients: w=>0.2472402732, b=>-4.2655619611
Iteration 918: Achieved Loss=> 39.9944370413
Gradients: w=>0.2471088326, b=>-4.2632942574
Iteration 919: Achieved Loss=> 39.9762051482
Gradients: w=>0.2469774620, b=>-4.2610277593
Iteration 920: Achieved Loss=> 39.9579926352
Gradients: w=>0.2468461612, b=>-4.2587624661
Iteration 921: Achieved Loss=> 39.9397994818
Gradients: w=>0.2467149302, b=>-4.2564983773
Iteration 922: Achieved Loss=> 39.9216256672
Gradients: w=>0.2465837690, b=>-4.2542354921
Iteration 923: Achieved Loss=> 39.9034711711
Gradients: w=>0.2464526775, b=>-4.2519738099
Iteration 924: Achieved Loss=> 39.8853359728
Gradients: w=>0.2463216557, b=>-4.2497133301
Iteration 925: Achieved Loss=> 39.8672200518
Gradients: w=>0.2461907035, b=>-4.2474540521
Iteration 926: Achieved Loss=> 39.8491233877
Gradients: w=>0.2460598210, b=>-4.2451959751
Iteration 927: Achieved Loss=> 39.8310459599
Gradients: w=>0.2459290080, b=>-4.2429390987
Iteration 928: Achieved Loss=> 39.8129877481
Gradients: w=>0.2457982646, b=>-4.2406834220
Iteration 929: Achieved Loss=> 39.7949487317
Gradients: w=>0.2456675907, b=>-4.2384289445
Iteration 930: Achieved Loss=> 39.7769288904
Gradients: w=>0.2455369863, b=>-4.2361756656
Iteration 931: Achieved Loss=> 39.7589282039
Gradients: w=>0.2454064513, b=>-4.2339235846
Iteration 932: Achieved Loss=> 39.7409466517
Gradients: w=>0.2452759857, b=>-4.2316727009
Iteration 933: Achieved Loss=> 39.7229842135
Gradients: w=>0.2451455894, b=>-4.2294230138
Iteration 934: Achieved Loss=> 39.7050408690
Gradients: w=>0.2450152625, b=>-4.2271745227
Iteration 935: Achieved Loss=> 39.6871165979
Gradients: w=>0.2448850049, b=>-4.2249272269
Iteration 936: Achieved Loss=> 39.6692113799
Gradients: w=>0.2447548165, b=>-4.2226811259
Iteration 937: Achieved Loss=> 39.6513251948
Gradients: w=>0.2446246973, b=>-4.2204362190
Iteration 938: Achieved Loss=> 39.6334580223
Gradients: w=>0.2444946473, b=>-4.2181925056
Iteration 939: Achieved Loss=> 39.6156098422
Gradients: w=>0.2443646665, b=>-4.2159499850
Iteration 940: Achieved Loss=> 39.5977806344
Gradients: w=>0.2442347547, b=>-4.2137086566
Iteration 941: Achieved Loss=> 39.5799703786
Gradients: w=>0.2441049120, b=>-4.2114685197
Iteration 942: Achieved Loss=> 39.5621790548
Gradients: w=>0.2439751383, b=>-4.2092295738
Iteration 943: Achieved Loss=> 39.5444066427
Gradients: w=>0.2438454337, b=>-4.2069918181
Iteration 944: Achieved Loss=> 39.5266531224
Gradients: w=>0.2437157980, b=>-4.2047552521
Iteration 945: Achieved Loss=> 39.5089184737
Gradients: w=>0.2435862312, b=>-4.2025198752
Iteration 946: Achieved Loss=> 39.4912026765
Gradients: w=>0.2434567332, b=>-4.2002856866
Iteration 947: Achieved Loss=> 39.4735057109
Gradients: w=>0.2433273042, b=>-4.1980526858
Iteration 948: Achieved Loss=> 39.4558275567
Gradients: w=>0.2431979439, b=>-4.1958208722
Iteration 949: Achieved Loss=> 39.4381681940
Gradients: w=>0.2430686524, b=>-4.1935902450
Iteration 950: Achieved Loss=> 39.4205276029
Gradients: w=>0.2429394297, b=>-4.1913608037
Iteration 951: Achieved Loss=> 39.4029057634
Gradients: w=>0.2428102756, b=>-4.1891325476
Iteration 952: Achieved Loss=> 39.3853026555
Gradients: w=>0.2426811902, b=>-4.1869054762
Iteration 953: Achieved Loss=> 39.3677182593
Gradients: w=>0.2425521734, b=>-4.1846795887
Iteration 954: Achieved Loss=> 39.3501525549
Gradients: w=>0.2424232253, b=>-4.1824548846
Iteration 955: Achieved Loss=> 39.3326055226
Gradients: w=>0.2422943456, b=>-4.1802313632
Iteration 956: Achieved Loss=> 39.3150771423
Gradients: w=>0.2421655345, b=>-4.1780090239
Iteration 957: Achieved Loss=> 39.2975673944
Gradients: w=>0.2420367919, b=>-4.1757878661
Iteration 958: Achieved Loss=> 39.2800762589
Gradients: w=>0.2419081177, b=>-4.1735678891
Iteration 959: Achieved Loss=> 39.2626037161
Gradients: w=>0.2417795119, b=>-4.1713490923
Iteration 960: Achieved Loss=> 39.2451497463
Gradients: w=>0.2416509745, b=>-4.1691314751
Iteration 961: Achieved Loss=> 39.2277143297
Gradients: w=>0.2415225054, b=>-4.1669150368
Iteration 962: Achieved Loss=> 39.2102974466
Gradients: w=>0.2413941047, b=>-4.1646997769
Iteration 963: Achieved Loss=> 39.1928990772
Gradients: w=>0.2412657721, b=>-4.1624856946
Iteration 964: Achieved Loss=> 39.1755192019
Gradients: w=>0.2411375078, b=>-4.1602727895
Iteration 965: Achieved Loss=> 39.1581578011
Gradients: w=>0.2410093117, b=>-4.1580610607
Iteration 966: Achieved Loss=> 39.1408148551
Gradients: w=>0.2408811838, b=>-4.1558505079
Iteration 967: Achieved Loss=> 39.1234903442
Gradients: w=>0.2407531240, b=>-4.1536411302
Iteration 968: Achieved Loss=> 39.1061842490
Gradients: w=>0.2406251322, b=>-4.1514329270
Iteration 969: Achieved Loss=> 39.0888965497
Gradients: w=>0.2404972085, b=>-4.1492258979
Iteration 970: Achieved Loss=> 39.0716272270
Gradients: w=>0.2403693528, b=>-4.1470200420
Iteration 971: Achieved Loss=> 39.0543762611
Gradients: w=>0.2402415651, b=>-4.1448153589
Iteration 972: Achieved Loss=> 39.0371436326
Gradients: w=>0.2401138453, b=>-4.1426118478
Iteration 973: Achieved Loss=> 39.0199293221
Gradients: w=>0.2399861934, b=>-4.1404095082
Iteration 974: Achieved Loss=> 39.0027333100
Gradients: w=>0.2398586094, b=>-4.1382083394
Iteration 975: Achieved Loss=> 38.9855555769
Gradients: w=>0.2397310932, b=>-4.1360083408
Iteration 976: Achieved Loss=> 38.9683961034
Gradients: w=>0.2396036448, b=>-4.1338095118
Iteration 977: Achieved Loss=> 38.9512548700
Gradients: w=>0.2394762641, b=>-4.1316118518
Iteration 978: Achieved Loss=> 38.9341318574
Gradients: w=>0.2393489512, b=>-4.1294153601
Iteration 979: Achieved Loss=> 38.9170270462
Gradients: w=>0.2392217059, b=>-4.1272200362
Iteration 980: Achieved Loss=> 38.8999404171
Gradients: w=>0.2390945283, b=>-4.1250258793
Iteration 981: Achieved Loss=> 38.8828719506
Gradients: w=>0.2389674183, b=>-4.1228328890
Iteration 982: Achieved Loss=> 38.8658216276
Gradients: w=>0.2388403759, b=>-4.1206410644
Iteration 983: Achieved Loss=> 38.8487894287
Gradients: w=>0.2387134011, b=>-4.1184504052
Iteration 984: Achieved Loss=> 38.8317753347
Gradients: w=>0.2385864937, b=>-4.1162609105
Iteration 985: Achieved Loss=> 38.8147793263
Gradients: w=>0.2384596538, b=>-4.1140725799
Iteration 986: Achieved Loss=> 38.7978013843
Gradients: w=>0.2383328813, b=>-4.1118854126
Iteration 987: Achieved Loss=> 38.7808414895
Gradients: w=>0.2382061762, b=>-4.1096994081
Iteration 988: Achieved Loss=> 38.7638996227
Gradients: w=>0.2380795385, b=>-4.1075145658
Iteration 989: Achieved Loss=> 38.7469757647
Gradients: w=>0.2379529681, b=>-4.1053308849
Iteration 990: Achieved Loss=> 38.7300698965
Gradients: w=>0.2378264650, b=>-4.1031483650
Iteration 991: Achieved Loss=> 38.7131819988
Gradients: w=>0.2377000292, b=>-4.1009670054
Iteration 992: Achieved Loss=> 38.6963120526
Gradients: w=>0.2375736605, b=>-4.0987868055
Iteration 993: Achieved Loss=> 38.6794600388
Gradients: w=>0.2374473591, b=>-4.0966077646
Iteration 994: Achieved Loss=> 38.6626259383
Gradients: w=>0.2373211248, b=>-4.0944298822
Iteration 995: Achieved Loss=> 38.6458097321
Gradients: w=>0.2371949576, b=>-4.0922531576
Iteration 996: Achieved Loss=> 38.6290114012
Gradients: w=>0.2370688575, b=>-4.0900775902
Iteration 997: Achieved Loss=> 38.6122309266
Gradients: w=>0.2369428244, b=>-4.0879031794
Iteration 998: Achieved Loss=> 38.5954682892
Gradients: w=>0.2368168583, b=>-4.0857299246
Iteration 999: Achieved Loss=> 38.5787234702
Gradients: w=>0.2366909592, b=>-4.0835578251
Iteration 1000: Achieved Loss=> 38.5619964506
Gradients: w=>0.2365651270, b=>-4.0813868805
Iteration 1001: Achieved Loss=> 38.5452872114
Gradients: w=>0.2364393617, b=>-4.0792170899
Iteration 1002: Achieved Loss=> 38.5285957338
Gradients: w=>0.2363136633, b=>-4.0770484529
Iteration 1003: Achieved Loss=> 38.5119219989
Gradients: w=>0.2361880317, b=>-4.0748809688
Iteration 1004: Achieved Loss=> 38.4952659878
Gradients: w=>0.2360624669, b=>-4.0727146370
Iteration 1005: Achieved Loss=> 38.4786276817
Gradients: w=>0.2359369689, b=>-4.0705494569
Iteration 1006: Achieved Loss=> 38.4620070617
Gradients: w=>0.2358115375, b=>-4.0683854278
Iteration 1007: Achieved Loss=> 38.4454041091
Gradients: w=>0.2356861729, b=>-4.0662225493
Iteration 1008: Achieved Loss=> 38.4288188051
Gradients: w=>0.2355608749, b=>-4.0640608206
Iteration 1009: Achieved Loss=> 38.4122511310
Gradients: w=>0.2354356435, b=>-4.0619002411
Iteration 1010: Achieved Loss=> 38.3957010679
Gradients: w=>0.2353104787, b=>-4.0597408102
Iteration 1011: Achieved Loss=> 38.3791685972
Gradients: w=>0.2351853804, b=>-4.0575825274
Iteration 1012: Achieved Loss=> 38.3626537001
Gradients: w=>0.2350603486, b=>-4.0554253920
Iteration 1013: Achieved Loss=> 38.3461563581
Gradients: w=>0.2349353833, b=>-4.0532694034
Iteration 1014: Achieved Loss=> 38.3296765523
Gradients: w=>0.2348104845, b=>-4.0511145609
Iteration 1015: Achieved Loss=> 38.3132142643
Gradients: w=>0.2346856520, b=>-4.0489608641
Iteration 1016: Achieved Loss=> 38.2967694753
Gradients: w=>0.2345608859, b=>-4.0468083122
Iteration 1017: Achieved Loss=> 38.2803421667
Gradients: w=>0.2344361861, b=>-4.0446569047
Iteration 1018: Achieved Loss=> 38.2639323201
Gradients: w=>0.2343115527, b=>-4.0425066409
Iteration 1019: Achieved Loss=> 38.2475399168
Gradients: w=>0.2341869855, b=>-4.0403575203
Iteration 1020: Achieved Loss=> 38.2311649382
Gradients: w=>0.2340624845, b=>-4.0382095422
Iteration 1021: Achieved Loss=> 38.2148073659
Gradients: w=>0.2339380497, b=>-4.0360627061
Iteration 1022: Achieved Loss=> 38.1984671813
Gradients: w=>0.2338136810, b=>-4.0339170113
Iteration 1023: Achieved Loss=> 38.1821443661
Gradients: w=>0.2336893785, b=>-4.0317724572
Iteration 1024: Achieved Loss=> 38.1658389016
Gradients: w=>0.2335651420, b=>-4.0296290432
Iteration 1025: Achieved Loss=> 38.1495507695
Gradients: w=>0.2334409716, b=>-4.0274867687
Iteration 1026: Achieved Loss=> 38.1332799513
Gradients: w=>0.2333168673, b=>-4.0253456331
Iteration 1027: Achieved Loss=> 38.1170264286
Gradients: w=>0.2331928289, b=>-4.0232056358
Iteration 1028: Achieved Loss=> 38.1007901832
Gradients: w=>0.2330688564, b=>-4.0210667762
Iteration 1029: Achieved Loss=> 38.0845711964
Gradients: w=>0.2329449498, b=>-4.0189290537
Iteration 1030: Achieved Loss=> 38.0683694502
Gradients: w=>0.2328211091, b=>-4.0167924676
Iteration 1031: Achieved Loss=> 38.0521849260
Gradients: w=>0.2326973343, b=>-4.0146570175
Iteration 1032: Achieved Loss=> 38.0360176057
Gradients: w=>0.2325736253, b=>-4.0125227026
Iteration 1033: Achieved Loss=> 38.0198674708
Gradients: w=>0.2324499820, b=>-4.0103895223
Iteration 1034: Achieved Loss=> 38.0037345032
Gradients: w=>0.2323264044, b=>-4.0082574762
Iteration 1035: Achieved Loss=> 37.9876186847
Gradients: w=>0.2322028926, b=>-4.0061265635
Iteration 1036: Achieved Loss=> 37.9715199969
Gradients: w=>0.2320794464, b=>-4.0039967836
Iteration 1037: Achieved Loss=> 37.9554384216
Gradients: w=>0.2319560658, b=>-4.0018681360
Iteration 1038: Achieved Loss=> 37.9393739407
Gradients: w=>0.2318327509, b=>-3.9997406201
Iteration 1039: Achieved Loss=> 37.9233265361
Gradients: w=>0.2317095015, b=>-3.9976142352
Iteration 1040: Achieved Loss=> 37.9072961894
Gradients: w=>0.2315863176, b=>-3.9954889808
Iteration 1041: Achieved Loss=> 37.8912828827
Gradients: w=>0.2314631992, b=>-3.9933648562
Iteration 1042: Achieved Loss=> 37.8752865978
Gradients: w=>0.2313401462, b=>-3.9912418608
Iteration 1043: Achieved Loss=> 37.8593073167
Gradients: w=>0.2312171587, b=>-3.9891199942
Iteration 1044: Achieved Loss=> 37.8433450211
Gradients: w=>0.2310942366, b=>-3.9869992555
Iteration 1045: Achieved Loss=> 37.8273996931
Gradients: w=>0.2309713798, b=>-3.9848796443
Iteration 1046: Achieved Loss=> 37.8114713147
Gradients: w=>0.2308485883, b=>-3.9827611600
Iteration 1047: Achieved Loss=> 37.7955598678
Gradients: w=>0.2307258621, b=>-3.9806438019
Iteration 1048: Achieved Loss=> 37.7796653344
Gradients: w=>0.2306032012, b=>-3.9785275695
Iteration 1049: Achieved Loss=> 37.7637876966
Gradients: w=>0.2304806054, b=>-3.9764124621
Iteration 1050: Achieved Loss=> 37.7479269364
Gradients: w=>0.2303580749, b=>-3.9742984792
Iteration 1051: Achieved Loss=> 37.7320830358
Gradients: w=>0.2302356094, b=>-3.9721856201
Iteration 1052: Achieved Loss=> 37.7162559770
Gradients: w=>0.2301132091, b=>-3.9700738843
Iteration 1053: Achieved Loss=> 37.7004457420
Gradients: w=>0.2299908739, b=>-3.9679632712
Iteration 1054: Achieved Loss=> 37.6846523129
Gradients: w=>0.2298686037, b=>-3.9658537801
Iteration 1055: Achieved Loss=> 37.6688756719
Gradients: w=>0.2297463985, b=>-3.9637454105
Iteration 1056: Achieved Loss=> 37.6531158011
Gradients: w=>0.2296242582, b=>-3.9616381618
Iteration 1057: Achieved Loss=> 37.6373726828
Gradients: w=>0.2295021829, b=>-3.9595320334
Iteration 1058: Achieved Loss=> 37.6216462991
Gradients: w=>0.2293801725, b=>-3.9574270246
Iteration 1059: Achieved Loss=> 37.6059366321
Gradients: w=>0.2292582270, b=>-3.9553231349
Iteration 1060: Achieved Loss=> 37.5902436642
Gradients: w=>0.2291363463, b=>-3.9532203637
Iteration 1061: Achieved Loss=> 37.5745673776
Gradients: w=>0.2290145304, b=>-3.9511187104
Iteration 1062: Achieved Loss=> 37.5589077546
Gradients: w=>0.2288927792, b=>-3.9490181745
Iteration 1063: Achieved Loss=> 37.5432647774
Gradients: w=>0.2287710928, b=>-3.9469187552
Iteration 1064: Achieved Loss=> 37.5276384283
Gradients: w=>0.2286494710, b=>-3.9448204520
Iteration 1065: Achieved Loss=> 37.5120286898
Gradients: w=>0.2285279140, b=>-3.9427232644
Iteration 1066: Achieved Loss=> 37.4964355440
Gradients: w=>0.2284064215, b=>-3.9406271917
Iteration 1067: Achieved Loss=> 37.4808589734
Gradients: w=>0.2282849936, b=>-3.9385322333
Iteration 1068: Achieved Loss=> 37.4652989604
Gradients: w=>0.2281636303, b=>-3.9364383887
Iteration 1069: Achieved Loss=> 37.4497554874
Gradients: w=>0.2280423315, b=>-3.9343456572
Iteration 1070: Achieved Loss=> 37.4342285368
Gradients: w=>0.2279210972, b=>-3.9322540383
Iteration 1071: Achieved Loss=> 37.4187180909
Gradients: w=>0.2277999274, b=>-3.9301635313
Iteration 1072: Achieved Loss=> 37.4032241324
Gradients: w=>0.2276788220, b=>-3.9280741358
Iteration 1073: Achieved Loss=> 37.3877466435
Gradients: w=>0.2275577809, b=>-3.9259858510
Iteration 1074: Achieved Loss=> 37.3722856070
Gradients: w=>0.2274368042, b=>-3.9238986764
Iteration 1075: Achieved Loss=> 37.3568410051
Gradients: w=>0.2273158918, b=>-3.9218126114
Iteration 1076: Achieved Loss=> 37.3414128206
Gradients: w=>0.2271950437, b=>-3.9197276555
Iteration 1077: Achieved Loss=> 37.3260010359
Gradients: w=>0.2270742599, b=>-3.9176438079
Iteration 1078: Achieved Loss=> 37.3106056336
Gradients: w=>0.2269535402, b=>-3.9155610682
Iteration 1079: Achieved Loss=> 37.2952265963
Gradients: w=>0.2268328847, b=>-3.9134794358
Iteration 1080: Achieved Loss=> 37.2798639066
Gradients: w=>0.2267122934, b=>-3.9113989100
Iteration 1081: Achieved Loss=> 37.2645175470
Gradients: w=>0.2265917662, b=>-3.9093194903
Iteration 1082: Achieved Loss=> 37.2491875004
Gradients: w=>0.2264713031, b=>-3.9072411760
Iteration 1083: Achieved Loss=> 37.2338737492
Gradients: w=>0.2263509040, b=>-3.9051639667
Iteration 1084: Achieved Loss=> 37.2185762763
Gradients: w=>0.2262305689, b=>-3.9030878616
Iteration 1085: Achieved Loss=> 37.2032950642
Gradients: w=>0.2261102978, b=>-3.9010128603
Iteration 1086: Achieved Loss=> 37.1880300958
Gradients: w=>0.2259900906, b=>-3.8989389622
Iteration 1087: Achieved Loss=> 37.1727813536
Gradients: w=>0.2258699473, b=>-3.8968661665
Iteration 1088: Achieved Loss=> 37.1575488206
Gradients: w=>0.2257498679, b=>-3.8947944729
Iteration 1089: Achieved Loss=> 37.1423324794
Gradients: w=>0.2256298524, b=>-3.8927238806
Iteration 1090: Achieved Loss=> 37.1271323129
Gradients: w=>0.2255099006, b=>-3.8906543891
Iteration 1091: Achieved Loss=> 37.1119483038
Gradients: w=>0.2253900126, b=>-3.8885859978
Iteration 1092: Achieved Loss=> 37.0967804350
Gradients: w=>0.2252701884, b=>-3.8865187061
Iteration 1093: Achieved Loss=> 37.0816286894
Gradients: w=>0.2251504278, b=>-3.8844525135
Iteration 1094: Achieved Loss=> 37.0664930497
Gradients: w=>0.2250307310, b=>-3.8823874193
Iteration 1095: Achieved Loss=> 37.0513734988
Gradients: w=>0.2249110977, b=>-3.8803234229
Iteration 1096: Achieved Loss=> 37.0362700198
Gradients: w=>0.2247915281, b=>-3.8782605239
Iteration 1097: Achieved Loss=> 37.0211825954
Gradients: w=>0.2246720220, b=>-3.8761987216
Iteration 1098: Achieved Loss=> 37.0061112086
Gradients: w=>0.2245525794, b=>-3.8741380154
Iteration 1099: Achieved Loss=> 36.9910558424
Gradients: w=>0.2244332004, b=>-3.8720784047
Iteration 1100: Achieved Loss=> 36.9760164797
Gradients: w=>0.2243138848, b=>-3.8700198889
Iteration 1101: Achieved Loss=> 36.9609931036
Gradients: w=>0.2241946327, b=>-3.8679624676
Iteration 1102: Achieved Loss=> 36.9459856969
Gradients: w=>0.2240754439, b=>-3.8659061400
Iteration 1103: Achieved Loss=> 36.9309942429
Gradients: w=>0.2239563185, b=>-3.8638509056
Iteration 1104: Achieved Loss=> 36.9160187244
Gradients: w=>0.2238372565, b=>-3.8617967639
Iteration 1105: Achieved Loss=> 36.9010591246
Gradients: w=>0.2237182577, b=>-3.8597437142
Iteration 1106: Achieved Loss=> 36.8861154265
Gradients: w=>0.2235993222, b=>-3.8576917560
Iteration 1107: Achieved Loss=> 36.8711876133
Gradients: w=>0.2234804500, b=>-3.8556408886
Iteration 1108: Achieved Loss=> 36.8562756680
Gradients: w=>0.2233616409, b=>-3.8535911116
Iteration 1109: Achieved Loss=> 36.8413795738
Gradients: w=>0.2232428950, b=>-3.8515424243
Iteration 1110: Achieved Loss=> 36.8264993138
Gradients: w=>0.2231242122, b=>-3.8494948261
Iteration 1111: Achieved Loss=> 36.8116348712
Gradients: w=>0.2230055926, b=>-3.8474483165
Iteration 1112: Achieved Loss=> 36.7967862292
Gradients: w=>0.2228870359, b=>-3.8454028948
Iteration 1113: Achieved Loss=> 36.7819533710
Gradients: w=>0.2227685423, b=>-3.8433585606
Iteration 1114: Achieved Loss=> 36.7671362798
Gradients: w=>0.2226501118, b=>-3.8413153133
Iteration 1115: Achieved Loss=> 36.7523349389
Gradients: w=>0.2225317441, b=>-3.8392731521
Iteration 1116: Achieved Loss=> 36.7375493314
Gradients: w=>0.2224134394, b=>-3.8372320767
Iteration 1117: Achieved Loss=> 36.7227794408
Gradients: w=>0.2222951976, b=>-3.8351920863
Iteration 1118: Achieved Loss=> 36.7080252502
Gradients: w=>0.2221770187, b=>-3.8331531805
Iteration 1119: Achieved Loss=> 36.6932867431
Gradients: w=>0.2220589025, b=>-3.8311153586
Iteration 1120: Achieved Loss=> 36.6785639026
Gradients: w=>0.2219408492, b=>-3.8290786201
Iteration 1121: Achieved Loss=> 36.6638567122
Gradients: w=>0.2218228586, b=>-3.8270429643
Iteration 1122: Achieved Loss=> 36.6491651553
Gradients: w=>0.2217049308, b=>-3.8250083908
Iteration 1123: Achieved Loss=> 36.6344892151
Gradients: w=>0.2215870656, b=>-3.8229748990
Iteration 1124: Achieved Loss=> 36.6198288752
Gradients: w=>0.2214692632, b=>-3.8209424882
Iteration 1125: Achieved Loss=> 36.6051841189
Gradients: w=>0.2213515233, b=>-3.8189111579
Iteration 1126: Achieved Loss=> 36.5905549297
Gradients: w=>0.2212338460, b=>-3.8168809075
Iteration 1127: Achieved Loss=> 36.5759412910
Gradients: w=>0.2211162313, b=>-3.8148517364
Iteration 1128: Achieved Loss=> 36.5613431862
Gradients: w=>0.2209986792, b=>-3.8128236441
Iteration 1129: Achieved Loss=> 36.5467605990
Gradients: w=>0.2208811895, b=>-3.8107966301
Iteration 1130: Achieved Loss=> 36.5321935127
Gradients: w=>0.2207637623, b=>-3.8087706936
Iteration 1131: Achieved Loss=> 36.5176419110
Gradients: w=>0.2206463975, b=>-3.8067458342
Iteration 1132: Achieved Loss=> 36.5031057772
Gradients: w=>0.2205290951, b=>-3.8047220513
Iteration 1133: Achieved Loss=> 36.4885850951
Gradients: w=>0.2204118550, b=>-3.8026993442
Iteration 1134: Achieved Loss=> 36.4740798482
Gradients: w=>0.2202946773, b=>-3.8006777125
Iteration 1135: Achieved Loss=> 36.4595900200
Gradients: w=>0.2201775619, b=>-3.7986571556
Iteration 1136: Achieved Loss=> 36.4451155942
Gradients: w=>0.2200605088, b=>-3.7966376729
Iteration 1137: Achieved Loss=> 36.4306565544
Gradients: w=>0.2199435178, b=>-3.7946192637
Iteration 1138: Achieved Loss=> 36.4162128843
Gradients: w=>0.2198265891, b=>-3.7926019277
Iteration 1139: Achieved Loss=> 36.4017845674
Gradients: w=>0.2197097226, b=>-3.7905856641
Iteration 1140: Achieved Loss=> 36.3873715876
Gradients: w=>0.2195929181, b=>-3.7885704724
Iteration 1141: Achieved Loss=> 36.3729739285
Gradients: w=>0.2194761758, b=>-3.7865563520
Iteration 1142: Achieved Loss=> 36.3585915737
Gradients: w=>0.2193594955, b=>-3.7845433024
Iteration 1143: Achieved Loss=> 36.3442245071
Gradients: w=>0.2192428773, b=>-3.7825313230
Iteration 1144: Achieved Loss=> 36.3298727124
Gradients: w=>0.2191263210, b=>-3.7805204133
Iteration 1145: Achieved Loss=> 36.3155361734
Gradients: w=>0.2190098267, b=>-3.7785105726
Iteration 1146: Achieved Loss=> 36.3012148738
Gradients: w=>0.2188933944, b=>-3.7765018004
Iteration 1147: Achieved Loss=> 36.2869087974
Gradients: w=>0.2187770240, b=>-3.7744940961
Iteration 1148: Achieved Loss=> 36.2726179281
Gradients: w=>0.2186607154, b=>-3.7724874592
Iteration 1149: Achieved Loss=> 36.2583422496
Gradients: w=>0.2185444686, b=>-3.7704818891
Iteration 1150: Achieved Loss=> 36.2440817459
Gradients: w=>0.2184282837, b=>-3.7684773852
Iteration 1151: Achieved Loss=> 36.2298364008
Gradients: w=>0.2183121605, b=>-3.7664739469
Iteration 1152: Achieved Loss=> 36.2156061983
Gradients: w=>0.2181960991, b=>-3.7644715738
Iteration 1153: Achieved Loss=> 36.2013911221
Gradients: w=>0.2180800993, b=>-3.7624702651
Iteration 1154: Achieved Loss=> 36.1871911562
Gradients: w=>0.2179641612, b=>-3.7604700204
Iteration 1155: Achieved Loss=> 36.1730062846
Gradients: w=>0.2178482848, b=>-3.7584708391
Iteration 1156: Achieved Loss=> 36.1588364913
Gradients: w=>0.2177324700, b=>-3.7564727207
Iteration 1157: Achieved Loss=> 36.1446817601
Gradients: w=>0.2176167167, b=>-3.7544756645
Iteration 1158: Achieved Loss=> 36.1305420751
Gradients: w=>0.2175010250, b=>-3.7524796700
Iteration 1159: Achieved Loss=> 36.1164174203
Gradients: w=>0.2173853947, b=>-3.7504847366
Iteration 1160: Achieved Loss=> 36.1023077796
Gradients: w=>0.2172698260, b=>-3.7484908638
Iteration 1161: Achieved Loss=> 36.0882131373
Gradients: w=>0.2171543187, b=>-3.7464980510
Iteration 1162: Achieved Loss=> 36.0741334772
Gradients: w=>0.2170388728, b=>-3.7445062976
Iteration 1163: Achieved Loss=> 36.0600687835
Gradients: w=>0.2169234883, b=>-3.7425156031
Iteration 1164: Achieved Loss=> 36.0460190403
Gradients: w=>0.2168081651, b=>-3.7405259670
Iteration 1165: Achieved Loss=> 36.0319842317
Gradients: w=>0.2166929032, b=>-3.7385373886
Iteration 1166: Achieved Loss=> 36.0179643418
Gradients: w=>0.2165777026, b=>-3.7365498673
Iteration 1167: Achieved Loss=> 36.0039593547
Gradients: w=>0.2164625633, b=>-3.7345634027
Iteration 1168: Achieved Loss=> 35.9899692546
Gradients: w=>0.2163474851, b=>-3.7325779942
Iteration 1169: Achieved Loss=> 35.9759940257
Gradients: w=>0.2162324681, b=>-3.7305936412
Iteration 1170: Achieved Loss=> 35.9620336522
Gradients: w=>0.2161175123, b=>-3.7286103431
Iteration 1171: Achieved Loss=> 35.9480881182
Gradients: w=>0.2160026176, b=>-3.7266280994
Iteration 1172: Achieved Loss=> 35.9341574081
Gradients: w=>0.2158877840, b=>-3.7246469095
Iteration 1173: Achieved Loss=> 35.9202415060
Gradients: w=>0.2157730114, b=>-3.7226667728
Iteration 1174: Achieved Loss=> 35.9063403962
Gradients: w=>0.2156582999, b=>-3.7206876889
Iteration 1175: Achieved Loss=> 35.8924540630
Gradients: w=>0.2155436493, b=>-3.7187096571
Iteration 1176: Achieved Loss=> 35.8785824907
Gradients: w=>0.2154290597, b=>-3.7167326769
Iteration 1177: Achieved Loss=> 35.8647256636
Gradients: w=>0.2153145310, b=>-3.7147567478
Iteration 1178: Achieved Loss=> 35.8508835660
Gradients: w=>0.2152000632, b=>-3.7127818691
Iteration 1179: Achieved Loss=> 35.8370561822
Gradients: w=>0.2150856562, b=>-3.7108080403
Iteration 1180: Achieved Loss=> 35.8232434966
Gradients: w=>0.2149713101, b=>-3.7088352608
Iteration 1181: Achieved Loss=> 35.8094454937
Gradients: w=>0.2148570247, b=>-3.7068635301
Iteration 1182: Achieved Loss=> 35.7956621577
Gradients: w=>0.2147428001, b=>-3.7048928477
Iteration 1183: Achieved Loss=> 35.7818934731
Gradients: w=>0.2146286363, b=>-3.7029232129
Iteration 1184: Achieved Loss=> 35.7681394244
Gradients: w=>0.2145145331, b=>-3.7009546253
Iteration 1185: Achieved Loss=> 35.7543999959
Gradients: w=>0.2144004906, b=>-3.6989870842
Iteration 1186: Achieved Loss=> 35.7406751721
Gradients: w=>0.2142865087, b=>-3.6970205891
Iteration 1187: Achieved Loss=> 35.7269649375
Gradients: w=>0.2141725874, b=>-3.6950551395
Iteration 1188: Achieved Loss=> 35.7132692766
Gradients: w=>0.2140587267, b=>-3.6930907348
Iteration 1189: Achieved Loss=> 35.6995881739
Gradients: w=>0.2139449265, b=>-3.6911273744
Iteration 1190: Achieved Loss=> 35.6859216139
Gradients: w=>0.2138311868, b=>-3.6891650578
Iteration 1191: Achieved Loss=> 35.6722695812
Gradients: w=>0.2137175076, b=>-3.6872037844
Iteration 1192: Achieved Loss=> 35.6586320603
Gradients: w=>0.2136038888, b=>-3.6852435537
Iteration 1193: Achieved Loss=> 35.6450090358
Gradients: w=>0.2134903304, b=>-3.6832843651
Iteration 1194: Achieved Loss=> 35.6314004923
Gradients: w=>0.2133768324, b=>-3.6813262181
Iteration 1195: Achieved Loss=> 35.6178064144
Gradients: w=>0.2132633947, b=>-3.6793691121
Iteration 1196: Achieved Loss=> 35.6042267867
Gradients: w=>0.2131500174, b=>-3.6774130465
Iteration 1197: Achieved Loss=> 35.5906615938
Gradients: w=>0.2130367003, b=>-3.6754580209
Iteration 1198: Achieved Loss=> 35.5771108205
Gradients: w=>0.2129234434, b=>-3.6735040346
Iteration 1199: Achieved Loss=> 35.5635744513
Gradients: w=>0.2128102468, b=>-3.6715510871
Iteration 1200: Achieved Loss=> 35.5500524710
Gradients: w=>0.2126971103, b=>-3.6695991778
Iteration 1201: Achieved Loss=> 35.5365448643
Gradients: w=>0.2125840340, b=>-3.6676483062
Iteration 1202: Achieved Loss=> 35.5230516159
Gradients: w=>0.2124710178, b=>-3.6656984718
Iteration 1203: Achieved Loss=> 35.5095727106
Gradients: w=>0.2123580617, b=>-3.6637496740
Iteration 1204: Achieved Loss=> 35.4961081330
Gradients: w=>0.2122451656, b=>-3.6618019122
Iteration 1205: Achieved Loss=> 35.4826578680
Gradients: w=>0.2121323296, b=>-3.6598551859
Iteration 1206: Achieved Loss=> 35.4692219003
Gradients: w=>0.2120195535, b=>-3.6579094946
Iteration 1207: Achieved Loss=> 35.4558002148
Gradients: w=>0.2119068374, b=>-3.6559648376
Iteration 1208: Achieved Loss=> 35.4423927962
Gradients: w=>0.2117941813, b=>-3.6540212145
Iteration 1209: Achieved Loss=> 35.4289996295
Gradients: w=>0.2116815850, b=>-3.6520786246
Iteration 1210: Achieved Loss=> 35.4156206994
Gradients: w=>0.2115690485, b=>-3.6501370675
Iteration 1211: Achieved Loss=> 35.4022559908
Gradients: w=>0.2114565719, b=>-3.6481965426
Iteration 1212: Achieved Loss=> 35.3889054886
Gradients: w=>0.2113441551, b=>-3.6462570494
Iteration 1213: Achieved Loss=> 35.3755691777
Gradients: w=>0.2112317981, b=>-3.6443185872
Iteration 1214: Achieved Loss=> 35.3622470430
Gradients: w=>0.2111195008, b=>-3.6423811556
Iteration 1215: Achieved Loss=> 35.3489390695
Gradients: w=>0.2110072632, b=>-3.6404447539
Iteration 1216: Achieved Loss=> 35.3356452421
Gradients: w=>0.2108950853, b=>-3.6385093818
Iteration 1217: Achieved Loss=> 35.3223655457
Gradients: w=>0.2107829669, b=>-3.6365750385
Iteration 1218: Achieved Loss=> 35.3090999654
Gradients: w=>0.2106709083, b=>-3.6346417236
Iteration 1219: Achieved Loss=> 35.2958484861
Gradients: w=>0.2105589091, b=>-3.6327094365
Iteration 1220: Achieved Loss=> 35.2826110928
Gradients: w=>0.2104469696, b=>-3.6307781766
Iteration 1221: Achieved Loss=> 35.2693877706
Gradients: w=>0.2103350895, b=>-3.6288479435
Iteration 1222: Achieved Loss=> 35.2561785045
Gradients: w=>0.2102232689, b=>-3.6269187366
Iteration 1223: Achieved Loss=> 35.2429832796
Gradients: w=>0.2101115078, b=>-3.6249905552
Iteration 1224: Achieved Loss=> 35.2298020809
Gradients: w=>0.2099998060, b=>-3.6230633990
Iteration 1225: Achieved Loss=> 35.2166348936
Gradients: w=>0.2098881637, b=>-3.6211372673
Iteration 1226: Achieved Loss=> 35.2034817027
Gradients: w=>0.2097765807, b=>-3.6192121596
Iteration 1227: Achieved Loss=> 35.1903424934
Gradients: w=>0.2096650570, b=>-3.6172880753
Iteration 1228: Achieved Loss=> 35.1772172507
Gradients: w=>0.2095535926, b=>-3.6153650139
Iteration 1229: Achieved Loss=> 35.1641059599
Gradients: w=>0.2094421875, b=>-3.6134429749
Iteration 1230: Achieved Loss=> 35.1510086062
Gradients: w=>0.2093308416, b=>-3.6115219577
Iteration 1231: Achieved Loss=> 35.1379251746
Gradients: w=>0.2092195549, b=>-3.6096019618
Iteration 1232: Achieved Loss=> 35.1248556505
Gradients: w=>0.2091083274, b=>-3.6076829866
Iteration 1233: Achieved Loss=> 35.1118000190
Gradients: w=>0.2089971590, b=>-3.6057650316
Iteration 1234: Achieved Loss=> 35.0987582653
Gradients: w=>0.2088860497, b=>-3.6038480962
Iteration 1235: Achieved Loss=> 35.0857303748
Gradients: w=>0.2087749994, b=>-3.6019321799
Iteration 1236: Achieved Loss=> 35.0727163326
Gradients: w=>0.2086640082, b=>-3.6000172822
Iteration 1237: Achieved Loss=> 35.0597161241
Gradients: w=>0.2085530761, b=>-3.5981034025
Iteration 1238: Achieved Loss=> 35.0467297345
Gradients: w=>0.2084422028, b=>-3.5961905403
Iteration 1239: Achieved Loss=> 35.0337571492
Gradients: w=>0.2083313886, b=>-3.5942786950
Iteration 1240: Achieved Loss=> 35.0207983534
Gradients: w=>0.2082206332, b=>-3.5923678662
Iteration 1241: Achieved Loss=> 35.0078533326
Gradients: w=>0.2081099367, b=>-3.5904580531
Iteration 1242: Achieved Loss=> 34.9949220721
Gradients: w=>0.2079992991, b=>-3.5885492554
Iteration 1243: Achieved Loss=> 34.9820045572
Gradients: w=>0.2078887203, b=>-3.5866414725
Iteration 1244: Achieved Loss=> 34.9691007733
Gradients: w=>0.2077782003, b=>-3.5847347038
Iteration 1245: Achieved Loss=> 34.9562107060
Gradients: w=>0.2076677390, b=>-3.5828289488
Iteration 1246: Achieved Loss=> 34.9433343404
Gradients: w=>0.2075573364, b=>-3.5809242069
Iteration 1247: Achieved Loss=> 34.9304716622
Gradients: w=>0.2074469926, b=>-3.5790204777
Iteration 1248: Achieved Loss=> 34.9176226568
Gradients: w=>0.2073367074, b=>-3.5771177606
Iteration 1249: Achieved Loss=> 34.9047873095
Gradients: w=>0.2072264808, b=>-3.5752160550
Iteration 1250: Achieved Loss=> 34.8919656060
Gradients: w=>0.2071163129, b=>-3.5733153604
Iteration 1251: Achieved Loss=> 34.8791575316
Gradients: w=>0.2070062035, b=>-3.5714156763
Iteration 1252: Achieved Loss=> 34.8663630720
Gradients: w=>0.2068961526, b=>-3.5695170021
Iteration 1253: Achieved Loss=> 34.8535822126
Gradients: w=>0.2067861603, b=>-3.5676193373
Iteration 1254: Achieved Loss=> 34.8408149390
Gradients: w=>0.2066762264, b=>-3.5657226813
Iteration 1255: Achieved Loss=> 34.8280612367
Gradients: w=>0.2065663510, b=>-3.5638270337
Iteration 1256: Achieved Loss=> 34.8153210914
Gradients: w=>0.2064565340, b=>-3.5619323938
Iteration 1257: Achieved Loss=> 34.8025944885
Gradients: w=>0.2063467754, b=>-3.5600387612
Iteration 1258: Achieved Loss=> 34.7898814138
Gradients: w=>0.2062370751, b=>-3.5581461354
Iteration 1259: Achieved Loss=> 34.7771818527
Gradients: w=>0.2061274331, b=>-3.5562545157
Iteration 1260: Achieved Loss=> 34.7644957911
Gradients: w=>0.2060178495, b=>-3.5543639016
Iteration 1261: Achieved Loss=> 34.7518232145
Gradients: w=>0.2059083240, b=>-3.5524742927
Iteration 1262: Achieved Loss=> 34.7391641085
Gradients: w=>0.2057988569, b=>-3.5505856883
Iteration 1263: Achieved Loss=> 34.7265184589
Gradients: w=>0.2056894479, b=>-3.5486980879
Iteration 1264: Achieved Loss=> 34.7138862514
Gradients: w=>0.2055800970, b=>-3.5468114911
Iteration 1265: Achieved Loss=> 34.7012674716
Gradients: w=>0.2054708044, b=>-3.5449258973
Iteration 1266: Achieved Loss=> 34.6886621053
Gradients: w=>0.2053615698, b=>-3.5430413058
Iteration 1267: Achieved Loss=> 34.6760701383
Gradients: w=>0.2052523932, b=>-3.5411577163
Iteration 1268: Achieved Loss=> 34.6634915562
Gradients: w=>0.2051432748, b=>-3.5392751282
Iteration 1269: Achieved Loss=> 34.6509263449
Gradients: w=>0.2050342143, b=>-3.5373935409
Iteration 1270: Achieved Loss=> 34.6383744902
Gradients: w=>0.2049252118, b=>-3.5355129539
Iteration 1271: Achieved Loss=> 34.6258359778
Gradients: w=>0.2048162673, b=>-3.5336333667
Iteration 1272: Achieved Loss=> 34.6133107936
Gradients: w=>0.2047073807, b=>-3.5317547787
Iteration 1273: Achieved Loss=> 34.6007989234
Gradients: w=>0.2045985520, b=>-3.5298771895
Iteration 1274: Achieved Loss=> 34.5883003531
Gradients: w=>0.2044897811, b=>-3.5280005984
Iteration 1275: Achieved Loss=> 34.5758150685
Gradients: w=>0.2043810680, b=>-3.5261250050
Iteration 1276: Achieved Loss=> 34.5633430554
Gradients: w=>0.2042724128, b=>-3.5242504087
Iteration 1277: Achieved Loss=> 34.5508842999
Gradients: w=>0.2041638153, b=>-3.5223768090
Iteration 1278: Achieved Loss=> 34.5384387877
Gradients: w=>0.2040552756, b=>-3.5205042054
Iteration 1279: Achieved Loss=> 34.5260065049
Gradients: w=>0.2039467935, b=>-3.5186325973
Iteration 1280: Achieved Loss=> 34.5135874373
Gradients: w=>0.2038383691, b=>-3.5167619842
Iteration 1281: Achieved Loss=> 34.5011815709
Gradients: w=>0.2037300024, b=>-3.5148923656
Iteration 1282: Achieved Loss=> 34.4887888917
Gradients: w=>0.2036216933, b=>-3.5130237409
Iteration 1283: Achieved Loss=> 34.4764093856
Gradients: w=>0.2035134417, b=>-3.5111561097
Iteration 1284: Achieved Loss=> 34.4640430387
Gradients: w=>0.2034052477, b=>-3.5092894713
Iteration 1285: Achieved Loss=> 34.4516898369
Gradients: w=>0.2032971113, b=>-3.5074238253
Iteration 1286: Achieved Loss=> 34.4393497664
Gradients: w=>0.2031890323, b=>-3.5055591712
Iteration 1287: Achieved Loss=> 34.4270228131
Gradients: w=>0.2030810108, b=>-3.5036955083
Iteration 1288: Achieved Loss=> 34.4147089630
Gradients: w=>0.2029730467, b=>-3.5018328362
Iteration 1289: Achieved Loss=> 34.4024082024
Gradients: w=>0.2028651400, b=>-3.4999711544
Iteration 1290: Achieved Loss=> 34.3901205171
Gradients: w=>0.2027572906, b=>-3.4981104623
Iteration 1291: Achieved Loss=> 34.3778458935
Gradients: w=>0.2026494986, b=>-3.4962507594
Iteration 1292: Achieved Loss=> 34.3655843175
Gradients: w=>0.2025417639, b=>-3.4943920452
Iteration 1293: Achieved Loss=> 34.3533357752
Gradients: w=>0.2024340865, b=>-3.4925343191
Iteration 1294: Achieved Loss=> 34.3411002530
Gradients: w=>0.2023264663, b=>-3.4906775807
Iteration 1295: Achieved Loss=> 34.3288777368
Gradients: w=>0.2022189034, b=>-3.4888218293
Iteration 1296: Achieved Loss=> 34.3166682129
Gradients: w=>0.2021113976, b=>-3.4869670645
Iteration 1297: Achieved Loss=> 34.3044716675
Gradients: w=>0.2020039490, b=>-3.4851132858
Iteration 1298: Achieved Loss=> 34.2922880867
Gradients: w=>0.2018965575, b=>-3.4832604926
Iteration 1299: Achieved Loss=> 34.2801174568
Gradients: w=>0.2017892231, b=>-3.4814086844
Iteration 1300: Achieved Loss=> 34.2679597641
Gradients: w=>0.2016819457, b=>-3.4795578607
Iteration 1301: Achieved Loss=> 34.2558149947
Gradients: w=>0.2015747254, b=>-3.4777080210
Iteration 1302: Achieved Loss=> 34.2436831349
Gradients: w=>0.2014675621, b=>-3.4758591646
Iteration 1303: Achieved Loss=> 34.2315641711
Gradients: w=>0.2013604558, b=>-3.4740112912
Iteration 1304: Achieved Loss=> 34.2194580894
Gradients: w=>0.2012534064, b=>-3.4721644002
Iteration 1305: Achieved Loss=> 34.2073648763
Gradients: w=>0.2011464139, b=>-3.4703184910
Iteration 1306: Achieved Loss=> 34.1952845180
Gradients: w=>0.2010394783, b=>-3.4684735632
Iteration 1307: Achieved Loss=> 34.1832170008
Gradients: w=>0.2009325995, b=>-3.4666296161
Iteration 1308: Achieved Loss=> 34.1711623112
Gradients: w=>0.2008257776, b=>-3.4647866494
Iteration 1309: Achieved Loss=> 34.1591204354
Gradients: w=>0.2007190124, b=>-3.4629446625
Iteration 1310: Achieved Loss=> 34.1470913599
Gradients: w=>0.2006123040, b=>-3.4611036548
Iteration 1311: Achieved Loss=> 34.1350750711
Gradients: w=>0.2005056524, b=>-3.4592636259
Iteration 1312: Achieved Loss=> 34.1230715553
Gradients: w=>0.2003990574, b=>-3.4574245752
Iteration 1313: Achieved Loss=> 34.1110807990
Gradients: w=>0.2002925191, b=>-3.4555865021
Iteration 1314: Achieved Loss=> 34.0991027886
Gradients: w=>0.2001860375, b=>-3.4537494063
Iteration 1315: Achieved Loss=> 34.0871375106
Gradients: w=>0.2000796124, b=>-3.4519132871
Iteration 1316: Achieved Loss=> 34.0751849514
Gradients: w=>0.1999732440, b=>-3.4500781440
Iteration 1317: Achieved Loss=> 34.0632450976
Gradients: w=>0.1998669321, b=>-3.4482439766
Iteration 1318: Achieved Loss=> 34.0513179355
Gradients: w=>0.1997606767, b=>-3.4464107842
Iteration 1319: Achieved Loss=> 34.0394034518
Gradients: w=>0.1996544778, b=>-3.4445785665
Iteration 1320: Achieved Loss=> 34.0275016329
Gradients: w=>0.1995483353, b=>-3.4427473228
Iteration 1321: Achieved Loss=> 34.0156124654
Gradients: w=>0.1994422493, b=>-3.4409170526
Iteration 1322: Achieved Loss=> 34.0037359359
Gradients: w=>0.1993362197, b=>-3.4390877555
Iteration 1323: Achieved Loss=> 33.9918720308
Gradients: w=>0.1992302464, b=>-3.4372594309
Iteration 1324: Achieved Loss=> 33.9800207368
Gradients: w=>0.1991243295, b=>-3.4354320783
Iteration 1325: Achieved Loss=> 33.9681820405
Gradients: w=>0.1990184689, b=>-3.4336056971
Iteration 1326: Achieved Loss=> 33.9563559284
Gradients: w=>0.1989126646, b=>-3.4317802870
Iteration 1327: Achieved Loss=> 33.9445423873
Gradients: w=>0.1988069165, b=>-3.4299558472
Iteration 1328: Achieved Loss=> 33.9327414037
Gradients: w=>0.1987012246, b=>-3.4281323774
Iteration 1329: Achieved Loss=> 33.9209529643
Gradients: w=>0.1985955890, b=>-3.4263098770
Iteration 1330: Achieved Loss=> 33.9091770558
Gradients: w=>0.1984900094, b=>-3.4244883455
Iteration 1331: Achieved Loss=> 33.8974136648
Gradients: w=>0.1983844861, b=>-3.4226677824
Iteration 1332: Achieved Loss=> 33.8856627781
Gradients: w=>0.1982790188, b=>-3.4208481871
Iteration 1333: Achieved Loss=> 33.8739243822
Gradients: w=>0.1981736076, b=>-3.4190295592
Iteration 1334: Achieved Loss=> 33.8621984641
Gradients: w=>0.1980682524, b=>-3.4172118982
Iteration 1335: Achieved Loss=> 33.8504850103
Gradients: w=>0.1979629532, b=>-3.4153952034
Iteration 1336: Achieved Loss=> 33.8387840077
Gradients: w=>0.1978577101, b=>-3.4135794745
Iteration 1337: Achieved Loss=> 33.8270954431
Gradients: w=>0.1977525228, b=>-3.4117647109
Iteration 1338: Achieved Loss=> 33.8154193031
Gradients: w=>0.1976473915, b=>-3.4099509120
Iteration 1339: Achieved Loss=> 33.8037555746
Gradients: w=>0.1975423161, b=>-3.4081380775
Iteration 1340: Achieved Loss=> 33.7921042444
Gradients: w=>0.1974372965, b=>-3.4063262067
Iteration 1341: Achieved Loss=> 33.7804652993
Gradients: w=>0.1973323328, b=>-3.4045152991
Iteration 1342: Achieved Loss=> 33.7688387262
Gradients: w=>0.1972274249, b=>-3.4027053543
Iteration 1343: Achieved Loss=> 33.7572245119
Gradients: w=>0.1971225727, b=>-3.4008963716
Iteration 1344: Achieved Loss=> 33.7456226432
Gradients: w=>0.1970177763, b=>-3.3990883507
Iteration 1345: Achieved Loss=> 33.7340331071
Gradients: w=>0.1969130356, b=>-3.3972812910
Iteration 1346: Achieved Loss=> 33.7224558904
Gradients: w=>0.1968083506, b=>-3.3954751920
Iteration 1347: Achieved Loss=> 33.7108909800
Gradients: w=>0.1967037212, b=>-3.3936700532
Iteration 1348: Achieved Loss=> 33.6993383629
Gradients: w=>0.1965991475, b=>-3.3918658740
Iteration 1349: Achieved Loss=> 33.6877980260
Gradients: w=>0.1964946294, b=>-3.3900626540
Iteration 1350: Achieved Loss=> 33.6762699562
Gradients: w=>0.1963901668, b=>-3.3882603927
Iteration 1351: Achieved Loss=> 33.6647541405
Gradients: w=>0.1962857598, b=>-3.3864590894
Iteration 1352: Achieved Loss=> 33.6532505659
Gradients: w=>0.1961814082, b=>-3.3846587438
Iteration 1353: Achieved Loss=> 33.6417592193
Gradients: w=>0.1960771122, b=>-3.3828593554
Iteration 1354: Achieved Loss=> 33.6302800878
Gradients: w=>0.1959728715, b=>-3.3810609235
Iteration 1355: Achieved Loss=> 33.6188131584
Gradients: w=>0.1958686864, b=>-3.3792634477
Iteration 1356: Achieved Loss=> 33.6073584181
Gradients: w=>0.1957645565, b=>-3.3774669275
Iteration 1357: Achieved Loss=> 33.5959158539
Gradients: w=>0.1956604821, b=>-3.3756713624
Iteration 1358: Achieved Loss=> 33.5844854529
Gradients: w=>0.1955564630, b=>-3.3738767519
Iteration 1359: Achieved Loss=> 33.5730672022
Gradients: w=>0.1954524992, b=>-3.3720830955
Iteration 1360: Achieved Loss=> 33.5616610888
Gradients: w=>0.1953485906, b=>-3.3702903926
Iteration 1361: Achieved Loss=> 33.5502670999
Gradients: w=>0.1952447373, b=>-3.3684986428
Iteration 1362: Achieved Loss=> 33.5388852226
Gradients: w=>0.1951409392, b=>-3.3667078455
Iteration 1363: Achieved Loss=> 33.5275154439
Gradients: w=>0.1950371963, b=>-3.3649180003
Iteration 1364: Achieved Loss=> 33.5161577511
Gradients: w=>0.1949335085, b=>-3.3631291066
Iteration 1365: Achieved Loss=> 33.5048121313
Gradients: w=>0.1948298759, b=>-3.3613411639
Iteration 1366: Achieved Loss=> 33.4934785716
Gradients: w=>0.1947262984, b=>-3.3595541718
Iteration 1367: Achieved Loss=> 33.4821570593
Gradients: w=>0.1946227759, b=>-3.3577681297
Iteration 1368: Achieved Loss=> 33.4708475815
Gradients: w=>0.1945193084, b=>-3.3559830371
Iteration 1369: Achieved Loss=> 33.4595501254
Gradients: w=>0.1944158960, b=>-3.3541988935
Iteration 1370: Achieved Loss=> 33.4482646783
Gradients: w=>0.1943125385, b=>-3.3524156984
Iteration 1371: Achieved Loss=> 33.4369912274
Gradients: w=>0.1942092360, b=>-3.3506334513
Iteration 1372: Achieved Loss=> 33.4257297599
Gradients: w=>0.1941059884, b=>-3.3488521517
Iteration 1373: Achieved Loss=> 33.4144802631
Gradients: w=>0.1940027957, b=>-3.3470717991
Iteration 1374: Achieved Loss=> 33.4032427243
Gradients: w=>0.1938996579, b=>-3.3452923930
Iteration 1375: Achieved Loss=> 33.3920171308
Gradients: w=>0.1937965749, b=>-3.3435139329
Iteration 1376: Achieved Loss=> 33.3808034698
Gradients: w=>0.1936935467, b=>-3.3417364183
Iteration 1377: Achieved Loss=> 33.3696017287
Gradients: w=>0.1935905733, b=>-3.3399598486
Iteration 1378: Achieved Loss=> 33.3584118948
Gradients: w=>0.1934876546, b=>-3.3381842234
Iteration 1379: Achieved Loss=> 33.3472339555
Gradients: w=>0.1933847906, b=>-3.3364095422
Iteration 1380: Achieved Loss=> 33.3360678981
Gradients: w=>0.1932819813, b=>-3.3346358045
Iteration 1381: Achieved Loss=> 33.3249137100
Gradients: w=>0.1931792267, b=>-3.3328630098
Iteration 1382: Achieved Loss=> 33.3137713785
Gradients: w=>0.1930765267, b=>-3.3310911575
Iteration 1383: Achieved Loss=> 33.3026408911
Gradients: w=>0.1929738812, b=>-3.3293202472
Iteration 1384: Achieved Loss=> 33.2915222352
Gradients: w=>0.1928712904, b=>-3.3275502783
Iteration 1385: Achieved Loss=> 33.2804153981
Gradients: w=>0.1927687541, b=>-3.3257812505
Iteration 1386: Achieved Loss=> 33.2693203674
Gradients: w=>0.1926662723, b=>-3.3240131631
Iteration 1387: Achieved Loss=> 33.2582371305
Gradients: w=>0.1925638450, b=>-3.3222460156
Iteration 1388: Achieved Loss=> 33.2471656748
Gradients: w=>0.1924614722, b=>-3.3204798077
Iteration 1389: Achieved Loss=> 33.2361059879
Gradients: w=>0.1923591537, b=>-3.3187145387
Iteration 1390: Achieved Loss=> 33.2250580571
Gradients: w=>0.1922568897, b=>-3.3169502082
Iteration 1391: Achieved Loss=> 33.2140218701
Gradients: w=>0.1921546800, b=>-3.3151868156
Iteration 1392: Achieved Loss=> 33.2029974143
Gradients: w=>0.1920525247, b=>-3.3134243606
Iteration 1393: Achieved Loss=> 33.1919846773
Gradients: w=>0.1919504237, b=>-3.3116628425
Iteration 1394: Achieved Loss=> 33.1809836465
Gradients: w=>0.1918483770, b=>-3.3099022609
Iteration 1395: Achieved Loss=> 33.1699943097
Gradients: w=>0.1917463845, b=>-3.3081426152
Iteration 1396: Achieved Loss=> 33.1590166542
Gradients: w=>0.1916444462, b=>-3.3063839051
Iteration 1397: Achieved Loss=> 33.1480506678
Gradients: w=>0.1915425621, b=>-3.3046261299
Iteration 1398: Achieved Loss=> 33.1370963380
Gradients: w=>0.1914407322, b=>-3.3028692892
Iteration 1399: Achieved Loss=> 33.1261536524
Gradients: w=>0.1913389565, b=>-3.3011133825
Iteration 1400: Achieved Loss=> 33.1152225987
Gradients: w=>0.1912372348, b=>-3.2993584093
Iteration 1401: Achieved Loss=> 33.1043031644
Gradients: w=>0.1911355672, b=>-3.2976043691
Iteration 1402: Achieved Loss=> 33.0933953373
Gradients: w=>0.1910339537, b=>-3.2958512614
Iteration 1403: Achieved Loss=> 33.0824991049
Gradients: w=>0.1909323942, b=>-3.2940990857
Iteration 1404: Achieved Loss=> 33.0716144550
Gradients: w=>0.1908308887, b=>-3.2923478415
Iteration 1405: Achieved Loss=> 33.0607413753
Gradients: w=>0.1907294371, b=>-3.2905975284
Iteration 1406: Achieved Loss=> 33.0498798534
Gradients: w=>0.1906280395, b=>-3.2888481457
Iteration 1407: Achieved Loss=> 33.0390298771
Gradients: w=>0.1905266958, b=>-3.2870996931
Iteration 1408: Achieved Loss=> 33.0281914341
Gradients: w=>0.1904254059, b=>-3.2853521700
Iteration 1409: Achieved Loss=> 33.0173645121
Gradients: w=>0.1903241699, b=>-3.2836055759
Iteration 1410: Achieved Loss=> 33.0065490989
Gradients: w=>0.1902229878, b=>-3.2818599104
Iteration 1411: Achieved Loss=> 32.9957451822
Gradients: w=>0.1901218594, b=>-3.2801151730
Iteration 1412: Achieved Loss=> 32.9849527499
Gradients: w=>0.1900207848, b=>-3.2783713630
Iteration 1413: Achieved Loss=> 32.9741717897
Gradients: w=>0.1899197639, b=>-3.2766284802
Iteration 1414: Achieved Loss=> 32.9634022895
Gradients: w=>0.1898187967, b=>-3.2748865239
Iteration 1415: Achieved Loss=> 32.9526442370
Gradients: w=>0.1897178832, b=>-3.2731454937
Iteration 1416: Achieved Loss=> 32.9418976200
Gradients: w=>0.1896170234, b=>-3.2714053891
Iteration 1417: Achieved Loss=> 32.9311624265
Gradients: w=>0.1895162171, b=>-3.2696662096
Iteration 1418: Achieved Loss=> 32.9204386443
Gradients: w=>0.1894154645, b=>-3.2679279547
Iteration 1419: Achieved Loss=> 32.9097262613
Gradients: w=>0.1893147654, b=>-3.2661906239
Iteration 1420: Achieved Loss=> 32.8990252652
Gradients: w=>0.1892141199, b=>-3.2644542167
Iteration 1421: Achieved Loss=> 32.8883356441
Gradients: w=>0.1891135278, b=>-3.2627187326
Iteration 1422: Achieved Loss=> 32.8776573859
Gradients: w=>0.1890129893, b=>-3.2609841712
Iteration 1423: Achieved Loss=> 32.8669904784
Gradients: w=>0.1889125042, b=>-3.2592505319
Iteration 1424: Achieved Loss=> 32.8563349096
Gradients: w=>0.1888120725, b=>-3.2575178143
Iteration 1425: Achieved Loss=> 32.8456906674
Gradients: w=>0.1887116942, b=>-3.2557860178
Iteration 1426: Achieved Loss=> 32.8350577399
Gradients: w=>0.1886113693, b=>-3.2540551420
Iteration 1427: Achieved Loss=> 32.8244361149
Gradients: w=>0.1885110977, b=>-3.2523251864
Iteration 1428: Achieved Loss=> 32.8138257805
Gradients: w=>0.1884108794, b=>-3.2505961505
Iteration 1429: Achieved Loss=> 32.8032267247
Gradients: w=>0.1883107144, b=>-3.2488680339
Iteration 1430: Achieved Loss=> 32.7926389354
Gradients: w=>0.1882106026, b=>-3.2471408359
Iteration 1431: Achieved Loss=> 32.7820624007
Gradients: w=>0.1881105441, b=>-3.2454145561
Iteration 1432: Achieved Loss=> 32.7714971086
Gradients: w=>0.1880105388, b=>-3.2436891941
Iteration 1433: Achieved Loss=> 32.7609430473
Gradients: w=>0.1879105866, b=>-3.2419647494
Iteration 1434: Achieved Loss=> 32.7504002046
Gradients: w=>0.1878106875, b=>-3.2402412214
Iteration 1435: Achieved Loss=> 32.7398685688
Gradients: w=>0.1877108416, b=>-3.2385186097
Iteration 1436: Achieved Loss=> 32.7293481279
Gradients: w=>0.1876110488, b=>-3.2367969138
Iteration 1437: Achieved Loss=> 32.7188388700
Gradients: w=>0.1875113090, b=>-3.2350761333
Iteration 1438: Achieved Loss=> 32.7083407832
Gradients: w=>0.1874116222, b=>-3.2333562675
Iteration 1439: Achieved Loss=> 32.6978538557
Gradients: w=>0.1873119884, b=>-3.2316373160
Iteration 1440: Achieved Loss=> 32.6873780755
Gradients: w=>0.1872124076, b=>-3.2299192785
Iteration 1441: Achieved Loss=> 32.6769134309
Gradients: w=>0.1871128798, b=>-3.2282021542
Iteration 1442: Achieved Loss=> 32.6664599100
Gradients: w=>0.1870134048, b=>-3.2264859429
Iteration 1443: Achieved Loss=> 32.6560175009
Gradients: w=>0.1869139828, b=>-3.2247706439
Iteration 1444: Achieved Loss=> 32.6455861919
Gradients: w=>0.1868146136, b=>-3.2230562568
Iteration 1445: Achieved Loss=> 32.6351659712
Gradients: w=>0.1867152972, b=>-3.2213427812
Iteration 1446: Achieved Loss=> 32.6247568269
Gradients: w=>0.1866160336, b=>-3.2196302165
Iteration 1447: Achieved Loss=> 32.6143587473
Gradients: w=>0.1865168228, b=>-3.2179185623
Iteration 1448: Achieved Loss=> 32.6039717207
Gradients: w=>0.1864176647, b=>-3.2162078180
Iteration 1449: Achieved Loss=> 32.5935957352
Gradients: w=>0.1863185594, b=>-3.2144979832
Iteration 1450: Achieved Loss=> 32.5832307792
Gradients: w=>0.1862195067, b=>-3.2127890574
Iteration 1451: Achieved Loss=> 32.5728768410
Gradients: w=>0.1861205067, b=>-3.2110810401
Iteration 1452: Achieved Loss=> 32.5625339087
Gradients: w=>0.1860215593, b=>-3.2093739309
Iteration 1453: Achieved Loss=> 32.5522019708
Gradients: w=>0.1859226645, b=>-3.2076677292
Iteration 1454: Achieved Loss=> 32.5418810155
Gradients: w=>0.1858238223, b=>-3.2059624345
Iteration 1455: Achieved Loss=> 32.5315710312
Gradients: w=>0.1857250327, b=>-3.2042580465
Iteration 1456: Achieved Loss=> 32.5212720061
Gradients: w=>0.1856262956, b=>-3.2025545646
Iteration 1457: Achieved Loss=> 32.5109839287
Gradients: w=>0.1855276109, b=>-3.2008519883
Iteration 1458: Achieved Loss=> 32.5007067873
Gradients: w=>0.1854289788, b=>-3.1991503171
Iteration 1459: Achieved Loss=> 32.4904405703
Gradients: w=>0.1853303990, b=>-3.1974495506
Iteration 1460: Achieved Loss=> 32.4801852660
Gradients: w=>0.1852318717, b=>-3.1957496883
Iteration 1461: Achieved Loss=> 32.4699408630
Gradients: w=>0.1851333967, b=>-3.1940507296
Iteration 1462: Achieved Loss=> 32.4597073495
Gradients: w=>0.1850349741, b=>-3.1923526742
Iteration 1463: Achieved Loss=> 32.4494847140
Gradients: w=>0.1849366039, b=>-3.1906555216
Iteration 1464: Achieved Loss=> 32.4392729450
Gradients: w=>0.1848382859, b=>-3.1889592712
Iteration 1465: Achieved Loss=> 32.4290720308
Gradients: w=>0.1847400202, b=>-3.1872639225
Iteration 1466: Achieved Loss=> 32.4188819600
Gradients: w=>0.1846418067, b=>-3.1855694752
Iteration 1467: Achieved Loss=> 32.4087027211
Gradients: w=>0.1845436455, b=>-3.1838759287
Iteration 1468: Achieved Loss=> 32.3985343024
Gradients: w=>0.1844455364, b=>-3.1821832825
Iteration 1469: Achieved Loss=> 32.3883766926
Gradients: w=>0.1843474795, b=>-3.1804915362
Iteration 1470: Achieved Loss=> 32.3782298801
Gradients: w=>0.1842494747, b=>-3.1788006893
Iteration 1471: Achieved Loss=> 32.3680938534
Gradients: w=>0.1841515220, b=>-3.1771107412
Iteration 1472: Achieved Loss=> 32.3579686011
Gradients: w=>0.1840536214, b=>-3.1754216917
Iteration 1473: Achieved Loss=> 32.3478541118
Gradients: w=>0.1839557729, b=>-3.1737335400
Iteration 1474: Achieved Loss=> 32.3377503739
Gradients: w=>0.1838579764, b=>-3.1720462859
Iteration 1475: Achieved Loss=> 32.3276573761
Gradients: w=>0.1837602318, b=>-3.1703599287
Iteration 1476: Achieved Loss=> 32.3175751069
Gradients: w=>0.1836625392, b=>-3.1686744680
Iteration 1477: Achieved Loss=> 32.3075035550
Gradients: w=>0.1835648986, b=>-3.1669899034
Iteration 1478: Achieved Loss=> 32.2974427089
Gradients: w=>0.1834673099, b=>-3.1653062344
Iteration 1479: Achieved Loss=> 32.2873925573
Gradients: w=>0.1833697730, b=>-3.1636234604
Iteration 1480: Achieved Loss=> 32.2773530888
Gradients: w=>0.1832722880, b=>-3.1619415811
Iteration 1481: Achieved Loss=> 32.2673242920
Gradients: w=>0.1831748548, b=>-3.1602605959
Iteration 1482: Achieved Loss=> 32.2573061556
Gradients: w=>0.1830774734, b=>-3.1585805044
Iteration 1483: Achieved Loss=> 32.2472986683
Gradients: w=>0.1829801438, b=>-3.1569013060
Iteration 1484: Achieved Loss=> 32.2373018188
Gradients: w=>0.1828828660, b=>-3.1552230004
Iteration 1485: Achieved Loss=> 32.2273155956
Gradients: w=>0.1827856398, b=>-3.1535455870
Iteration 1486: Achieved Loss=> 32.2173399876
Gradients: w=>0.1826884654, b=>-3.1518690653
Iteration 1487: Achieved Loss=> 32.2073749835
Gradients: w=>0.1825913426, b=>-3.1501934350
Iteration 1488: Achieved Loss=> 32.1974205720
Gradients: w=>0.1824942714, b=>-3.1485186955
Iteration 1489: Achieved Loss=> 32.1874767418
Gradients: w=>0.1823972519, b=>-3.1468448463
Iteration 1490: Achieved Loss=> 32.1775434816
Gradients: w=>0.1823002839, b=>-3.1451718870
Iteration 1491: Achieved Loss=> 32.1676207804
Gradients: w=>0.1822033675, b=>-3.1434998171
Iteration 1492: Achieved Loss=> 32.1577086267
Gradients: w=>0.1821065026, b=>-3.1418286361
Iteration 1493: Achieved Loss=> 32.1478070094
Gradients: w=>0.1820096892, b=>-3.1401583435
Iteration 1494: Achieved Loss=> 32.1379159174
Gradients: w=>0.1819129272, b=>-3.1384889390
Iteration 1495: Achieved Loss=> 32.1280353394
Gradients: w=>0.1818162167, b=>-3.1368204219
Iteration 1496: Achieved Loss=> 32.1181652642
Gradients: w=>0.1817195576, b=>-3.1351527919
Iteration 1497: Achieved Loss=> 32.1083056807
Gradients: w=>0.1816229499, b=>-3.1334860484
Iteration 1498: Achieved Loss=> 32.0984565777
Gradients: w=>0.1815263936, b=>-3.1318201911
Iteration 1499: Achieved Loss=> 32.0886179441
Gradients: w=>0.1814298886, b=>-3.1301552193
Iteration 1500: Achieved Loss=> 32.0787897688
Gradients: w=>0.1813334349, b=>-3.1284911327
Iteration 1501: Achieved Loss=> 32.0689720406
Gradients: w=>0.1812370325, b=>-3.1268279308
Iteration 1502: Achieved Loss=> 32.0591647485
Gradients: w=>0.1811406813, b=>-3.1251656131
Iteration 1503: Achieved Loss=> 32.0493678813
Gradients: w=>0.1810443814, b=>-3.1235041792
Iteration 1504: Achieved Loss=> 32.0395814279
Gradients: w=>0.1809481326, b=>-3.1218436285
Iteration 1505: Achieved Loss=> 32.0298053774
Gradients: w=>0.1808519350, b=>-3.1201839606
Iteration 1506: Achieved Loss=> 32.0200397185
Gradients: w=>0.1807557886, b=>-3.1185251750
Iteration 1507: Achieved Loss=> 32.0102844404
Gradients: w=>0.1806596933, b=>-3.1168672713
Iteration 1508: Achieved Loss=> 32.0005395319
Gradients: w=>0.1805636490, b=>-3.1152102490
Iteration 1509: Achieved Loss=> 31.9908049821
Gradients: w=>0.1804676559, b=>-3.1135541076
Iteration 1510: Achieved Loss=> 31.9810807799
Gradients: w=>0.1803717137, b=>-3.1118988467
Iteration 1511: Achieved Loss=> 31.9713669143
Gradients: w=>0.1802758226, b=>-3.1102444657
Iteration 1512: Achieved Loss=> 31.9616633743
Gradients: w=>0.1801799824, b=>-3.1085909643
Iteration 1513: Achieved Loss=> 31.9519701490
Gradients: w=>0.1800841932, b=>-3.1069383419
Iteration 1514: Achieved Loss=> 31.9422872274
Gradients: w=>0.1799884549, b=>-3.1052865981
Iteration 1515: Achieved Loss=> 31.9326145985
Gradients: w=>0.1798927675, b=>-3.1036357324
Iteration 1516: Achieved Loss=> 31.9229522514
Gradients: w=>0.1797971310, b=>-3.1019857444
Iteration 1517: Achieved Loss=> 31.9133001752
Gradients: w=>0.1797015453, b=>-3.1003366336
Iteration 1518: Achieved Loss=> 31.9036583589
Gradients: w=>0.1796060105, b=>-3.0986883995
Iteration 1519: Achieved Loss=> 31.8940267917
Gradients: w=>0.1795105264, b=>-3.0970410416
Iteration 1520: Achieved Loss=> 31.8844054626
Gradients: w=>0.1794150931, b=>-3.0953945595
Iteration 1521: Achieved Loss=> 31.8747943608
Gradients: w=>0.1793197105, b=>-3.0937489528
Iteration 1522: Achieved Loss=> 31.8651934754
Gradients: w=>0.1792243787, b=>-3.0921042209
Iteration 1523: Achieved Loss=> 31.8556027956
Gradients: w=>0.1791290975, b=>-3.0904603633
Iteration 1524: Achieved Loss=> 31.8460223104
Gradients: w=>0.1790338670, b=>-3.0888173797
Iteration 1525: Achieved Loss=> 31.8364520091
Gradients: w=>0.1789386871, b=>-3.0871752696
Iteration 1526: Achieved Loss=> 31.8268918808
Gradients: w=>0.1788435578, b=>-3.0855340325
Iteration 1527: Achieved Loss=> 31.8173419147
Gradients: w=>0.1787484790, b=>-3.0838936679
Iteration 1528: Achieved Loss=> 31.8078021000
Gradients: w=>0.1786534509, b=>-3.0822541754
Iteration 1529: Achieved Loss=> 31.7982724260
Gradients: w=>0.1785584732, b=>-3.0806155544
Iteration 1530: Achieved Loss=> 31.7887528818
Gradients: w=>0.1784635460, b=>-3.0789778046
Iteration 1531: Achieved Loss=> 31.7792434566
Gradients: w=>0.1783686693, b=>-3.0773409255
Iteration 1532: Achieved Loss=> 31.7697441398
Gradients: w=>0.1782738431, b=>-3.0757049166
Iteration 1533: Achieved Loss=> 31.7602549206
Gradients: w=>0.1781790672, b=>-3.0740697775
Iteration 1534: Achieved Loss=> 31.7507757882
Gradients: w=>0.1780843418, b=>-3.0724355076
Iteration 1535: Achieved Loss=> 31.7413067319
Gradients: w=>0.1779896667, b=>-3.0708021066
Iteration 1536: Achieved Loss=> 31.7318477410
Gradients: w=>0.1778950419, b=>-3.0691695740
Iteration 1537: Achieved Loss=> 31.7223988048
Gradients: w=>0.1778004674, b=>-3.0675379092
Iteration 1538: Achieved Loss=> 31.7129599126
Gradients: w=>0.1777059433, b=>-3.0659071119
Iteration 1539: Achieved Loss=> 31.7035310538
Gradients: w=>0.1776114693, b=>-3.0642771816
Iteration 1540: Achieved Loss=> 31.6941122177
Gradients: w=>0.1775170456, b=>-3.0626481178
Iteration 1541: Achieved Loss=> 31.6847033936
Gradients: w=>0.1774226721, b=>-3.0610199200
Iteration 1542: Achieved Loss=> 31.6753045708
Gradients: w=>0.1773283488, b=>-3.0593925879
Iteration 1543: Achieved Loss=> 31.6659157388
Gradients: w=>0.1772340756, b=>-3.0577661209
Iteration 1544: Achieved Loss=> 31.6565368870
Gradients: w=>0.1771398525, b=>-3.0561405185
Iteration 1545: Achieved Loss=> 31.6471680046
Gradients: w=>0.1770456795, b=>-3.0545157804
Iteration 1546: Achieved Loss=> 31.6378090812
Gradients: w=>0.1769515566, b=>-3.0528919061
Iteration 1547: Achieved Loss=> 31.6284601061
Gradients: w=>0.1768574837, b=>-3.0512688950
Iteration 1548: Achieved Loss=> 31.6191210688
Gradients: w=>0.1767634608, b=>-3.0496467468
Iteration 1549: Achieved Loss=> 31.6097919587
Gradients: w=>0.1766694880, b=>-3.0480254610
Iteration 1550: Achieved Loss=> 31.6004727652
Gradients: w=>0.1765755650, b=>-3.0464050371
Iteration 1551: Achieved Loss=> 31.5911634778
Gradients: w=>0.1764816921, b=>-3.0447854747
Iteration 1552: Achieved Loss=> 31.5818640860
Gradients: w=>0.1763878690, b=>-3.0431667733
Iteration 1553: Achieved Loss=> 31.5725745793
Gradients: w=>0.1762940958, b=>-3.0415489324
Iteration 1554: Achieved Loss=> 31.5632949471
Gradients: w=>0.1762003724, b=>-3.0399319516
Iteration 1555: Achieved Loss=> 31.5540251790
Gradients: w=>0.1761066989, b=>-3.0383158305
Iteration 1556: Achieved Loss=> 31.5447652644
Gradients: w=>0.1760130752, b=>-3.0367005685
Iteration 1557: Achieved Loss=> 31.5355151929
Gradients: w=>0.1759195012, b=>-3.0350861653
Iteration 1558: Achieved Loss=> 31.5262749541
Gradients: w=>0.1758259770, b=>-3.0334726203
Iteration 1559: Achieved Loss=> 31.5170445374
Gradients: w=>0.1757325025, b=>-3.0318599332
Iteration 1560: Achieved Loss=> 31.5078239324
Gradients: w=>0.1756390777, b=>-3.0302481034
Iteration 1561: Achieved Loss=> 31.4986131288
Gradients: w=>0.1755457026, b=>-3.0286371305
Iteration 1562: Achieved Loss=> 31.4894121161
Gradients: w=>0.1754523771, b=>-3.0270270140
Iteration 1563: Achieved Loss=> 31.4802208838
Gradients: w=>0.1753591012, b=>-3.0254177535
Iteration 1564: Achieved Loss=> 31.4710394216
Gradients: w=>0.1752658750, b=>-3.0238093486
Iteration 1565: Achieved Loss=> 31.4618677192
Gradients: w=>0.1751726982, b=>-3.0222017987
Iteration 1566: Achieved Loss=> 31.4527057660
Gradients: w=>0.1750795711, b=>-3.0205951035
Iteration 1567: Achieved Loss=> 31.4435535518
Gradients: w=>0.1749864934, b=>-3.0189892624
Iteration 1568: Achieved Loss=> 31.4344110662
Gradients: w=>0.1748934652, b=>-3.0173842751
Iteration 1569: Achieved Loss=> 31.4252782989
Gradients: w=>0.1748004865, b=>-3.0157801410
Iteration 1570: Achieved Loss=> 31.4161552396
Gradients: w=>0.1747075572, b=>-3.0141768597
Iteration 1571: Achieved Loss=> 31.4070418778
Gradients: w=>0.1746146773, b=>-3.0125744307
Iteration 1572: Achieved Loss=> 31.3979382034
Gradients: w=>0.1745218467, b=>-3.0109728537
Iteration 1573: Achieved Loss=> 31.3888442059
Gradients: w=>0.1744290656, b=>-3.0093721281
Iteration 1574: Achieved Loss=> 31.3797598752
Gradients: w=>0.1743363337, b=>-3.0077722535
Iteration 1575: Achieved Loss=> 31.3706852010
Gradients: w=>0.1742436512, b=>-3.0061732295
Iteration 1576: Achieved Loss=> 31.3616201729
Gradients: w=>0.1741510179, b=>-3.0045750555
Iteration 1577: Achieved Loss=> 31.3525647807
Gradients: w=>0.1740584339, b=>-3.0029777312
Iteration 1578: Achieved Loss=> 31.3435190143
Gradients: w=>0.1739658991, b=>-3.0013812560
Iteration 1579: Achieved Loss=> 31.3344828633
Gradients: w=>0.1738734135, b=>-2.9997856296
Iteration 1580: Achieved Loss=> 31.3254563175
Gradients: w=>0.1737809770, b=>-2.9981908515
Iteration 1581: Achieved Loss=> 31.3164393668
Gradients: w=>0.1736885897, b=>-2.9965969212
Iteration 1582: Achieved Loss=> 31.3074320009
Gradients: w=>0.1735962515, b=>-2.9950038383
Iteration 1583: Achieved Loss=> 31.2984342097
Gradients: w=>0.1735039624, b=>-2.9934116024
Iteration 1584: Achieved Loss=> 31.2894459829
Gradients: w=>0.1734117224, b=>-2.9918202129
Iteration 1585: Achieved Loss=> 31.2804673103
Gradients: w=>0.1733195314, b=>-2.9902296694
Iteration 1586: Achieved Loss=> 31.2714981820
Gradients: w=>0.1732273894, b=>-2.9886399715
Iteration 1587: Achieved Loss=> 31.2625385876
Gradients: w=>0.1731352964, b=>-2.9870511188
Iteration 1588: Achieved Loss=> 31.2535885171
Gradients: w=>0.1730432524, b=>-2.9854631107
Iteration 1589: Achieved Loss=> 31.2446479603
Gradients: w=>0.1729512573, b=>-2.9838759469
Iteration 1590: Achieved Loss=> 31.2357169071
Gradients: w=>0.1728593111, b=>-2.9822896269
Iteration 1591: Achieved Loss=> 31.2267953475
Gradients: w=>0.1727674138, b=>-2.9807041501
Iteration 1592: Achieved Loss=> 31.2178832713
Gradients: w=>0.1726755653, b=>-2.9791195163
Iteration 1593: Achieved Loss=> 31.2089806684
Gradients: w=>0.1725837657, b=>-2.9775357250
Iteration 1594: Achieved Loss=> 31.2000875288
Gradients: w=>0.1724920148, b=>-2.9759527756
Iteration 1595: Achieved Loss=> 31.1912038425
Gradients: w=>0.1724003128, b=>-2.9743706677
Iteration 1596: Achieved Loss=> 31.1823295993
Gradients: w=>0.1723086595, b=>-2.9727894010
Iteration 1597: Achieved Loss=> 31.1734647892
Gradients: w=>0.1722170549, b=>-2.9712089749
Iteration 1598: Achieved Loss=> 31.1646094022
Gradients: w=>0.1721254991, b=>-2.9696293890
Iteration 1599: Achieved Loss=> 31.1557634284
Gradients: w=>0.1720339919, b=>-2.9680506429
Iteration 1600: Achieved Loss=> 31.1469268576
Gradients: w=>0.1719425333, b=>-2.9664727360
Iteration 1601: Achieved Loss=> 31.1380996799
Gradients: w=>0.1718511234, b=>-2.9648956681
Iteration 1602: Achieved Loss=> 31.1292818853
Gradients: w=>0.1717597620, b=>-2.9633194385
Iteration 1603: Achieved Loss=> 31.1204734638
Gradients: w=>0.1716684493, b=>-2.9617440470
Iteration 1604: Achieved Loss=> 31.1116744055
Gradients: w=>0.1715771851, b=>-2.9601694929
Iteration 1605: Achieved Loss=> 31.1028847004
Gradients: w=>0.1714859694, b=>-2.9585957759
Iteration 1606: Achieved Loss=> 31.0941043385
Gradients: w=>0.1713948022, b=>-2.9570228956
Iteration 1607: Achieved Loss=> 31.0853333101
Gradients: w=>0.1713036834, b=>-2.9554508515
Iteration 1608: Achieved Loss=> 31.0765716050
Gradients: w=>0.1712126131, b=>-2.9538796431
Iteration 1609: Achieved Loss=> 31.0678192134
Gradients: w=>0.1711215912, b=>-2.9523092700
Iteration 1610: Achieved Loss=> 31.0590761255
Gradients: w=>0.1710306178, b=>-2.9507397318
Iteration 1611: Achieved Loss=> 31.0503423312
Gradients: w=>0.1709396926, b=>-2.9491710280
Iteration 1612: Achieved Loss=> 31.0416178208
Gradients: w=>0.1708488158, b=>-2.9476031581
Iteration 1613: Achieved Loss=> 31.0329025844
Gradients: w=>0.1707579874, b=>-2.9460361218
Iteration 1614: Achieved Loss=> 31.0241966120
Gradients: w=>0.1706672072, b=>-2.9444699186
Iteration 1615: Achieved Loss=> 31.0154998940
Gradients: w=>0.1705764753, b=>-2.9429045480
Iteration 1616: Achieved Loss=> 31.0068124203
Gradients: w=>0.1704857916, b=>-2.9413400096
Iteration 1617: Achieved Loss=> 30.9981341813
Gradients: w=>0.1703951561, b=>-2.9397763030
Iteration 1618: Achieved Loss=> 30.9894651671
Gradients: w=>0.1703045688, b=>-2.9382134276
Iteration 1619: Achieved Loss=> 30.9808053678
Gradients: w=>0.1702140297, b=>-2.9366513832
Iteration 1620: Achieved Loss=> 30.9721547737
Gradients: w=>0.1701235387, b=>-2.9350901692
Iteration 1621: Achieved Loss=> 30.9635133750
Gradients: w=>0.1700330958, b=>-2.9335297852
Iteration 1622: Achieved Loss=> 30.9548811619
Gradients: w=>0.1699427009, b=>-2.9319702307
Iteration 1623: Achieved Loss=> 30.9462581247
Gradients: w=>0.1698523542, b=>-2.9304115053
Iteration 1624: Achieved Loss=> 30.9376442536
Gradients: w=>0.1697620555, b=>-2.9288536086
Iteration 1625: Achieved Loss=> 30.9290395389
Gradients: w=>0.1696718047, b=>-2.9272965401
Iteration 1626: Achieved Loss=> 30.9204439708
Gradients: w=>0.1695816020, b=>-2.9257402994
Iteration 1627: Achieved Loss=> 30.9118575396
Gradients: w=>0.1694914472, b=>-2.9241848861
Iteration 1628: Achieved Loss=> 30.9032802355
Gradients: w=>0.1694013404, b=>-2.9226302996
Iteration 1629: Achieved Loss=> 30.8947120490
Gradients: w=>0.1693112814, b=>-2.9210765397
Iteration 1630: Achieved Loss=> 30.8861529703
Gradients: w=>0.1692212703, b=>-2.9195236057
Iteration 1631: Achieved Loss=> 30.8776029897
Gradients: w=>0.1691313071, b=>-2.9179714974
Iteration 1632: Achieved Loss=> 30.8690620975
Gradients: w=>0.1690413917, b=>-2.9164202141
Iteration 1633: Achieved Loss=> 30.8605302842
Gradients: w=>0.1689515241, b=>-2.9148697556
Iteration 1634: Achieved Loss=> 30.8520075399
Gradients: w=>0.1688617043, b=>-2.9133201214
Iteration 1635: Achieved Loss=> 30.8434938552
Gradients: w=>0.1687719322, b=>-2.9117713110
Iteration 1636: Achieved Loss=> 30.8349892203
Gradients: w=>0.1686822079, b=>-2.9102233240
Iteration 1637: Achieved Loss=> 30.8264936257
Gradients: w=>0.1685925313, b=>-2.9086761599
Iteration 1638: Achieved Loss=> 30.8180070617
Gradients: w=>0.1685029023, b=>-2.9071298184
Iteration 1639: Achieved Loss=> 30.8095295188
Gradients: w=>0.1684133210, b=>-2.9055842990
Iteration 1640: Achieved Loss=> 30.8010609873
Gradients: w=>0.1683237873, b=>-2.9040396012
Iteration 1641: Achieved Loss=> 30.7926014577
Gradients: w=>0.1682343012, b=>-2.9024957246
Iteration 1642: Achieved Loss=> 30.7841509203
Gradients: w=>0.1681448627, b=>-2.9009526688
Iteration 1643: Achieved Loss=> 30.7757093657
Gradients: w=>0.1680554717, b=>-2.8994104333
Iteration 1644: Achieved Loss=> 30.7672767843
Gradients: w=>0.1679661283, b=>-2.8978690177
Iteration 1645: Achieved Loss=> 30.7588531666
Gradients: w=>0.1678768323, b=>-2.8963284216
Iteration 1646: Achieved Loss=> 30.7504385030
Gradients: w=>0.1677875838, b=>-2.8947886445
Iteration 1647: Achieved Loss=> 30.7420327840
Gradients: w=>0.1676983828, b=>-2.8932496860
Iteration 1648: Achieved Loss=> 30.7336360001
Gradients: w=>0.1676092292, b=>-2.8917115456
Iteration 1649: Achieved Loss=> 30.7252481418
Gradients: w=>0.1675201230, b=>-2.8901742230
Iteration 1650: Achieved Loss=> 30.7168691996
Gradients: w=>0.1674310642, b=>-2.8886377177
Iteration 1651: Achieved Loss=> 30.7084991640
Gradients: w=>0.1673420527, b=>-2.8871020292
Iteration 1652: Achieved Loss=> 30.7001380256
Gradients: w=>0.1672530885, b=>-2.8855671572
Iteration 1653: Achieved Loss=> 30.6917857750
Gradients: w=>0.1671641716, b=>-2.8840331011
Iteration 1654: Achieved Loss=> 30.6834424026
Gradients: w=>0.1670753020, b=>-2.8824998606
Iteration 1655: Achieved Loss=> 30.6751078990
Gradients: w=>0.1669864797, b=>-2.8809674352
Iteration 1656: Achieved Loss=> 30.6667822548
Gradients: w=>0.1668977045, b=>-2.8794358245
Iteration 1657: Achieved Loss=> 30.6584654606
Gradients: w=>0.1668089766, b=>-2.8779050280
Iteration 1658: Achieved Loss=> 30.6501575070
Gradients: w=>0.1667202958, b=>-2.8763750453
Iteration 1659: Achieved Loss=> 30.6418583846
Gradients: w=>0.1666316622, b=>-2.8748458761
Iteration 1660: Achieved Loss=> 30.6335680839
Gradients: w=>0.1665430757, b=>-2.8733175198
Iteration 1661: Achieved Loss=> 30.6252865957
Gradients: w=>0.1664545363, b=>-2.8717899760
Iteration 1662: Achieved Loss=> 30.6170139105
Gradients: w=>0.1663660439, b=>-2.8702632443
Iteration 1663: Achieved Loss=> 30.6087500190
Gradients: w=>0.1662775986, b=>-2.8687373242
Iteration 1664: Achieved Loss=> 30.6004949119
Gradients: w=>0.1661892003, b=>-2.8672122154
Iteration 1665: Achieved Loss=> 30.5922485797
Gradients: w=>0.1661008491, b=>-2.8656879174
Iteration 1666: Achieved Loss=> 30.5840110132
Gradients: w=>0.1660125448, b=>-2.8641644297
Iteration 1667: Achieved Loss=> 30.5757822031
Gradients: w=>0.1659242874, b=>-2.8626417520
Iteration 1668: Achieved Loss=> 30.5675621400
Gradients: w=>0.1658360769, b=>-2.8611198838
Iteration 1669: Achieved Loss=> 30.5593508147
Gradients: w=>0.1657479134, b=>-2.8595988246
Iteration 1670: Achieved Loss=> 30.5511482179
Gradients: w=>0.1656597967, b=>-2.8580785741
Iteration 1671: Achieved Loss=> 30.5429543402
Gradients: w=>0.1655717269, b=>-2.8565591318
Iteration 1672: Achieved Loss=> 30.5347691725
Gradients: w=>0.1654837039, b=>-2.8550404973
Iteration 1673: Achieved Loss=> 30.5265927054
Gradients: w=>0.1653957277, b=>-2.8535226701
Iteration 1674: Achieved Loss=> 30.5184249297
Gradients: w=>0.1653077982, b=>-2.8520056499
Iteration 1675: Achieved Loss=> 30.5102658362
Gradients: w=>0.1652199155, b=>-2.8504894362
Iteration 1676: Achieved Loss=> 30.5021154156
Gradients: w=>0.1651320795, b=>-2.8489740285
Iteration 1677: Achieved Loss=> 30.4939736588
Gradients: w=>0.1650442903, b=>-2.8474594264
Iteration 1678: Achieved Loss=> 30.4858405564
Gradients: w=>0.1649565476, b=>-2.8459456296
Iteration 1679: Achieved Loss=> 30.4777160994
Gradients: w=>0.1648688517, b=>-2.8444326375
Iteration 1680: Achieved Loss=> 30.4696002786
Gradients: w=>0.1647812023, b=>-2.8429204499
Iteration 1681: Achieved Loss=> 30.4614930846
Gradients: w=>0.1646935996, b=>-2.8414090661
Iteration 1682: Achieved Loss=> 30.4533945085
Gradients: w=>0.1646060434, b=>-2.8398984858
Iteration 1683: Achieved Loss=> 30.4453045409
Gradients: w=>0.1645185338, b=>-2.8383887086
Iteration 1684: Achieved Loss=> 30.4372231728
Gradients: w=>0.1644310707, b=>-2.8368797340
Iteration 1685: Achieved Loss=> 30.4291503951
Gradients: w=>0.1643436541, b=>-2.8353715617
Iteration 1686: Achieved Loss=> 30.4210861985
Gradients: w=>0.1642562840, b=>-2.8338641911
Iteration 1687: Achieved Loss=> 30.4130305740
Gradients: w=>0.1641689603, b=>-2.8323576219
Iteration 1688: Achieved Loss=> 30.4049835124
Gradients: w=>0.1640816830, b=>-2.8308518537
Iteration 1689: Achieved Loss=> 30.3969450047
Gradients: w=>0.1639944522, b=>-2.8293468860
Iteration 1690: Achieved Loss=> 30.3889150418
Gradients: w=>0.1639072677, b=>-2.8278427183
Iteration 1691: Achieved Loss=> 30.3808936146
Gradients: w=>0.1638201295, b=>-2.8263393503
Iteration 1692: Achieved Loss=> 30.3728807139
Gradients: w=>0.1637330377, b=>-2.8248367816
Iteration 1693: Achieved Loss=> 30.3648763308
Gradients: w=>0.1636459922, b=>-2.8233350116
Iteration 1694: Achieved Loss=> 30.3568804562
Gradients: w=>0.1635589930, b=>-2.8218340401
Iteration 1695: Achieved Loss=> 30.3488930811
Gradients: w=>0.1634720400, b=>-2.8203338665
Iteration 1696: Achieved Loss=> 30.3409141963
Gradients: w=>0.1633851332, b=>-2.8188344904
Iteration 1697: Achieved Loss=> 30.3329437929
Gradients: w=>0.1632982727, b=>-2.8173359115
Iteration 1698: Achieved Loss=> 30.3249818619
Gradients: w=>0.1632114583, b=>-2.8158381292
Iteration 1699: Achieved Loss=> 30.3170283943
Gradients: w=>0.1631246901, b=>-2.8143411433
Iteration 1700: Achieved Loss=> 30.3090833810
Gradients: w=>0.1630379680, b=>-2.8128449531
Iteration 1701: Achieved Loss=> 30.3011468132
Gradients: w=>0.1629512920, b=>-2.8113495584
Iteration 1702: Achieved Loss=> 30.2932186817
Gradients: w=>0.1628646621, b=>-2.8098549587
Iteration 1703: Achieved Loss=> 30.2852989776
Gradients: w=>0.1627780783, b=>-2.8083611535
Iteration 1704: Achieved Loss=> 30.2773876921
Gradients: w=>0.1626915404, b=>-2.8068681426
Iteration 1705: Achieved Loss=> 30.2694848160
Gradients: w=>0.1626050486, b=>-2.8053759253
Iteration 1706: Achieved Loss=> 30.2615903406
Gradients: w=>0.1625186028, b=>-2.8038845013
Iteration 1707: Achieved Loss=> 30.2537042568
Gradients: w=>0.1624322029, b=>-2.8023938703
Iteration 1708: Achieved Loss=> 30.2458265557
Gradients: w=>0.1623458490, b=>-2.8009040317
Iteration 1709: Achieved Loss=> 30.2379572285
Gradients: w=>0.1622595409, b=>-2.7994149851
Iteration 1710: Achieved Loss=> 30.2300962662
Gradients: w=>0.1621732788, b=>-2.7979267302
Iteration 1711: Achieved Loss=> 30.2222436599
Gradients: w=>0.1620870625, b=>-2.7964392665
Iteration 1712: Achieved Loss=> 30.2143994008
Gradients: w=>0.1620008920, b=>-2.7949525935
Iteration 1713: Achieved Loss=> 30.2065634800
Gradients: w=>0.1619147674, b=>-2.7934667109
Iteration 1714: Achieved Loss=> 30.1987358886
Gradients: w=>0.1618286885, b=>-2.7919816183
Iteration 1715: Achieved Loss=> 30.1909166177
Gradients: w=>0.1617426554, b=>-2.7904973151
Iteration 1716: Achieved Loss=> 30.1831056586
Gradients: w=>0.1616566681, b=>-2.7890138011
Iteration 1717: Achieved Loss=> 30.1753030024
Gradients: w=>0.1615707264, b=>-2.7875310758
Iteration 1718: Achieved Loss=> 30.1675086402
Gradients: w=>0.1614848305, b=>-2.7860491387
Iteration 1719: Achieved Loss=> 30.1597225632
Gradients: w=>0.1613989802, b=>-2.7845679894
Iteration 1720: Achieved Loss=> 30.1519447627
Gradients: w=>0.1613131755, b=>-2.7830876276
Iteration 1721: Achieved Loss=> 30.1441752298
Gradients: w=>0.1612274165, b=>-2.7816080528
Iteration 1722: Achieved Loss=> 30.1364139558
Gradients: w=>0.1611417030, b=>-2.7801292646
Iteration 1723: Achieved Loss=> 30.1286609319
Gradients: w=>0.1610560352, b=>-2.7786512625
Iteration 1724: Achieved Loss=> 30.1209161492
Gradients: w=>0.1609704128, b=>-2.7771740462
Iteration 1725: Achieved Loss=> 30.1131795991
Gradients: w=>0.1608848360, b=>-2.7756976153
Iteration 1726: Achieved Loss=> 30.1054512728
Gradients: w=>0.1607993047, b=>-2.7742219692
Iteration 1727: Achieved Loss=> 30.0977311615
Gradients: w=>0.1607138188, b=>-2.7727471077
Iteration 1728: Achieved Loss=> 30.0900192565
Gradients: w=>0.1606283784, b=>-2.7712730302
Iteration 1729: Achieved Loss=> 30.0823155491
Gradients: w=>0.1605429835, b=>-2.7697997364
Iteration 1730: Achieved Loss=> 30.0746200306
Gradients: w=>0.1604576339, b=>-2.7683272258
Iteration 1731: Achieved Loss=> 30.0669326923
Gradients: w=>0.1603723297, b=>-2.7668554981
Iteration 1732: Achieved Loss=> 30.0592535255
Gradients: w=>0.1602870708, b=>-2.7653845528
Iteration 1733: Achieved Loss=> 30.0515825214
Gradients: w=>0.1602018573, b=>-2.7639143895
Iteration 1734: Achieved Loss=> 30.0439196715
Gradients: w=>0.1601166891, b=>-2.7624450078
Iteration 1735: Achieved Loss=> 30.0362649670
Gradients: w=>0.1600315661, b=>-2.7609764072
Iteration 1736: Achieved Loss=> 30.0286183993
Gradients: w=>0.1599464884, b=>-2.7595085874
Iteration 1737: Achieved Loss=> 30.0209799597
Gradients: w=>0.1598614560, b=>-2.7580415479
Iteration 1738: Achieved Loss=> 30.0133496396
Gradients: w=>0.1597764687, b=>-2.7565752884
Iteration 1739: Achieved Loss=> 30.0057274304
Gradients: w=>0.1596915267, b=>-2.7551098084
Iteration 1740: Achieved Loss=> 29.9981133235
Gradients: w=>0.1596066297, b=>-2.7536451074
Iteration 1741: Achieved Loss=> 29.9905073102
Gradients: w=>0.1595217779, b=>-2.7521811851
Iteration 1742: Achieved Loss=> 29.9829093819
Gradients: w=>0.1594369713, b=>-2.7507180412
Iteration 1743: Achieved Loss=> 29.9753195300
Gradients: w=>0.1593522097, b=>-2.7492556750
Iteration 1744: Achieved Loss=> 29.9677377460
Gradients: w=>0.1592674932, b=>-2.7477940863
Iteration 1745: Achieved Loss=> 29.9601640213
Gradients: w=>0.1591828217, b=>-2.7463332746
Iteration 1746: Achieved Loss=> 29.9525983473
Gradients: w=>0.1590981952, b=>-2.7448732396
Iteration 1747: Achieved Loss=> 29.9450407154
Gradients: w=>0.1590136137, b=>-2.7434139807
Iteration 1748: Achieved Loss=> 29.9374911171
Gradients: w=>0.1589290772, b=>-2.7419554976
Iteration 1749: Achieved Loss=> 29.9299495439
Gradients: w=>0.1588445856, b=>-2.7404977899
Iteration 1750: Achieved Loss=> 29.9224159872
Gradients: w=>0.1587601390, b=>-2.7390408572
Iteration 1751: Achieved Loss=> 29.9148904386
Gradients: w=>0.1586757372, b=>-2.7375846990
Iteration 1752: Achieved Loss=> 29.9073728894
Gradients: w=>0.1585913803, b=>-2.7361293149
Iteration 1753: Achieved Loss=> 29.8998633313
Gradients: w=>0.1585070683, b=>-2.7346747046
Iteration 1754: Achieved Loss=> 29.8923617556
Gradients: w=>0.1584228010, b=>-2.7332208676
Iteration 1755: Achieved Loss=> 29.8848681539
Gradients: w=>0.1583385786, b=>-2.7317678035
Iteration 1756: Achieved Loss=> 29.8773825178
Gradients: w=>0.1582544010, b=>-2.7303155118
Iteration 1757: Achieved Loss=> 29.8699048388
Gradients: w=>0.1581702681, b=>-2.7288639923
Iteration 1758: Achieved Loss=> 29.8624351084
Gradients: w=>0.1580861799, b=>-2.7274132445
Iteration 1759: Achieved Loss=> 29.8549733181
Gradients: w=>0.1580021364, b=>-2.7259632679
Iteration 1760: Achieved Loss=> 29.8475194596
Gradients: w=>0.1579181377, b=>-2.7245140621
Iteration 1761: Achieved Loss=> 29.8400735243
Gradients: w=>0.1578341835, b=>-2.7230656268
Iteration 1762: Achieved Loss=> 29.8326355040
Gradients: w=>0.1577502740, b=>-2.7216179615
Iteration 1763: Achieved Loss=> 29.8252053900
Gradients: w=>0.1576664091, b=>-2.7201710659
Iteration 1764: Achieved Loss=> 29.8177831742
Gradients: w=>0.1575825888, b=>-2.7187249395
Iteration 1765: Achieved Loss=> 29.8103688480
Gradients: w=>0.1574988131, b=>-2.7172795818
Iteration 1766: Achieved Loss=> 29.8029624030
Gradients: w=>0.1574150819, b=>-2.7158349926
Iteration 1767: Achieved Loss=> 29.7955638310
Gradients: w=>0.1573313952, b=>-2.7143911713
Iteration 1768: Achieved Loss=> 29.7881731235
Gradients: w=>0.1572477530, b=>-2.7129481177
Iteration 1769: Achieved Loss=> 29.7807902721
Gradients: w=>0.1571641553, b=>-2.7115058312
Iteration 1770: Achieved Loss=> 29.7734152686
Gradients: w=>0.1570806020, b=>-2.7100643115
Iteration 1771: Achieved Loss=> 29.7660481045
Gradients: w=>0.1569970931, b=>-2.7086235581
Iteration 1772: Achieved Loss=> 29.7586887716
Gradients: w=>0.1569136286, b=>-2.7071835707
Iteration 1773: Achieved Loss=> 29.7513372615
Gradients: w=>0.1568302085, b=>-2.7057443488
Iteration 1774: Achieved Loss=> 29.7439935659
Gradients: w=>0.1567468328, b=>-2.7043058920
Iteration 1775: Achieved Loss=> 29.7366576765
Gradients: w=>0.1566635014, b=>-2.7028682000
Iteration 1776: Achieved Loss=> 29.7293295850
Gradients: w=>0.1565802142, b=>-2.7014312723
Iteration 1777: Achieved Loss=> 29.7220092831
Gradients: w=>0.1564969714, b=>-2.6999951085
Iteration 1778: Achieved Loss=> 29.7146967625
Gradients: w=>0.1564137728, b=>-2.6985597083
Iteration 1779: Achieved Loss=> 29.7073920149
Gradients: w=>0.1563306184, b=>-2.6971250711
Iteration 1780: Achieved Loss=> 29.7000950322
Gradients: w=>0.1562475083, b=>-2.6956911966
Iteration 1781: Achieved Loss=> 29.6928058060
Gradients: w=>0.1561644423, b=>-2.6942580844
Iteration 1782: Achieved Loss=> 29.6855243281
Gradients: w=>0.1560814205, b=>-2.6928257341
Iteration 1783: Achieved Loss=> 29.6782505902
Gradients: w=>0.1559984428, b=>-2.6913941453
Iteration 1784: Achieved Loss=> 29.6709845842
Gradients: w=>0.1559155093, b=>-2.6899633176
Iteration 1785: Achieved Loss=> 29.6637263017
Gradients: w=>0.1558326198, b=>-2.6885332505
Iteration 1786: Achieved Loss=> 29.6564757347
Gradients: w=>0.1557497744, b=>-2.6871039437
Iteration 1787: Achieved Loss=> 29.6492328749
Gradients: w=>0.1556669730, b=>-2.6856753968
Iteration 1788: Achieved Loss=> 29.6419977141
Gradients: w=>0.1555842157, b=>-2.6842476093
Iteration 1789: Achieved Loss=> 29.6347702441
Gradients: w=>0.1555015023, b=>-2.6828205809
Iteration 1790: Achieved Loss=> 29.6275504568
Gradients: w=>0.1554188330, b=>-2.6813943111
Iteration 1791: Achieved Loss=> 29.6203383439
Gradients: w=>0.1553362075, b=>-2.6799687996
Iteration 1792: Achieved Loss=> 29.6131338973
Gradients: w=>0.1552536260, b=>-2.6785440459
Iteration 1793: Achieved Loss=> 29.6059371090
Gradients: w=>0.1551710884, b=>-2.6771200497
Iteration 1794: Achieved Loss=> 29.5987479706
Gradients: w=>0.1550885947, b=>-2.6756968105
Iteration 1795: Achieved Loss=> 29.5915664742
Gradients: w=>0.1550061449, b=>-2.6742743279
Iteration 1796: Achieved Loss=> 29.5843926115
Gradients: w=>0.1549237389, b=>-2.6728526016
Iteration 1797: Achieved Loss=> 29.5772263746
Gradients: w=>0.1548413766, b=>-2.6714316311
Iteration 1798: Achieved Loss=> 29.5700677551
Gradients: w=>0.1547590582, b=>-2.6700114161
Iteration 1799: Achieved Loss=> 29.5629167451
Gradients: w=>0.1546767835, b=>-2.6685919560
Iteration 1800: Achieved Loss=> 29.5557733365
Gradients: w=>0.1545945526, b=>-2.6671732506
Iteration 1801: Achieved Loss=> 29.5486375212
Gradients: w=>0.1545123654, b=>-2.6657552995
Iteration 1802: Achieved Loss=> 29.5415092911
Gradients: w=>0.1544302219, b=>-2.6643381021
Iteration 1803: Achieved Loss=> 29.5343886382
Gradients: w=>0.1543481220, b=>-2.6629216582
Iteration 1804: Achieved Loss=> 29.5272755544
Gradients: w=>0.1542660658, b=>-2.6615059673
Iteration 1805: Achieved Loss=> 29.5201700316
Gradients: w=>0.1541840533, b=>-2.6600910291
Iteration 1806: Achieved Loss=> 29.5130720618
Gradients: w=>0.1541020843, b=>-2.6586768430
Iteration 1807: Achieved Loss=> 29.5059816371
Gradients: w=>0.1540201589, b=>-2.6572634088
Iteration 1808: Achieved Loss=> 29.4988987493
Gradients: w=>0.1539382771, b=>-2.6558507260
Iteration 1809: Achieved Loss=> 29.4918233905
Gradients: w=>0.1538564387, b=>-2.6544387942
Iteration 1810: Achieved Loss=> 29.4847555526
Gradients: w=>0.1537746439, b=>-2.6530276131
Iteration 1811: Achieved Loss=> 29.4776952277
Gradients: w=>0.1536928926, b=>-2.6516171822
Iteration 1812: Achieved Loss=> 29.4706424077
Gradients: w=>0.1536111848, b=>-2.6502075011
Iteration 1813: Achieved Loss=> 29.4635970848
Gradients: w=>0.1535295203, b=>-2.6487985695
Iteration 1814: Achieved Loss=> 29.4565592509
Gradients: w=>0.1534478993, b=>-2.6473903868
Iteration 1815: Achieved Loss=> 29.4495288980
Gradients: w=>0.1533663217, b=>-2.6459829528
Iteration 1816: Achieved Loss=> 29.4425060183
Gradients: w=>0.1532847875, b=>-2.6445762671
Iteration 1817: Achieved Loss=> 29.4354906037
Gradients: w=>0.1532032966, b=>-2.6431703292
Iteration 1818: Achieved Loss=> 29.4284826464
Gradients: w=>0.1531218490, b=>-2.6417651387
Iteration 1819: Achieved Loss=> 29.4214821384
Gradients: w=>0.1530404447, b=>-2.6403606952
Iteration 1820: Achieved Loss=> 29.4144890718
Gradients: w=>0.1529590837, b=>-2.6389569985
Iteration 1821: Achieved Loss=> 29.4075034386
Gradients: w=>0.1528777660, b=>-2.6375540479
Iteration 1822: Achieved Loss=> 29.4005252311
Gradients: w=>0.1527964915, b=>-2.6361518432
Iteration 1823: Achieved Loss=> 29.3935544412
Gradients: w=>0.1527152602, b=>-2.6347503840
Iteration 1824: Achieved Loss=> 29.3865910611
Gradients: w=>0.1526340720, b=>-2.6333496698
Iteration 1825: Achieved Loss=> 29.3796350830
Gradients: w=>0.1525529271, b=>-2.6319497003
Iteration 1826: Achieved Loss=> 29.3726864989
Gradients: w=>0.1524718253, b=>-2.6305504750
Iteration 1827: Achieved Loss=> 29.3657453010
Gradients: w=>0.1523907666, b=>-2.6291519937
Iteration 1828: Achieved Loss=> 29.3588114815
Gradients: w=>0.1523097510, b=>-2.6277542558
Iteration 1829: Achieved Loss=> 29.3518850324
Gradients: w=>0.1522287784, b=>-2.6263572610
Iteration 1830: Achieved Loss=> 29.3449659461
Gradients: w=>0.1521478489, b=>-2.6249610088
Iteration 1831: Achieved Loss=> 29.3380542145
Gradients: w=>0.1520669625, b=>-2.6235654990
Iteration 1832: Achieved Loss=> 29.3311498300
Gradients: w=>0.1519861190, b=>-2.6221707310
Iteration 1833: Achieved Loss=> 29.3242527847
Gradients: w=>0.1519053185, b=>-2.6207767046
Iteration 1834: Achieved Loss=> 29.3173630708
Gradients: w=>0.1518245610, b=>-2.6193834192
Iteration 1835: Achieved Loss=> 29.3104806806
Gradients: w=>0.1517438464, b=>-2.6179908746
Iteration 1836: Achieved Loss=> 29.3036056061
Gradients: w=>0.1516631747, b=>-2.6165990703
Iteration 1837: Achieved Loss=> 29.2967378398
Gradients: w=>0.1515825459, b=>-2.6152080059
Iteration 1838: Achieved Loss=> 29.2898773737
Gradients: w=>0.1515019600, b=>-2.6138176811
Iteration 1839: Achieved Loss=> 29.2830242001
Gradients: w=>0.1514214169, b=>-2.6124280954
Iteration 1840: Achieved Loss=> 29.2761783114
Gradients: w=>0.1513409166, b=>-2.6110392484
Iteration 1841: Achieved Loss=> 29.2693396996
Gradients: w=>0.1512604591, b=>-2.6096511398
Iteration 1842: Achieved Loss=> 29.2625083572
Gradients: w=>0.1511800444, b=>-2.6082637691
Iteration 1843: Achieved Loss=> 29.2556842763
Gradients: w=>0.1510996725, b=>-2.6068771361
Iteration 1844: Achieved Loss=> 29.2488674493
Gradients: w=>0.1510193432, b=>-2.6054912402
Iteration 1845: Achieved Loss=> 29.2420578684
Gradients: w=>0.1509390567, b=>-2.6041060810
Iteration 1846: Achieved Loss=> 29.2352555260
Gradients: w=>0.1508588129, b=>-2.6027216583
Iteration 1847: Achieved Loss=> 29.2284604143
Gradients: w=>0.1507786117, b=>-2.6013379716
Iteration 1848: Achieved Loss=> 29.2216725257
Gradients: w=>0.1506984532, b=>-2.5999550205
Iteration 1849: Achieved Loss=> 29.2148918524
Gradients: w=>0.1506183373, b=>-2.5985728046
Iteration 1850: Achieved Loss=> 29.2081183869
Gradients: w=>0.1505382639, b=>-2.5971913235
Iteration 1851: Achieved Loss=> 29.2013521214
Gradients: w=>0.1504582332, b=>-2.5958105769
Iteration 1852: Achieved Loss=> 29.1945930483
Gradients: w=>0.1503782449, b=>-2.5944305643
Iteration 1853: Achieved Loss=> 29.1878411600
Gradients: w=>0.1502982993, b=>-2.5930512854
Iteration 1854: Achieved Loss=> 29.1810964488
Gradients: w=>0.1502183961, b=>-2.5916727397
Iteration 1855: Achieved Loss=> 29.1743589070
Gradients: w=>0.1501385354, b=>-2.5902949269
Iteration 1856: Achieved Loss=> 29.1676285272
Gradients: w=>0.1500587171, b=>-2.5889178467
Iteration 1857: Achieved Loss=> 29.1609053015
Gradients: w=>0.1499789413, b=>-2.5875414985
Iteration 1858: Achieved Loss=> 29.1541892226
Gradients: w=>0.1498992079, b=>-2.5861658820
Iteration 1859: Achieved Loss=> 29.1474802826
Gradients: w=>0.1498195168, b=>-2.5847909968
Iteration 1860: Achieved Loss=> 29.1407784742
Gradients: w=>0.1497398682, b=>-2.5834168426
Iteration 1861: Achieved Loss=> 29.1340837896
Gradients: w=>0.1496602619, b=>-2.5820434189
Iteration 1862: Achieved Loss=> 29.1273962213
Gradients: w=>0.1495806979, b=>-2.5806707254
Iteration 1863: Achieved Loss=> 29.1207157618
Gradients: w=>0.1495011762, b=>-2.5792987616
Iteration 1864: Achieved Loss=> 29.1140424034
Gradients: w=>0.1494216968, b=>-2.5779275272
Iteration 1865: Achieved Loss=> 29.1073761387
Gradients: w=>0.1493422596, b=>-2.5765570218
Iteration 1866: Achieved Loss=> 29.1007169601
Gradients: w=>0.1492628647, b=>-2.5751872450
Iteration 1867: Achieved Loss=> 29.0940648600
Gradients: w=>0.1491835120, b=>-2.5738181964
Iteration 1868: Achieved Loss=> 29.0874198310
Gradients: w=>0.1491042014, b=>-2.5724498756
Iteration 1869: Achieved Loss=> 29.0807818655
Gradients: w=>0.1490249331, b=>-2.5710822823
Iteration 1870: Achieved Loss=> 29.0741509561
Gradients: w=>0.1489457068, b=>-2.5697154161
Iteration 1871: Achieved Loss=> 29.0675270951
Gradients: w=>0.1488665227, b=>-2.5683492765
Iteration 1872: Achieved Loss=> 29.0609102752
Gradients: w=>0.1487873807, b=>-2.5669838632
Iteration 1873: Achieved Loss=> 29.0543004888
Gradients: w=>0.1487082808, b=>-2.5656191758
Iteration 1874: Achieved Loss=> 29.0476977285
Gradients: w=>0.1486292229, b=>-2.5642552139
Iteration 1875: Achieved Loss=> 29.0411019868
Gradients: w=>0.1485502070, b=>-2.5628919771
Iteration 1876: Achieved Loss=> 29.0345132562
Gradients: w=>0.1484712332, b=>-2.5615294650
Iteration 1877: Achieved Loss=> 29.0279315292
Gradients: w=>0.1483923013, b=>-2.5601676773
Iteration 1878: Achieved Loss=> 29.0213567986
Gradients: w=>0.1483134114, b=>-2.5588066136
Iteration 1879: Achieved Loss=> 29.0147890567
Gradients: w=>0.1482345635, b=>-2.5574462735
Iteration 1880: Achieved Loss=> 29.0082282962
Gradients: w=>0.1481557574, b=>-2.5560866565
Iteration 1881: Achieved Loss=> 29.0016745096
Gradients: w=>0.1480769933, b=>-2.5547277624
Iteration 1882: Achieved Loss=> 28.9951276896
Gradients: w=>0.1479982710, b=>-2.5533695907
Iteration 1883: Achieved Loss=> 28.9885878287
Gradients: w=>0.1479195906, b=>-2.5520121411
Iteration 1884: Achieved Loss=> 28.9820549195
Gradients: w=>0.1478409520, b=>-2.5506554131
Iteration 1885: Achieved Loss=> 28.9755289547
Gradients: w=>0.1477623552, b=>-2.5492994064
Iteration 1886: Achieved Loss=> 28.9690099268
Gradients: w=>0.1476838002, b=>-2.5479441206
Iteration 1887: Achieved Loss=> 28.9624978286
Gradients: w=>0.1476052869, b=>-2.5465895553
Iteration 1888: Achieved Loss=> 28.9559926525
Gradients: w=>0.1475268154, b=>-2.5452357101
Iteration 1889: Achieved Loss=> 28.9494943913
Gradients: w=>0.1474483857, b=>-2.5438825847
Iteration 1890: Achieved Loss=> 28.9430030377
Gradients: w=>0.1473699976, b=>-2.5425301786
Iteration 1891: Achieved Loss=> 28.9365185842
Gradients: w=>0.1472916512, b=>-2.5411784915
Iteration 1892: Achieved Loss=> 28.9300410235
Gradients: w=>0.1472133464, b=>-2.5398275230
Iteration 1893: Achieved Loss=> 28.9235703483
Gradients: w=>0.1471350833, b=>-2.5384772727
Iteration 1894: Achieved Loss=> 28.9171065514
Gradients: w=>0.1470568617, b=>-2.5371277403
Iteration 1895: Achieved Loss=> 28.9106496253
Gradients: w=>0.1469786818, b=>-2.5357789253
Iteration 1896: Achieved Loss=> 28.9041995628
Gradients: w=>0.1469005434, b=>-2.5344308274
Iteration 1897: Achieved Loss=> 28.8977563565
Gradients: w=>0.1468224466, b=>-2.5330834462
Iteration 1898: Achieved Loss=> 28.8913199993
Gradients: w=>0.1467443913, b=>-2.5317367813
Iteration 1899: Achieved Loss=> 28.8848904838
Gradients: w=>0.1466663774, b=>-2.5303908323
Iteration 1900: Achieved Loss=> 28.8784678027
Gradients: w=>0.1465884051, b=>-2.5290455989
Iteration 1901: Achieved Loss=> 28.8720519488
Gradients: w=>0.1465104742, b=>-2.5277010806
Iteration 1902: Achieved Loss=> 28.8656429148
Gradients: w=>0.1464325847, b=>-2.5263572771
Iteration 1903: Achieved Loss=> 28.8592406934
Gradients: w=>0.1463547367, b=>-2.5250141880
Iteration 1904: Achieved Loss=> 28.8528452775
Gradients: w=>0.1462769300, b=>-2.5236718130
Iteration 1905: Achieved Loss=> 28.8464566598
Gradients: w=>0.1461991647, b=>-2.5223301516
Iteration 1906: Achieved Loss=> 28.8400748331
Gradients: w=>0.1461214407, b=>-2.5209892035
Iteration 1907: Achieved Loss=> 28.8336997901
Gradients: w=>0.1460437581, b=>-2.5196489682
Iteration 1908: Achieved Loss=> 28.8273315236
Gradients: w=>0.1459661167, b=>-2.5183094455
Iteration 1909: Achieved Loss=> 28.8209700265
Gradients: w=>0.1458885167, b=>-2.5169706349
Iteration 1910: Achieved Loss=> 28.8146152915
Gradients: w=>0.1458109579, b=>-2.5156325360
Iteration 1911: Achieved Loss=> 28.8082673114
Gradients: w=>0.1457334403, b=>-2.5142951486
Iteration 1912: Achieved Loss=> 28.8019260791
Gradients: w=>0.1456559639, b=>-2.5129584721
Iteration 1913: Achieved Loss=> 28.7955915874
Gradients: w=>0.1455785287, b=>-2.5116225062
Iteration 1914: Achieved Loss=> 28.7892638292
Gradients: w=>0.1455011347, b=>-2.5102872506
Iteration 1915: Achieved Loss=> 28.7829427972
Gradients: w=>0.1454237818, b=>-2.5089527049
Iteration 1916: Achieved Loss=> 28.7766284843
Gradients: w=>0.1453464701, b=>-2.5076188686
Iteration 1917: Achieved Loss=> 28.7703208834
Gradients: w=>0.1452691995, b=>-2.5062857414
Iteration 1918: Achieved Loss=> 28.7640199874
Gradients: w=>0.1451919699, b=>-2.5049533230
Iteration 1919: Achieved Loss=> 28.7577257891
Gradients: w=>0.1451147814, b=>-2.5036216129
Iteration 1920: Achieved Loss=> 28.7514382814
Gradients: w=>0.1450376339, b=>-2.5022906108
Iteration 1921: Achieved Loss=> 28.7451574571
Gradients: w=>0.1449605275, b=>-2.5009603163
Iteration 1922: Achieved Loss=> 28.7388833093
Gradients: w=>0.1448834620, b=>-2.4996307291
Iteration 1923: Achieved Loss=> 28.7326158307
Gradients: w=>0.1448064375, b=>-2.4983018486
Iteration 1924: Achieved Loss=> 28.7263550143
Gradients: w=>0.1447294539, b=>-2.4969736747
Iteration 1925: Achieved Loss=> 28.7201008531
Gradients: w=>0.1446525113, b=>-2.4956462068
Iteration 1926: Achieved Loss=> 28.7138533399
Gradients: w=>0.1445756096, b=>-2.4943194447
Iteration 1927: Achieved Loss=> 28.7076124676
Gradients: w=>0.1444987488, b=>-2.4929933879
Iteration 1928: Achieved Loss=> 28.7013782293
Gradients: w=>0.1444219288, b=>-2.4916680361
Iteration 1929: Achieved Loss=> 28.6951506179
Gradients: w=>0.1443451497, b=>-2.4903433889
Iteration 1930: Achieved Loss=> 28.6889296262
Gradients: w=>0.1442684114, b=>-2.4890194459
Iteration 1931: Achieved Loss=> 28.6827152474
Gradients: w=>0.1441917138, b=>-2.4876962068
Iteration 1932: Achieved Loss=> 28.6765074742
Gradients: w=>0.1441150571, b=>-2.4863736711
Iteration 1933: Achieved Loss=> 28.6703062999
Gradients: w=>0.1440384411, b=>-2.4850518386
Iteration 1934: Achieved Loss=> 28.6641117172
Gradients: w=>0.1439618659, b=>-2.4837307087
Iteration 1935: Achieved Loss=> 28.6579237193
Gradients: w=>0.1438853313, b=>-2.4824102812
Iteration 1936: Achieved Loss=> 28.6517422990
Gradients: w=>0.1438088375, b=>-2.4810905557
Iteration 1937: Achieved Loss=> 28.6455674495
Gradients: w=>0.1437323843, b=>-2.4797715318
Iteration 1938: Achieved Loss=> 28.6393991637
Gradients: w=>0.1436559717, b=>-2.4784532092
Iteration 1939: Achieved Loss=> 28.6332374347
Gradients: w=>0.1435795998, b=>-2.4771355874
Iteration 1940: Achieved Loss=> 28.6270822554
Gradients: w=>0.1435032685, b=>-2.4758186661
Iteration 1941: Achieved Loss=> 28.6209336190
Gradients: w=>0.1434269777, b=>-2.4745024449
Iteration 1942: Achieved Loss=> 28.6147915185
Gradients: w=>0.1433507275, b=>-2.4731869234
Iteration 1943: Achieved Loss=> 28.6086559468
Gradients: w=>0.1432745179, b=>-2.4718721013
Iteration 1944: Achieved Loss=> 28.6025268972
Gradients: w=>0.1431983488, b=>-2.4705579783
Iteration 1945: Achieved Loss=> 28.5964043626
Gradients: w=>0.1431222201, b=>-2.4692445538
Iteration 1946: Achieved Loss=> 28.5902883361
Gradients: w=>0.1430461320, b=>-2.4679318276
Iteration 1947: Achieved Loss=> 28.5841788109
Gradients: w=>0.1429700842, b=>-2.4666197993
Iteration 1948: Achieved Loss=> 28.5780757799
Gradients: w=>0.1428940770, b=>-2.4653084685
Iteration 1949: Achieved Loss=> 28.5719792363
Gradients: w=>0.1428181101, b=>-2.4639978348
Iteration 1950: Achieved Loss=> 28.5658891732
Gradients: w=>0.1427421836, b=>-2.4626878980
Iteration 1951: Achieved Loss=> 28.5598055838
Gradients: w=>0.1426662975, b=>-2.4613786575
Iteration 1952: Achieved Loss=> 28.5537284610
Gradients: w=>0.1425904517, b=>-2.4600701130
Iteration 1953: Achieved Loss=> 28.5476577981
Gradients: w=>0.1425146462, b=>-2.4587622643
Iteration 1954: Achieved Loss=> 28.5415935882
Gradients: w=>0.1424388811, b=>-2.4574551108
Iteration 1955: Achieved Loss=> 28.5355358244
Gradients: w=>0.1423631562, b=>-2.4561486522
Iteration 1956: Achieved Loss=> 28.5294844999
Gradients: w=>0.1422874716, b=>-2.4548428882
Iteration 1957: Achieved Loss=> 28.5234396079
Gradients: w=>0.1422118272, b=>-2.4535378183
Iteration 1958: Achieved Loss=> 28.5174011414
Gradients: w=>0.1421362230, b=>-2.4522334423
Iteration 1959: Achieved Loss=> 28.5113690936
Gradients: w=>0.1420606590, b=>-2.4509297598
Iteration 1960: Achieved Loss=> 28.5053434579
Gradients: w=>0.1419851352, b=>-2.4496267703
Iteration 1961: Achieved Loss=> 28.4993242272
Gradients: w=>0.1419096516, b=>-2.4483244735
Iteration 1962: Achieved Loss=> 28.4933113949
Gradients: w=>0.1418342080, b=>-2.4470228691
Iteration 1963: Achieved Loss=> 28.4873049540
Gradients: w=>0.1417588046, b=>-2.4457219566
Iteration 1964: Achieved Loss=> 28.4813048979
Gradients: w=>0.1416834413, b=>-2.4444217357
Iteration 1965: Achieved Loss=> 28.4753112197
Gradients: w=>0.1416081180, b=>-2.4431222061
Iteration 1966: Achieved Loss=> 28.4693239127
Gradients: w=>0.1415328348, b=>-2.4418233673
Iteration 1967: Achieved Loss=> 28.4633429700
Gradients: w=>0.1414575916, b=>-2.4405252191
Iteration 1968: Achieved Loss=> 28.4573683850
Gradients: w=>0.1413823884, b=>-2.4392277610
Iteration 1969: Achieved Loss=> 28.4514001508
Gradients: w=>0.1413072252, b=>-2.4379309926
Iteration 1970: Achieved Loss=> 28.4454382607
Gradients: w=>0.1412321019, b=>-2.4366349137
Iteration 1971: Achieved Loss=> 28.4394827080
Gradients: w=>0.1411570186, b=>-2.4353395238
Iteration 1972: Achieved Loss=> 28.4335334859
Gradients: w=>0.1410819752, b=>-2.4340448225
Iteration 1973: Achieved Loss=> 28.4275905878
Gradients: w=>0.1410069717, b=>-2.4327508096
Iteration 1974: Achieved Loss=> 28.4216540067
Gradients: w=>0.1409320081, b=>-2.4314574846
Iteration 1975: Achieved Loss=> 28.4157237362
Gradients: w=>0.1408570843, b=>-2.4301648471
Iteration 1976: Achieved Loss=> 28.4097997694
Gradients: w=>0.1407822003, b=>-2.4288728969
Iteration 1977: Achieved Loss=> 28.4038820996
Gradients: w=>0.1407073562, b=>-2.4275816335
Iteration 1978: Achieved Loss=> 28.3979707202
Gradients: w=>0.1406325518, b=>-2.4262910566
Iteration 1979: Achieved Loss=> 28.3920656245
Gradients: w=>0.1405577873, b=>-2.4250011658
Iteration 1980: Achieved Loss=> 28.3861668058
Gradients: w=>0.1404830624, b=>-2.4237119608
Iteration 1981: Achieved Loss=> 28.3802742574
Gradients: w=>0.1404083773, b=>-2.4224234411
Iteration 1982: Achieved Loss=> 28.3743879726
Gradients: w=>0.1403337319, b=>-2.4211356064
Iteration 1983: Achieved Loss=> 28.3685079448
Gradients: w=>0.1402591262, b=>-2.4198484564
Iteration 1984: Achieved Loss=> 28.3626341674
Gradients: w=>0.1401845601, b=>-2.4185619907
Iteration 1985: Achieved Loss=> 28.3567666337
Gradients: w=>0.1401100337, b=>-2.4172762089
Iteration 1986: Achieved Loss=> 28.3509053371
Gradients: w=>0.1400355469, b=>-2.4159911107
Iteration 1987: Achieved Loss=> 28.3450502708
Gradients: w=>0.1399610997, b=>-2.4147066957
Iteration 1988: Achieved Loss=> 28.3392014284
Gradients: w=>0.1398866921, b=>-2.4134229635
Iteration 1989: Achieved Loss=> 28.3333588032
Gradients: w=>0.1398123240, b=>-2.4121399137
Iteration 1990: Achieved Loss=> 28.3275223886
Gradients: w=>0.1397379955, b=>-2.4108575461
Iteration 1991: Achieved Loss=> 28.3216921779
Gradients: w=>0.1396637065, b=>-2.4095758602
Iteration 1992: Achieved Loss=> 28.3158681647
Gradients: w=>0.1395894570, b=>-2.4082948558
Iteration 1993: Achieved Loss=> 28.3100503422
Gradients: w=>0.1395152470, b=>-2.4070145323
Iteration 1994: Achieved Loss=> 28.3042387040
Gradients: w=>0.1394410764, b=>-2.4057348895
Iteration 1995: Achieved Loss=> 28.2984332434
Gradients: w=>0.1393669452, b=>-2.4044559270
Iteration 1996: Achieved Loss=> 28.2926339539
Gradients: w=>0.1392928535, b=>-2.4031776444
Iteration 1997: Achieved Loss=> 28.2868408289
Gradients: w=>0.1392188011, b=>-2.4019000414
Iteration 1998: Achieved Loss=> 28.2810538619
Gradients: w=>0.1391447881, b=>-2.4006231176
Iteration 1999: Achieved Loss=> 28.2752730463
Gradients: w=>0.1390708145, b=>-2.3993468727
Iteration 2000: Achieved Loss=> 28.2694983756
Gradients: w=>0.1389968802, b=>-2.3980713062
Iteration 2001: Achieved Loss=> 28.2637298433
Gradients: w=>0.1389229852, b=>-2.3967964179
Iteration 2002: Achieved Loss=> 28.2579674428
Gradients: w=>0.1388491294, b=>-2.3955222074
Iteration 2003: Achieved Loss=> 28.2522111676
Gradients: w=>0.1387753130, b=>-2.3942486742
Iteration 2004: Achieved Loss=> 28.2464610111
Gradients: w=>0.1387015358, b=>-2.3929758181
Iteration 2005: Achieved Loss=> 28.2407169670
Gradients: w=>0.1386277978, b=>-2.3917036387
Iteration 2006: Achieved Loss=> 28.2349790287
Gradients: w=>0.1385540990, b=>-2.3904321357
Iteration 2007: Achieved Loss=> 28.2292471897
Gradients: w=>0.1384804394, b=>-2.3891613086
Iteration 2008: Achieved Loss=> 28.2235214435
Gradients: w=>0.1384068189, b=>-2.3878911571
Iteration 2009: Achieved Loss=> 28.2178017836
Gradients: w=>0.1383332376, b=>-2.3866216809
Iteration 2010: Achieved Loss=> 28.2120882037
Gradients: w=>0.1382596954, b=>-2.3853528795
Iteration 2011: Achieved Loss=> 28.2063806971
Gradients: w=>0.1381861923, b=>-2.3840847527
Iteration 2012: Achieved Loss=> 28.2006792575
Gradients: w=>0.1381127283, b=>-2.3828173001
Iteration 2013: Achieved Loss=> 28.1949838784
Gradients: w=>0.1380393033, b=>-2.3815505213
Iteration 2014: Achieved Loss=> 28.1892945534
Gradients: w=>0.1379659174, b=>-2.3802844159
Iteration 2015: Achieved Loss=> 28.1836112760
Gradients: w=>0.1378925705, b=>-2.3790189836
Iteration 2016: Achieved Loss=> 28.1779340398
Gradients: w=>0.1378192626, b=>-2.3777542241
Iteration 2017: Achieved Loss=> 28.1722628384
Gradients: w=>0.1377459936, b=>-2.3764901370
Iteration 2018: Achieved Loss=> 28.1665976654
Gradients: w=>0.1376727636, b=>-2.3752267219
Iteration 2019: Achieved Loss=> 28.1609385143
Gradients: w=>0.1375995725, b=>-2.3739639785
Iteration 2020: Achieved Loss=> 28.1552853788
Gradients: w=>0.1375264204, b=>-2.3727019063
Iteration 2021: Achieved Loss=> 28.1496382524
Gradients: w=>0.1374533071, b=>-2.3714405052
Iteration 2022: Achieved Loss=> 28.1439971288
Gradients: w=>0.1373802327, b=>-2.3701797746
Iteration 2023: Achieved Loss=> 28.1383620016
Gradients: w=>0.1373071972, b=>-2.3689197143
Iteration 2024: Achieved Loss=> 28.1327328645
Gradients: w=>0.1372342005, b=>-2.3676603239
Iteration 2025: Achieved Loss=> 28.1271097109
Gradients: w=>0.1371612425, b=>-2.3664016030
Iteration 2026: Achieved Loss=> 28.1214925347
Gradients: w=>0.1370883234, b=>-2.3651435512
Iteration 2027: Achieved Loss=> 28.1158813294
Gradients: w=>0.1370154431, b=>-2.3638861683
Iteration 2028: Achieved Loss=> 28.1102760887
Gradients: w=>0.1369426014, b=>-2.3626294539
Iteration 2029: Achieved Loss=> 28.1046768063
Gradients: w=>0.1368697985, b=>-2.3613734075
Iteration 2030: Achieved Loss=> 28.0990834758
Gradients: w=>0.1367970344, b=>-2.3601180290
Iteration 2031: Achieved Loss=> 28.0934960908
Gradients: w=>0.1367243089, b=>-2.3588633178
Iteration 2032: Achieved Loss=> 28.0879146452
Gradients: w=>0.1366516220, b=>-2.3576092736
Iteration 2033: Achieved Loss=> 28.0823391325
Gradients: w=>0.1365789738, b=>-2.3563558962
Iteration 2034: Achieved Loss=> 28.0767695464
Gradients: w=>0.1365063642, b=>-2.3551031850
Iteration 2035: Achieved Loss=> 28.0712058807
Gradients: w=>0.1364337933, b=>-2.3538511399
Iteration 2036: Achieved Loss=> 28.0656481291
Gradients: w=>0.1363612609, b=>-2.3525997604
Iteration 2037: Achieved Loss=> 28.0600962852
Gradients: w=>0.1362887670, b=>-2.3513490461
Iteration 2038: Achieved Loss=> 28.0545503429
Gradients: w=>0.1362163118, b=>-2.3500989968
Iteration 2039: Achieved Loss=> 28.0490102957
Gradients: w=>0.1361438950, b=>-2.3488496121
Iteration 2040: Achieved Loss=> 28.0434761375
Gradients: w=>0.1360715167, b=>-2.3476008915
Iteration 2041: Achieved Loss=> 28.0379478620
Gradients: w=>0.1359991769, b=>-2.3463528348
Iteration 2042: Achieved Loss=> 28.0324254629
Gradients: w=>0.1359268756, b=>-2.3451054416
Iteration 2043: Achieved Loss=> 28.0269089340
Gradients: w=>0.1358546127, b=>-2.3438587116
Iteration 2044: Achieved Loss=> 28.0213982691
Gradients: w=>0.1357823882, b=>-2.3426126443
Iteration 2045: Achieved Loss=> 28.0158934619
Gradients: w=>0.1357102021, b=>-2.3413672396
Iteration 2046: Achieved Loss=> 28.0103945062
Gradients: w=>0.1356380544, b=>-2.3401224969
Iteration 2047: Achieved Loss=> 28.0049013957
Gradients: w=>0.1355659451, b=>-2.3388784159
Iteration 2048: Achieved Loss=> 27.9994141244
Gradients: w=>0.1354938740, b=>-2.3376349964
Iteration 2049: Achieved Loss=> 27.9939326858
Gradients: w=>0.1354218413, b=>-2.3363922378
Iteration 2050: Achieved Loss=> 27.9884570739
Gradients: w=>0.1353498469, b=>-2.3351501400
Iteration 2051: Achieved Loss=> 27.9829872825
Gradients: w=>0.1352778908, b=>-2.3339087025
Iteration 2052: Achieved Loss=> 27.9775233054
Gradients: w=>0.1352059729, b=>-2.3326679250
Iteration 2053: Achieved Loss=> 27.9720651363
Gradients: w=>0.1351340933, b=>-2.3314278072
Iteration 2054: Achieved Loss=> 27.9666127692
Gradients: w=>0.1350622518, b=>-2.3301883486
Iteration 2055: Achieved Loss=> 27.9611661978
Gradients: w=>0.1349904486, b=>-2.3289495489
Iteration 2056: Achieved Loss=> 27.9557254160
Gradients: w=>0.1349186835, b=>-2.3277114078
Iteration 2057: Achieved Loss=> 27.9502904177
Gradients: w=>0.1348469566, b=>-2.3264739250
Iteration 2058: Achieved Loss=> 27.9448611966
Gradients: w=>0.1347752678, b=>-2.3252371001
Iteration 2059: Achieved Loss=> 27.9394377467
Gradients: w=>0.1347036172, b=>-2.3240009327
Iteration 2060: Achieved Loss=> 27.9340200618
Gradients: w=>0.1346320046, b=>-2.3227654224
Iteration 2061: Achieved Loss=> 27.9286081357
Gradients: w=>0.1345604301, b=>-2.3215305690
Iteration 2062: Achieved Loss=> 27.9232019625
Gradients: w=>0.1344888936, b=>-2.3202963721
Iteration 2063: Achieved Loss=> 27.9178015358
Gradients: w=>0.1344173952, b=>-2.3190628313
Iteration 2064: Achieved Loss=> 27.9124068498
Gradients: w=>0.1343459348, b=>-2.3178299464
Iteration 2065: Achieved Loss=> 27.9070178981
Gradients: w=>0.1342745124, b=>-2.3165977168
Iteration 2066: Achieved Loss=> 27.9016346748
Gradients: w=>0.1342031279, b=>-2.3153661424
Iteration 2067: Achieved Loss=> 27.8962571738
Gradients: w=>0.1341317814, b=>-2.3141352226
Iteration 2068: Achieved Loss=> 27.8908853889
Gradients: w=>0.1340604728, b=>-2.3129049573
Iteration 2069: Achieved Loss=> 27.8855193141
Gradients: w=>0.1339892022, b=>-2.3116753461
Iteration 2070: Achieved Loss=> 27.8801589433
Gradients: w=>0.1339179694, b=>-2.3104463885
Iteration 2071: Achieved Loss=> 27.8748042705
Gradients: w=>0.1338467745, b=>-2.3092180843
Iteration 2072: Achieved Loss=> 27.8694552896
Gradients: w=>0.1337756174, b=>-2.3079904331
Iteration 2073: Achieved Loss=> 27.8641119946
Gradients: w=>0.1337044982, b=>-2.3067634345
Iteration 2074: Achieved Loss=> 27.8587743793
Gradients: w=>0.1336334168, b=>-2.3055370883
Iteration 2075: Achieved Loss=> 27.8534424379
Gradients: w=>0.1335623732, b=>-2.3043113940
Iteration 2076: Achieved Loss=> 27.8481161641
Gradients: w=>0.1334913673, b=>-2.3030863513
Iteration 2077: Achieved Loss=> 27.8427955521
Gradients: w=>0.1334203992, b=>-2.3018619599
Iteration 2078: Achieved Loss=> 27.8374805958
Gradients: w=>0.1333494688, b=>-2.3006382194
Iteration 2079: Achieved Loss=> 27.8321712892
Gradients: w=>0.1332785762, b=>-2.2994151295
Iteration 2080: Achieved Loss=> 27.8268676262
Gradients: w=>0.1332077212, b=>-2.2981926899
Iteration 2081: Achieved Loss=> 27.8215696010
Gradients: w=>0.1331369039, b=>-2.2969709001
Iteration 2082: Achieved Loss=> 27.8162772074
Gradients: w=>0.1330661242, b=>-2.2957497599
Iteration 2083: Achieved Loss=> 27.8109904395
Gradients: w=>0.1329953822, b=>-2.2945292688
Iteration 2084: Achieved Loss=> 27.8057092914
Gradients: w=>0.1329246777, b=>-2.2933094266
Iteration 2085: Achieved Loss=> 27.8004337570
Gradients: w=>0.1328540109, b=>-2.2920902330
Iteration 2086: Achieved Loss=> 27.7951638303
Gradients: w=>0.1327833816, b=>-2.2908716874
Iteration 2087: Achieved Loss=> 27.7898995055
Gradients: w=>0.1327127899, b=>-2.2896537897
Iteration 2088: Achieved Loss=> 27.7846407766
Gradients: w=>0.1326422357, b=>-2.2884365395
Iteration 2089: Achieved Loss=> 27.7793876376
Gradients: w=>0.1325717190, b=>-2.2872199364
Iteration 2090: Achieved Loss=> 27.7741400825
Gradients: w=>0.1325012398, b=>-2.2860039801
Iteration 2091: Achieved Loss=> 27.7688981055
Gradients: w=>0.1324307981, b=>-2.2847886702
Iteration 2092: Achieved Loss=> 27.7636617006
Gradients: w=>0.1323603938, b=>-2.2835740064
Iteration 2093: Achieved Loss=> 27.7584308619
Gradients: w=>0.1322900270, b=>-2.2823599883
Iteration 2094: Achieved Loss=> 27.7532055835
Gradients: w=>0.1322196976, b=>-2.2811466157
Iteration 2095: Achieved Loss=> 27.7479858594
Gradients: w=>0.1321494055, b=>-2.2799338882
Iteration 2096: Achieved Loss=> 27.7427716838
Gradients: w=>0.1320791508, b=>-2.2787218053
Iteration 2097: Achieved Loss=> 27.7375630507
Gradients: w=>0.1320089335, b=>-2.2775103669
Iteration 2098: Achieved Loss=> 27.7323599543
Gradients: w=>0.1319387535, b=>-2.2762995724
Iteration 2099: Achieved Loss=> 27.7271623887
Gradients: w=>0.1318686108, b=>-2.2750894217
Iteration 2100: Achieved Loss=> 27.7219703480
Gradients: w=>0.1317985054, b=>-2.2738799144
Iteration 2101: Achieved Loss=> 27.7167838263
Gradients: w=>0.1317284373, b=>-2.2726710500
Iteration 2102: Achieved Loss=> 27.7116028178
Gradients: w=>0.1316584064, b=>-2.2714628283
Iteration 2103: Achieved Loss=> 27.7064273165
Gradients: w=>0.1315884127, b=>-2.2702552489
Iteration 2104: Achieved Loss=> 27.7012573168
Gradients: w=>0.1315184563, b=>-2.2690483116
Iteration 2105: Achieved Loss=> 27.6960928126
Gradients: w=>0.1314485370, b=>-2.2678420158
Iteration 2106: Achieved Loss=> 27.6909337982
Gradients: w=>0.1313786550, b=>-2.2666363614
Iteration 2107: Achieved Loss=> 27.6857802677
Gradients: w=>0.1313088100, b=>-2.2654313480
Iteration 2108: Achieved Loss=> 27.6806322153
Gradients: w=>0.1312390023, b=>-2.2642269751
Iteration 2109: Achieved Loss=> 27.6754896352
Gradients: w=>0.1311692316, b=>-2.2630232425
Iteration 2110: Achieved Loss=> 27.6703525215
Gradients: w=>0.1310994980, b=>-2.2618201499
Iteration 2111: Achieved Loss=> 27.6652208684
Gradients: w=>0.1310298015, b=>-2.2606176969
Iteration 2112: Achieved Loss=> 27.6600946702
Gradients: w=>0.1309601420, b=>-2.2594158832
Iteration 2113: Achieved Loss=> 27.6549739211
Gradients: w=>0.1308905196, b=>-2.2582147083
Iteration 2114: Achieved Loss=> 27.6498586152
Gradients: w=>0.1308209341, b=>-2.2570141721
Iteration 2115: Achieved Loss=> 27.6447487467
Gradients: w=>0.1307513857, b=>-2.2558142741
Iteration 2116: Achieved Loss=> 27.6396443100
Gradients: w=>0.1306818743, b=>-2.2546150139
Iteration 2117: Achieved Loss=> 27.6345452991
Gradients: w=>0.1306123998, b=>-2.2534163914
Iteration 2118: Achieved Loss=> 27.6294517084
Gradients: w=>0.1305429622, b=>-2.2522184061
Iteration 2119: Achieved Loss=> 27.6243635321
Gradients: w=>0.1304735616, b=>-2.2510210576
Iteration 2120: Achieved Loss=> 27.6192807643
Gradients: w=>0.1304041978, b=>-2.2498243458
Iteration 2121: Achieved Loss=> 27.6142033995
Gradients: w=>0.1303348709, b=>-2.2486282701
Iteration 2122: Achieved Loss=> 27.6091314318
Gradients: w=>0.1302655809, b=>-2.2474328303
Iteration 2123: Achieved Loss=> 27.6040648555
Gradients: w=>0.1301963277, b=>-2.2462380260
Iteration 2124: Achieved Loss=> 27.5990036648
Gradients: w=>0.1301271114, b=>-2.2450438569
Iteration 2125: Achieved Loss=> 27.5939478541
Gradients: w=>0.1300579318, b=>-2.2438503227
Iteration 2126: Achieved Loss=> 27.5888974176
Gradients: w=>0.1299887890, b=>-2.2426574230
Iteration 2127: Achieved Loss=> 27.5838523496
Gradients: w=>0.1299196830, b=>-2.2414651574
Iteration 2128: Achieved Loss=> 27.5788126444
Gradients: w=>0.1298506137, b=>-2.2402735258
Iteration 2129: Achieved Loss=> 27.5737782963
Gradients: w=>0.1297815811, b=>-2.2390825276
Iteration 2130: Achieved Loss=> 27.5687492997
Gradients: w=>0.1297125853, b=>-2.2378921626
Iteration 2131: Achieved Loss=> 27.5637256487
Gradients: w=>0.1296436261, b=>-2.2367024304
Iteration 2132: Achieved Loss=> 27.5587073378
Gradients: w=>0.1295747035, b=>-2.2355133308
Iteration 2133: Achieved Loss=> 27.5536943612
Gradients: w=>0.1295058177, b=>-2.2343248633
Iteration 2134: Achieved Loss=> 27.5486867133
Gradients: w=>0.1294369684, b=>-2.2331370276
Iteration 2135: Achieved Loss=> 27.5436843884
Gradients: w=>0.1293681557, b=>-2.2319498234
Iteration 2136: Achieved Loss=> 27.5386873809
Gradients: w=>0.1292993796, b=>-2.2307632504
Iteration 2137: Achieved Loss=> 27.5336956851
Gradients: w=>0.1292306401, b=>-2.2295773081
Iteration 2138: Achieved Loss=> 27.5287092954
Gradients: w=>0.1291619371, b=>-2.2283919964
Iteration 2139: Achieved Loss=> 27.5237282061
Gradients: w=>0.1290932707, b=>-2.2272073148
Iteration 2140: Achieved Loss=> 27.5187524116
Gradients: w=>0.1290246407, b=>-2.2260232630
Iteration 2141: Achieved Loss=> 27.5137819063
Gradients: w=>0.1289560473, b=>-2.2248398408
Iteration 2142: Achieved Loss=> 27.5088166845
Gradients: w=>0.1288874903, b=>-2.2236570476
Iteration 2143: Achieved Loss=> 27.5038567406
Gradients: w=>0.1288189697, b=>-2.2224748833
Iteration 2144: Achieved Loss=> 27.4989020691
Gradients: w=>0.1287504856, b=>-2.2212933474
Iteration 2145: Achieved Loss=> 27.4939526642
Gradients: w=>0.1286820379, b=>-2.2201124397
Iteration 2146: Achieved Loss=> 27.4890085205
Gradients: w=>0.1286136266, b=>-2.2189321598
Iteration 2147: Achieved Loss=> 27.4840696323
Gradients: w=>0.1285452516, b=>-2.2177525073
Iteration 2148: Achieved Loss=> 27.4791359940
Gradients: w=>0.1284769130, b=>-2.2165734820
Iteration 2149: Achieved Loss=> 27.4742076001
Gradients: w=>0.1284086108, b=>-2.2153950835
Iteration 2150: Achieved Loss=> 27.4692844449
Gradients: w=>0.1283403448, b=>-2.2142173115
Iteration 2151: Achieved Loss=> 27.4643665230
Gradients: w=>0.1282721151, b=>-2.2130401656
Iteration 2152: Achieved Loss=> 27.4594538287
Gradients: w=>0.1282039218, b=>-2.2118636455
Iteration 2153: Achieved Loss=> 27.4545463565
Gradients: w=>0.1281357646, b=>-2.2106877509
Iteration 2154: Achieved Loss=> 27.4496441008
Gradients: w=>0.1280676437, b=>-2.2095124814
Iteration 2155: Achieved Loss=> 27.4447470561
Gradients: w=>0.1279995590, b=>-2.2083378368
Iteration 2156: Achieved Loss=> 27.4398552169
Gradients: w=>0.1279315105, b=>-2.2071638166
Iteration 2157: Achieved Loss=> 27.4349685776
Gradients: w=>0.1278634982, b=>-2.2059904206
Iteration 2158: Achieved Loss=> 27.4300871327
Gradients: w=>0.1277955220, b=>-2.2048176483
Iteration 2159: Achieved Loss=> 27.4252108767
Gradients: w=>0.1277275820, b=>-2.2036454996
Iteration 2160: Achieved Loss=> 27.4203398040
Gradients: w=>0.1276596781, b=>-2.2024739740
Iteration 2161: Achieved Loss=> 27.4154739092
Gradients: w=>0.1275918103, b=>-2.2013030713
Iteration 2162: Achieved Loss=> 27.4106131867
Gradients: w=>0.1275239786, b=>-2.2001327910
Iteration 2163: Achieved Loss=> 27.4057576311
Gradients: w=>0.1274561829, b=>-2.1989631328
Iteration 2164: Achieved Loss=> 27.4009072368
Gradients: w=>0.1273884233, b=>-2.1977940965
Iteration 2165: Achieved Loss=> 27.3960619984
Gradients: w=>0.1273206997, b=>-2.1966256817
Iteration 2166: Achieved Loss=> 27.3912219103
Gradients: w=>0.1272530121, b=>-2.1954578881
Iteration 2167: Achieved Loss=> 27.3863869672
Gradients: w=>0.1271853605, b=>-2.1942907153
Iteration 2168: Achieved Loss=> 27.3815571635
Gradients: w=>0.1271177449, b=>-2.1931241630
Iteration 2169: Achieved Loss=> 27.3767324938
Gradients: w=>0.1270501652, b=>-2.1919582309
Iteration 2170: Achieved Loss=> 27.3719129527
Gradients: w=>0.1269826214, b=>-2.1907929186
Iteration 2171: Achieved Loss=> 27.3670985345
Gradients: w=>0.1269151136, b=>-2.1896282258
Iteration 2172: Achieved Loss=> 27.3622892341
Gradients: w=>0.1268476416, b=>-2.1884641523
Iteration 2173: Achieved Loss=> 27.3574850457
Gradients: w=>0.1267802055, b=>-2.1873006976
Iteration 2174: Achieved Loss=> 27.3526859642
Gradients: w=>0.1267128052, b=>-2.1861378614
Iteration 2175: Achieved Loss=> 27.3478919840
Gradients: w=>0.1266454408, b=>-2.1849756434
Iteration 2176: Achieved Loss=> 27.3431030996
Gradients: w=>0.1265781122, b=>-2.1838140433
Iteration 2177: Achieved Loss=> 27.3383193058
Gradients: w=>0.1265108194, b=>-2.1826530607
Iteration 2178: Achieved Loss=> 27.3335405970
Gradients: w=>0.1264435624, b=>-2.1814926953
Iteration 2179: Achieved Loss=> 27.3287669679
Gradients: w=>0.1263763411, b=>-2.1803329468
Iteration 2180: Achieved Loss=> 27.3239984130
Gradients: w=>0.1263091556, b=>-2.1791738149
Iteration 2181: Achieved Loss=> 27.3192349271
Gradients: w=>0.1262420057, b=>-2.1780152992
Iteration 2182: Achieved Loss=> 27.3144765046
Gradients: w=>0.1261748916, b=>-2.1768573994
Iteration 2183: Achieved Loss=> 27.3097231402
Gradients: w=>0.1261078132, b=>-2.1757001152
Iteration 2184: Achieved Loss=> 27.3049748285
Gradients: w=>0.1260407704, b=>-2.1745434463
Iteration 2185: Achieved Loss=> 27.3002315642
Gradients: w=>0.1259737632, b=>-2.1733873922
Iteration 2186: Achieved Loss=> 27.2954933419
Gradients: w=>0.1259067917, b=>-2.1722319528
Iteration 2187: Achieved Loss=> 27.2907601562
Gradients: w=>0.1258398558, b=>-2.1710771276
Iteration 2188: Achieved Loss=> 27.2860320018
Gradients: w=>0.1257729555, b=>-2.1699229164
Iteration 2189: Achieved Loss=> 27.2813088733
Gradients: w=>0.1257060907, b=>-2.1687693187
Iteration 2190: Achieved Loss=> 27.2765907654
Gradients: w=>0.1256392615, b=>-2.1676163344
Iteration 2191: Achieved Loss=> 27.2718776728
Gradients: w=>0.1255724678, b=>-2.1664639630
Iteration 2192: Achieved Loss=> 27.2671695900
Gradients: w=>0.1255057096, b=>-2.1653122043
Iteration 2193: Achieved Loss=> 27.2624665119
Gradients: w=>0.1254389869, b=>-2.1641610578
Iteration 2194: Achieved Loss=> 27.2577684330
Gradients: w=>0.1253722997, b=>-2.1630105234
Iteration 2195: Achieved Loss=> 27.2530753481
Gradients: w=>0.1253056480, b=>-2.1618606006
Iteration 2196: Achieved Loss=> 27.2483872518
Gradients: w=>0.1252390316, b=>-2.1607112891
Iteration 2197: Achieved Loss=> 27.2437041389
Gradients: w=>0.1251724507, b=>-2.1595625887
Iteration 2198: Achieved Loss=> 27.2390260041
Gradients: w=>0.1251059052, b=>-2.1584144989
Iteration 2199: Achieved Loss=> 27.2343528420
Gradients: w=>0.1250393951, b=>-2.1572670195
Iteration 2200: Achieved Loss=> 27.2296846473
Gradients: w=>0.1249729203, b=>-2.1561201502
Iteration 2201: Achieved Loss=> 27.2250214149
Gradients: w=>0.1249064808, b=>-2.1549738905
Iteration 2202: Achieved Loss=> 27.2203631393
Gradients: w=>0.1248400767, b=>-2.1538282403
Iteration 2203: Achieved Loss=> 27.2157098155
Gradients: w=>0.1247737079, b=>-2.1526831991
Iteration 2204: Achieved Loss=> 27.2110614380
Gradients: w=>0.1247073744, b=>-2.1515387666
Iteration 2205: Achieved Loss=> 27.2064180016
Gradients: w=>0.1246410761, b=>-2.1503949425
Iteration 2206: Achieved Loss=> 27.2017795011
Gradients: w=>0.1245748131, b=>-2.1492517266
Iteration 2207: Achieved Loss=> 27.1971459312
Gradients: w=>0.1245085853, b=>-2.1481091184
Iteration 2208: Achieved Loss=> 27.1925172868
Gradients: w=>0.1244423927, b=>-2.1469671177
Iteration 2209: Achieved Loss=> 27.1878935624
Gradients: w=>0.1243762353, b=>-2.1458257240
Iteration 2210: Achieved Loss=> 27.1832747530
Gradients: w=>0.1243101131, b=>-2.1446849372
Iteration 2211: Achieved Loss=> 27.1786608533
Gradients: w=>0.1242440260, b=>-2.1435447569
Iteration 2212: Achieved Loss=> 27.1740518581
Gradients: w=>0.1241779741, b=>-2.1424051827
Iteration 2213: Achieved Loss=> 27.1694477621
Gradients: w=>0.1241119572, b=>-2.1412662143
Iteration 2214: Achieved Loss=> 27.1648485602
Gradients: w=>0.1240459755, b=>-2.1401278515
Iteration 2215: Achieved Loss=> 27.1602542471
Gradients: w=>0.1239800289, b=>-2.1389900938
Iteration 2216: Achieved Loss=> 27.1556648177
Gradients: w=>0.1239141173, b=>-2.1378529410
Iteration 2217: Achieved Loss=> 27.1510802668
Gradients: w=>0.1238482407, b=>-2.1367163928
Iteration 2218: Achieved Loss=> 27.1465005891
Gradients: w=>0.1237823992, b=>-2.1355804488
Iteration 2219: Achieved Loss=> 27.1419257796
Gradients: w=>0.1237165927, b=>-2.1344451086
Iteration 2220: Achieved Loss=> 27.1373558329
Gradients: w=>0.1236508211, b=>-2.1333103721
Iteration 2221: Achieved Loss=> 27.1327907441
Gradients: w=>0.1235850846, b=>-2.1321762388
Iteration 2222: Achieved Loss=> 27.1282305078
Gradients: w=>0.1235193829, b=>-2.1310427085
Iteration 2223: Achieved Loss=> 27.1236751189
Gradients: w=>0.1234537163, b=>-2.1299097808
Iteration 2224: Achieved Loss=> 27.1191245724
Gradients: w=>0.1233880845, b=>-2.1287774554
Iteration 2225: Achieved Loss=> 27.1145788629
Gradients: w=>0.1233224876, b=>-2.1276457319
Iteration 2226: Achieved Loss=> 27.1100379855
Gradients: w=>0.1232569256, b=>-2.1265146102
Iteration 2227: Achieved Loss=> 27.1055019349
Gradients: w=>0.1231913984, b=>-2.1253840897
Iteration 2228: Achieved Loss=> 27.1009707060
Gradients: w=>0.1231259061, b=>-2.1242541703
Iteration 2229: Achieved Loss=> 27.0964442938
Gradients: w=>0.1230604485, b=>-2.1231248516
Iteration 2230: Achieved Loss=> 27.0919226930
Gradients: w=>0.1229950258, b=>-2.1219961332
Iteration 2231: Achieved Loss=> 27.0874058986
Gradients: w=>0.1229296379, b=>-2.1208680150
Iteration 2232: Achieved Loss=> 27.0828939054
Gradients: w=>0.1228642847, b=>-2.1197404964
Iteration 2233: Achieved Loss=> 27.0783867084
Gradients: w=>0.1227989663, b=>-2.1186135773
Iteration 2234: Achieved Loss=> 27.0738843025
Gradients: w=>0.1227336826, b=>-2.1174872573
Iteration 2235: Achieved Loss=> 27.0693866825
Gradients: w=>0.1226684336, b=>-2.1163615361
Iteration 2236: Achieved Loss=> 27.0648938434
Gradients: w=>0.1226032193, b=>-2.1152364133
Iteration 2237: Achieved Loss=> 27.0604057801
Gradients: w=>0.1225380397, b=>-2.1141118887
Iteration 2238: Achieved Loss=> 27.0559224875
Gradients: w=>0.1224728947, b=>-2.1129879619
Iteration 2239: Achieved Loss=> 27.0514439605
Gradients: w=>0.1224077843, b=>-2.1118646327
Iteration 2240: Achieved Loss=> 27.0469701941
Gradients: w=>0.1223427086, b=>-2.1107419006
Iteration 2241: Achieved Loss=> 27.0425011833
Gradients: w=>0.1222776675, b=>-2.1096197654
Iteration 2242: Achieved Loss=> 27.0380369229
Gradients: w=>0.1222126609, b=>-2.1084982268
Iteration 2243: Achieved Loss=> 27.0335774079
Gradients: w=>0.1221476889, b=>-2.1073772844
Iteration 2244: Achieved Loss=> 27.0291226333
Gradients: w=>0.1220827514, b=>-2.1062569380
Iteration 2245: Achieved Loss=> 27.0246725940
Gradients: w=>0.1220178485, b=>-2.1051371872
Iteration 2246: Achieved Loss=> 27.0202272850
Gradients: w=>0.1219529801, b=>-2.1040180316
Iteration 2247: Achieved Loss=> 27.0157867013
Gradients: w=>0.1218881461, b=>-2.1028994710
Iteration 2248: Achieved Loss=> 27.0113508378
Gradients: w=>0.1218233466, b=>-2.1017815051
Iteration 2249: Achieved Loss=> 27.0069196896
Gradients: w=>0.1217585816, b=>-2.1006641336
Iteration 2250: Achieved Loss=> 27.0024932515
Gradients: w=>0.1216938510, b=>-2.0995473560
Iteration 2251: Achieved Loss=> 26.9980715187
Gradients: w=>0.1216291548, b=>-2.0984311722
Iteration 2252: Achieved Loss=> 26.9936544861
Gradients: w=>0.1215644930, b=>-2.0973155818
Iteration 2253: Achieved Loss=> 26.9892421487
Gradients: w=>0.1214998656, b=>-2.0962005845
Iteration 2254: Achieved Loss=> 26.9848345015
Gradients: w=>0.1214352726, b=>-2.0950861799
Iteration 2255: Achieved Loss=> 26.9804315396
Gradients: w=>0.1213707138, b=>-2.0939723678
Iteration 2256: Achieved Loss=> 26.9760332579
Gradients: w=>0.1213061895, b=>-2.0928591478
Iteration 2257: Achieved Loss=> 26.9716396515
Gradients: w=>0.1212416994, b=>-2.0917465196
Iteration 2258: Achieved Loss=> 26.9672507154
Gradients: w=>0.1211772435, b=>-2.0906344830
Iteration 2259: Achieved Loss=> 26.9628664447
Gradients: w=>0.1211128220, b=>-2.0895230375
Iteration 2260: Achieved Loss=> 26.9584868343
Gradients: w=>0.1210484347, b=>-2.0884121829
Iteration 2261: Achieved Loss=> 26.9541118794
Gradients: w=>0.1209840817, b=>-2.0873019189
Iteration 2262: Achieved Loss=> 26.9497415749
Gradients: w=>0.1209197628, b=>-2.0861922451
Iteration 2263: Achieved Loss=> 26.9453759160
Gradients: w=>0.1208554781, b=>-2.0850831613
Iteration 2264: Achieved Loss=> 26.9410148977
Gradients: w=>0.1207912277, b=>-2.0839746671
Iteration 2265: Achieved Loss=> 26.9366585151
Gradients: w=>0.1207270113, b=>-2.0828667622
Iteration 2266: Achieved Loss=> 26.9323067632
Gradients: w=>0.1206628292, b=>-2.0817594463
Iteration 2267: Achieved Loss=> 26.9279596371
Gradients: w=>0.1205986811, b=>-2.0806527191
Iteration 2268: Achieved Loss=> 26.9236171319
Gradients: w=>0.1205345671, b=>-2.0795465802
Iteration 2269: Achieved Loss=> 26.9192792428
Gradients: w=>0.1204704873, b=>-2.0784410294
Iteration 2270: Achieved Loss=> 26.9149459647
Gradients: w=>0.1204064414, b=>-2.0773360664
Iteration 2271: Achieved Loss=> 26.9106172928
Gradients: w=>0.1203424297, b=>-2.0762316907
Iteration 2272: Achieved Loss=> 26.9062932221
Gradients: w=>0.1202784520, b=>-2.0751279023
Iteration 2273: Achieved Loss=> 26.9019737479
Gradients: w=>0.1202145082, b=>-2.0740247006
Iteration 2274: Achieved Loss=> 26.8976588652
Gradients: w=>0.1201505985, b=>-2.0729220854
Iteration 2275: Achieved Loss=> 26.8933485691
Gradients: w=>0.1200867228, b=>-2.0718200564
Iteration 2276: Achieved Loss=> 26.8890428548
Gradients: w=>0.1200228810, b=>-2.0707186132
Iteration 2277: Achieved Loss=> 26.8847417173
Gradients: w=>0.1199590731, b=>-2.0696177557
Iteration 2278: Achieved Loss=> 26.8804451519
Gradients: w=>0.1198952992, b=>-2.0685174833
Iteration 2279: Achieved Loss=> 26.8761531536
Gradients: w=>0.1198315592, b=>-2.0674177959
Iteration 2280: Achieved Loss=> 26.8718657176
Gradients: w=>0.1197678530, b=>-2.0663186932
Iteration 2281: Achieved Loss=> 26.8675828391
Gradients: w=>0.1197041808, b=>-2.0652201747
Iteration 2282: Achieved Loss=> 26.8633045132
Gradients: w=>0.1196405424, b=>-2.0641222403
Iteration 2283: Achieved Loss=> 26.8590307350
Gradients: w=>0.1195769378, b=>-2.0630248896
Iteration 2284: Achieved Loss=> 26.8547614998
Gradients: w=>0.1195133670, b=>-2.0619281222
Iteration 2285: Achieved Loss=> 26.8504968027
Gradients: w=>0.1194498300, b=>-2.0608319379
Iteration 2286: Achieved Loss=> 26.8462366389
Gradients: w=>0.1193863268, b=>-2.0597363364
Iteration 2287: Achieved Loss=> 26.8419810035
Gradients: w=>0.1193228574, b=>-2.0586413174
Iteration 2288: Achieved Loss=> 26.8377298918
Gradients: w=>0.1192594217, b=>-2.0575468804
Iteration 2289: Achieved Loss=> 26.8334832990
Gradients: w=>0.1191960197, b=>-2.0564530254
Iteration 2290: Achieved Loss=> 26.8292412202
Gradients: w=>0.1191326515, b=>-2.0553597518
Iteration 2291: Achieved Loss=> 26.8250036506
Gradients: w=>0.1190693169, b=>-2.0542670595
Iteration 2292: Achieved Loss=> 26.8207705854
Gradients: w=>0.1190060160, b=>-2.0531749481
Iteration 2293: Achieved Loss=> 26.8165420199
Gradients: w=>0.1189427487, b=>-2.0520834172
Iteration 2294: Achieved Loss=> 26.8123179493
Gradients: w=>0.1188795151, b=>-2.0509924667
Iteration 2295: Achieved Loss=> 26.8080983688
Gradients: w=>0.1188163151, b=>-2.0499020961
Iteration 2296: Achieved Loss=> 26.8038832736
Gradients: w=>0.1187531487, b=>-2.0488123053
Iteration 2297: Achieved Loss=> 26.7996726590
Gradients: w=>0.1186900159, b=>-2.0477230938
Iteration 2298: Achieved Loss=> 26.7954665202
Gradients: w=>0.1186269167, b=>-2.0466344613
Iteration 2299: Achieved Loss=> 26.7912648524
Gradients: w=>0.1185638509, b=>-2.0455464076
Iteration 2300: Achieved Loss=> 26.7870676509
Gradients: w=>0.1185008188, b=>-2.0444589324
Iteration 2301: Achieved Loss=> 26.7828749109
Gradients: w=>0.1184378201, b=>-2.0433720352
Iteration 2302: Achieved Loss=> 26.7786866277
Gradients: w=>0.1183748549, b=>-2.0422857159
Iteration 2303: Achieved Loss=> 26.7745027966
Gradients: w=>0.1183119232, b=>-2.0411999742
Iteration 2304: Achieved Loss=> 26.7703234128
Gradients: w=>0.1182490250, b=>-2.0401148096
Iteration 2305: Achieved Loss=> 26.7661484715
Gradients: w=>0.1181861601, b=>-2.0390302219
Iteration 2306: Achieved Loss=> 26.7619779682
Gradients: w=>0.1181233288, b=>-2.0379462109
Iteration 2307: Achieved Loss=> 26.7578118980
Gradients: w=>0.1180605308, b=>-2.0368627761
Iteration 2308: Achieved Loss=> 26.7536502563
Gradients: w=>0.1179977662, b=>-2.0357799174
Iteration 2309: Achieved Loss=> 26.7494930383
Gradients: w=>0.1179350349, b=>-2.0346976343
Iteration 2310: Achieved Loss=> 26.7453402393
Gradients: w=>0.1178723370, b=>-2.0336159266
Iteration 2311: Achieved Loss=> 26.7411918547
Gradients: w=>0.1178096725, b=>-2.0325347939
Iteration 2312: Achieved Loss=> 26.7370478797
Gradients: w=>0.1177470413, b=>-2.0314542360
Iteration 2313: Achieved Loss=> 26.7329083097
Gradients: w=>0.1176844433, b=>-2.0303742526
Iteration 2314: Achieved Loss=> 26.7287731400
Gradients: w=>0.1176218787, b=>-2.0292948433
Iteration 2315: Achieved Loss=> 26.7246423658
Gradients: w=>0.1175593473, b=>-2.0282160079
Iteration 2316: Achieved Loss=> 26.7205159826
Gradients: w=>0.1174968491, b=>-2.0271377460
Iteration 2317: Achieved Loss=> 26.7163939856
Gradients: w=>0.1174343842, b=>-2.0260600574
Iteration 2318: Achieved Loss=> 26.7122763702
Gradients: w=>0.1173719524, b=>-2.0249829416
Iteration 2319: Achieved Loss=> 26.7081631318
Gradients: w=>0.1173095539, b=>-2.0239063986
Iteration 2320: Achieved Loss=> 26.7040542656
Gradients: w=>0.1172471885, b=>-2.0228304278
Iteration 2321: Achieved Loss=> 26.6999497671
Gradients: w=>0.1171848563, b=>-2.0217550290
Iteration 2322: Achieved Loss=> 26.6958496316
Gradients: w=>0.1171225573, b=>-2.0206802020
Iteration 2323: Achieved Loss=> 26.6917538544
Gradients: w=>0.1170602913, b=>-2.0196059464
Iteration 2324: Achieved Loss=> 26.6876624310
Gradients: w=>0.1169980585, b=>-2.0185322619
Iteration 2325: Achieved Loss=> 26.6835753567
Gradients: w=>0.1169358587, b=>-2.0174591482
Iteration 2326: Achieved Loss=> 26.6794926268
Gradients: w=>0.1168736920, b=>-2.0163866050
Iteration 2327: Achieved Loss=> 26.6754142368
Gradients: w=>0.1168115584, b=>-2.0153146319
Iteration 2328: Achieved Loss=> 26.6713401820
Gradients: w=>0.1167494578, b=>-2.0142432288
Iteration 2329: Achieved Loss=> 26.6672704579
Gradients: w=>0.1166873902, b=>-2.0131723953
Iteration 2330: Achieved Loss=> 26.6632050598
Gradients: w=>0.1166253556, b=>-2.0121021311
Iteration 2331: Achieved Loss=> 26.6591439831
Gradients: w=>0.1165633540, b=>-2.0110324358
Iteration 2332: Achieved Loss=> 26.6550872233
Gradients: w=>0.1165013853, b=>-2.0099633092
Iteration 2333: Achieved Loss=> 26.6510347757
Gradients: w=>0.1164394496, b=>-2.0088947510
Iteration 2334: Achieved Loss=> 26.6469866357
Gradients: w=>0.1163775468, b=>-2.0078267609
Iteration 2335: Achieved Loss=> 26.6429427989
Gradients: w=>0.1163156769, b=>-2.0067593386
Iteration 2336: Achieved Loss=> 26.6389032606
Gradients: w=>0.1162538400, b=>-2.0056924837
Iteration 2337: Achieved Loss=> 26.6348680162
Gradients: w=>0.1161920358, b=>-2.0046261961
Iteration 2338: Achieved Loss=> 26.6308370612
Gradients: w=>0.1161302646, b=>-2.0035604752
Iteration 2339: Achieved Loss=> 26.6268103910
Gradients: w=>0.1160685262, b=>-2.0024953210
Iteration 2340: Achieved Loss=> 26.6227880011
Gradients: w=>0.1160068206, b=>-2.0014307330
Iteration 2341: Achieved Loss=> 26.6187698869
Gradients: w=>0.1159451478, b=>-2.0003667110
Iteration 2342: Achieved Loss=> 26.6147560439
Gradients: w=>0.1158835078, b=>-1.9993032547
Iteration 2343: Achieved Loss=> 26.6107464675
Gradients: w=>0.1158219006, b=>-1.9982403637
Iteration 2344: Achieved Loss=> 26.6067411532
Gradients: w=>0.1157603261, b=>-1.9971780378
Iteration 2345: Achieved Loss=> 26.6027400964
Gradients: w=>0.1156987844, b=>-1.9961162766
Iteration 2346: Achieved Loss=> 26.5987432927
Gradients: w=>0.1156372753, b=>-1.9950550799
Iteration 2347: Achieved Loss=> 26.5947507375
Gradients: w=>0.1155757990, b=>-1.9939944474
Iteration 2348: Achieved Loss=> 26.5907624263
Gradients: w=>0.1155143554, b=>-1.9929343788
Iteration 2349: Achieved Loss=> 26.5867783546
Gradients: w=>0.1154529444, b=>-1.9918748737
Iteration 2350: Achieved Loss=> 26.5827985179
Gradients: w=>0.1153915661, b=>-1.9908159318
Iteration 2351: Achieved Loss=> 26.5788229117
Gradients: w=>0.1153302204, b=>-1.9897575530
Iteration 2352: Achieved Loss=> 26.5748515315
Gradients: w=>0.1152689073, b=>-1.9886997368
Iteration 2353: Achieved Loss=> 26.5708843727
Gradients: w=>0.1152076268, b=>-1.9876424830
Iteration 2354: Achieved Loss=> 26.5669214310
Gradients: w=>0.1151463789, b=>-1.9865857912
Iteration 2355: Achieved Loss=> 26.5629627017
Gradients: w=>0.1150851635, b=>-1.9855296612
Iteration 2356: Achieved Loss=> 26.5590081806
Gradients: w=>0.1150239807, b=>-1.9844740927
Iteration 2357: Achieved Loss=> 26.5550578630
Gradients: w=>0.1149628304, b=>-1.9834190854
Iteration 2358: Achieved Loss=> 26.5511117445
Gradients: w=>0.1149017127, b=>-1.9823646389
Iteration 2359: Achieved Loss=> 26.5471698206
Gradients: w=>0.1148406274, b=>-1.9813107530
Iteration 2360: Achieved Loss=> 26.5432320869
Gradients: w=>0.1147795746, b=>-1.9802574274
Iteration 2361: Achieved Loss=> 26.5392985390
Gradients: w=>0.1147185542, b=>-1.9792046618
Iteration 2362: Achieved Loss=> 26.5353691723
Gradients: w=>0.1146575663, b=>-1.9781524558
Iteration 2363: Achieved Loss=> 26.5314439824
Gradients: w=>0.1145966109, b=>-1.9771008093
Iteration 2364: Achieved Loss=> 26.5275229650
Gradients: w=>0.1145356878, b=>-1.9760497218
Iteration 2365: Achieved Loss=> 26.5236061155
Gradients: w=>0.1144747971, b=>-1.9749991931
Iteration 2366: Achieved Loss=> 26.5196934295
Gradients: w=>0.1144139388, b=>-1.9739492229
Iteration 2367: Achieved Loss=> 26.5157849027
Gradients: w=>0.1143531128, b=>-1.9728998109
Iteration 2368: Achieved Loss=> 26.5118805305
Gradients: w=>0.1142923192, b=>-1.9718509568
Iteration 2369: Achieved Loss=> 26.5079803086
Gradients: w=>0.1142315579, b=>-1.9708026603
Iteration 2370: Achieved Loss=> 26.5040842325
Gradients: w=>0.1141708289, b=>-1.9697549211
Iteration 2371: Achieved Loss=> 26.5001922979
Gradients: w=>0.1141101322, b=>-1.9687077390
Iteration 2372: Achieved Loss=> 26.4963045003
Gradients: w=>0.1140494677, b=>-1.9676611135
Iteration 2373: Achieved Loss=> 26.4924208354
Gradients: w=>0.1139888355, b=>-1.9666150445
Iteration 2374: Achieved Loss=> 26.4885412987
Gradients: w=>0.1139282356, b=>-1.9655695315
Iteration 2375: Achieved Loss=> 26.4846658859
Gradients: w=>0.1138676678, b=>-1.9645245744
Iteration 2376: Achieved Loss=> 26.4807945926
Gradients: w=>0.1138071323, b=>-1.9634801729
Iteration 2377: Achieved Loss=> 26.4769274144
Gradients: w=>0.1137466289, b=>-1.9624363265
Iteration 2378: Achieved Loss=> 26.4730643469
Gradients: w=>0.1136861577, b=>-1.9613930352
Iteration 2379: Achieved Loss=> 26.4692053857
Gradients: w=>0.1136257186, b=>-1.9603502984
Iteration 2380: Achieved Loss=> 26.4653505266
Gradients: w=>0.1135653117, b=>-1.9593081160
Iteration 2381: Achieved Loss=> 26.4614997651
Gradients: w=>0.1135049369, b=>-1.9582664877
Iteration 2382: Achieved Loss=> 26.4576530968
Gradients: w=>0.1134445942, b=>-1.9572254131
Iteration 2383: Achieved Loss=> 26.4538105175
Gradients: w=>0.1133842836, b=>-1.9561848920
Iteration 2384: Achieved Loss=> 26.4499720228
Gradients: w=>0.1133240050, b=>-1.9551449241
Iteration 2385: Achieved Loss=> 26.4461376083
Gradients: w=>0.1132637585, b=>-1.9541055090
Iteration 2386: Achieved Loss=> 26.4423072697
Gradients: w=>0.1132035440, b=>-1.9530666465
Iteration 2387: Achieved Loss=> 26.4384810027
Gradients: w=>0.1131433615, b=>-1.9520283364
Iteration 2388: Achieved Loss=> 26.4346588029
Gradients: w=>0.1130832110, b=>-1.9509905782
Iteration 2389: Achieved Loss=> 26.4308406661
Gradients: w=>0.1130230925, b=>-1.9499533717
Iteration 2390: Achieved Loss=> 26.4270265878
Gradients: w=>0.1129630060, b=>-1.9489167166
Iteration 2391: Achieved Loss=> 26.4232165639
Gradients: w=>0.1129029514, b=>-1.9478806127
Iteration 2392: Achieved Loss=> 26.4194105898
Gradients: w=>0.1128429287, b=>-1.9468450596
Iteration 2393: Achieved Loss=> 26.4156086615
Gradients: w=>0.1127829380, b=>-1.9458100570
Iteration 2394: Achieved Loss=> 26.4118107746
Gradients: w=>0.1127229791, b=>-1.9447756046
Iteration 2395: Achieved Loss=> 26.4080169247
Gradients: w=>0.1126630521, b=>-1.9437417022
Iteration 2396: Achieved Loss=> 26.4042271076
Gradients: w=>0.1126031569, b=>-1.9427083495
Iteration 2397: Achieved Loss=> 26.4004413189
Gradients: w=>0.1125432937, b=>-1.9416755461
Iteration 2398: Achieved Loss=> 26.3966595545
Gradients: w=>0.1124834622, b=>-1.9406432918
Iteration 2399: Achieved Loss=> 26.3928818101
Gradients: w=>0.1124236625, b=>-1.9396115862
Iteration 2400: Achieved Loss=> 26.3891080813
Gradients: w=>0.1123638947, b=>-1.9385804292
Iteration 2401: Achieved Loss=> 26.3853383639
Gradients: w=>0.1123041586, b=>-1.9375498203
Iteration 2402: Achieved Loss=> 26.3815726536
Gradients: w=>0.1122444542, b=>-1.9365197594
Iteration 2403: Achieved Loss=> 26.3778109462
Gradients: w=>0.1121847816, b=>-1.9354902460
Iteration 2404: Achieved Loss=> 26.3740532374
Gradients: w=>0.1121251408, b=>-1.9344612800
Iteration 2405: Achieved Loss=> 26.3702995229
Gradients: w=>0.1120655316, b=>-1.9334328610
Iteration 2406: Achieved Loss=> 26.3665497986
Gradients: w=>0.1120059541, b=>-1.9324049887
Iteration 2407: Achieved Loss=> 26.3628040602
Gradients: w=>0.1119464083, b=>-1.9313776629
Iteration 2408: Achieved Loss=> 26.3590623034
Gradients: w=>0.1118868942, b=>-1.9303508833
Iteration 2409: Achieved Loss=> 26.3553245240
Gradients: w=>0.1118274117, b=>-1.9293246495
Iteration 2410: Achieved Loss=> 26.3515907178
Gradients: w=>0.1117679608, b=>-1.9282989613
Iteration 2411: Achieved Loss=> 26.3478608806
Gradients: w=>0.1117085415, b=>-1.9272738184
Iteration 2412: Achieved Loss=> 26.3441350080
Gradients: w=>0.1116491538, b=>-1.9262492205
Iteration 2413: Achieved Loss=> 26.3404130961
Gradients: w=>0.1115897977, b=>-1.9252251672
Iteration 2414: Achieved Loss=> 26.3366951404
Gradients: w=>0.1115304731, b=>-1.9242016585
Iteration 2415: Achieved Loss=> 26.3329811368
Gradients: w=>0.1114711801, b=>-1.9231786938
Iteration 2416: Achieved Loss=> 26.3292710812
Gradients: w=>0.1114119186, b=>-1.9221562730
Iteration 2417: Achieved Loss=> 26.3255649692
Gradients: w=>0.1113526886, b=>-1.9211343957
Iteration 2418: Achieved Loss=> 26.3218627968
Gradients: w=>0.1112934901, b=>-1.9201130617
Iteration 2419: Achieved Loss=> 26.3181645597
Gradients: w=>0.1112343231, b=>-1.9190922706
Iteration 2420: Achieved Loss=> 26.3144702537
Gradients: w=>0.1111751875, b=>-1.9180720223
Iteration 2421: Achieved Loss=> 26.3107798747
Gradients: w=>0.1111160833, b=>-1.9170523163
Iteration 2422: Achieved Loss=> 26.3070934185
Gradients: w=>0.1110570106, b=>-1.9160331525
Iteration 2423: Achieved Loss=> 26.3034108810
Gradients: w=>0.1109979693, b=>-1.9150145304
Iteration 2424: Achieved Loss=> 26.2997322579
Gradients: w=>0.1109389594, b=>-1.9139964499
Iteration 2425: Achieved Loss=> 26.2960575451
Gradients: w=>0.1108799808, b=>-1.9129789107
Iteration 2426: Achieved Loss=> 26.2923867384
Gradients: w=>0.1108210336, b=>-1.9119619123
Iteration 2427: Achieved Loss=> 26.2887198337
Gradients: w=>0.1107621178, b=>-1.9109454547
Iteration 2428: Achieved Loss=> 26.2850568269
Gradients: w=>0.1107032332, b=>-1.9099295374
Iteration 2429: Achieved Loss=> 26.2813977137
Gradients: w=>0.1106443800, b=>-1.9089141603
Iteration 2430: Achieved Loss=> 26.2777424902
Gradients: w=>0.1105855580, b=>-1.9078993229
Iteration 2431: Achieved Loss=> 26.2740911520
Gradients: w=>0.1105267673, b=>-1.9068850251
Iteration 2432: Achieved Loss=> 26.2704436952
Gradients: w=>0.1104680079, b=>-1.9058712665
Iteration 2433: Achieved Loss=> 26.2668001155
Gradients: w=>0.1104092797, b=>-1.9048580468
Iteration 2434: Achieved Loss=> 26.2631604088
Gradients: w=>0.1103505828, b=>-1.9038453658
Iteration 2435: Achieved Loss=> 26.2595245711
Gradients: w=>0.1102919170, b=>-1.9028332232
Iteration 2436: Achieved Loss=> 26.2558925982
Gradients: w=>0.1102332825, b=>-1.9018216186
Iteration 2437: Achieved Loss=> 26.2522644860
Gradients: w=>0.1101746791, b=>-1.9008105519
Iteration 2438: Achieved Loss=> 26.2486402305
Gradients: w=>0.1101161068, b=>-1.8998000226
Iteration 2439: Achieved Loss=> 26.2450198274
Gradients: w=>0.1100575657, b=>-1.8987900306
Iteration 2440: Achieved Loss=> 26.2414032727
Gradients: w=>0.1099990557, b=>-1.8977805756
Iteration 2441: Achieved Loss=> 26.2377905624
Gradients: w=>0.1099405769, b=>-1.8967716572
Iteration 2442: Achieved Loss=> 26.2341816923
Gradients: w=>0.1098821291, b=>-1.8957632751
Iteration 2443: Achieved Loss=> 26.2305766584
Gradients: w=>0.1098237124, b=>-1.8947554292
Iteration 2444: Achieved Loss=> 26.2269754565
Gradients: w=>0.1097653267, b=>-1.8937481190
Iteration 2445: Achieved Loss=> 26.2233780826
Gradients: w=>0.1097069721, b=>-1.8927413444
Iteration 2446: Achieved Loss=> 26.2197845327
Gradients: w=>0.1096486485, b=>-1.8917351050
Iteration 2447: Achieved Loss=> 26.2161948026
Gradients: w=>0.1095903559, b=>-1.8907294006
Iteration 2448: Achieved Loss=> 26.2126088883
Gradients: w=>0.1095320943, b=>-1.8897242308
Iteration 2449: Achieved Loss=> 26.2090267858
Gradients: w=>0.1094738637, b=>-1.8887195954
Iteration 2450: Achieved Loss=> 26.2054484910
Gradients: w=>0.1094156640, b=>-1.8877154941
Iteration 2451: Achieved Loss=> 26.2018739998
Gradients: w=>0.1093574953, b=>-1.8867119266
Iteration 2452: Achieved Loss=> 26.1983033083
Gradients: w=>0.1092993575, b=>-1.8857088926
Iteration 2453: Achieved Loss=> 26.1947364123
Gradients: w=>0.1092412506, b=>-1.8847063919
Iteration 2454: Achieved Loss=> 26.1911733078
Gradients: w=>0.1091831746, b=>-1.8837044241
Iteration 2455: Achieved Loss=> 26.1876139909
Gradients: w=>0.1091251295, b=>-1.8827029890
Iteration 2456: Achieved Loss=> 26.1840584574
Gradients: w=>0.1090671152, b=>-1.8817020863
Iteration 2457: Achieved Loss=> 26.1805067034
Gradients: w=>0.1090091318, b=>-1.8807017157
Iteration 2458: Achieved Loss=> 26.1769587248
Gradients: w=>0.1089511792, b=>-1.8797018769
Iteration 2459: Achieved Loss=> 26.1734145177
Gradients: w=>0.1088932574, b=>-1.8787025697
Iteration 2460: Achieved Loss=> 26.1698740779
Gradients: w=>0.1088353664, b=>-1.8777037938
Iteration 2461: Achieved Loss=> 26.1663374016
Gradients: w=>0.1087775062, b=>-1.8767055488
Iteration 2462: Achieved Loss=> 26.1628044847
Gradients: w=>0.1087196767, b=>-1.8757078345
Iteration 2463: Achieved Loss=> 26.1592753232
Gradients: w=>0.1086618780, b=>-1.8747106507
Iteration 2464: Achieved Loss=> 26.1557499131
Gradients: w=>0.1086041100, b=>-1.8737139969
Iteration 2465: Achieved Loss=> 26.1522282505
Gradients: w=>0.1085463727, b=>-1.8727178731
Iteration 2466: Achieved Loss=> 26.1487103313
Gradients: w=>0.1084886661, b=>-1.8717222788
Iteration 2467: Achieved Loss=> 26.1451961516
Gradients: w=>0.1084309902, b=>-1.8707272138
Iteration 2468: Achieved Loss=> 26.1416857074
Gradients: w=>0.1083733450, b=>-1.8697326777
Iteration 2469: Achieved Loss=> 26.1381789947
Gradients: w=>0.1083157304, b=>-1.8687386705
Iteration 2470: Achieved Loss=> 26.1346760096
Gradients: w=>0.1082581464, b=>-1.8677451916
Iteration 2471: Achieved Loss=> 26.1311767481
Gradients: w=>0.1082005931, b=>-1.8667522410
Iteration 2472: Achieved Loss=> 26.1276812062
Gradients: w=>0.1081430703, b=>-1.8657598182
Iteration 2473: Achieved Loss=> 26.1241893800
Gradients: w=>0.1080855781, b=>-1.8647679230
Iteration 2474: Achieved Loss=> 26.1207012655
Gradients: w=>0.1080281165, b=>-1.8637765551
Iteration 2475: Achieved Loss=> 26.1172168589
Gradients: w=>0.1079706855, b=>-1.8627857143
Iteration 2476: Achieved Loss=> 26.1137361561
Gradients: w=>0.1079132849, b=>-1.8617954002
Iteration 2477: Achieved Loss=> 26.1102591532
Gradients: w=>0.1078559149, b=>-1.8608056126
Iteration 2478: Achieved Loss=> 26.1067858462
Gradients: w=>0.1077985754, b=>-1.8598163513
Iteration 2479: Achieved Loss=> 26.1033162314
Gradients: w=>0.1077412664, b=>-1.8588276158
Iteration 2480: Achieved Loss=> 26.0998503046
Gradients: w=>0.1076839878, b=>-1.8578394060
Iteration 2481: Achieved Loss=> 26.0963880621
Gradients: w=>0.1076267397, b=>-1.8568517215
Iteration 2482: Achieved Loss=> 26.0929294998
Gradients: w=>0.1075695220, b=>-1.8558645622
Iteration 2483: Achieved Loss=> 26.0894746140
Gradients: w=>0.1075123348, b=>-1.8548779276
Iteration 2484: Achieved Loss=> 26.0860234006
Gradients: w=>0.1074551779, b=>-1.8538918175
Iteration 2485: Achieved Loss=> 26.0825758557
Gradients: w=>0.1073980514, b=>-1.8529062318
Iteration 2486: Achieved Loss=> 26.0791319756
Gradients: w=>0.1073409553, b=>-1.8519211699
Iteration 2487: Achieved Loss=> 26.0756917562
Gradients: w=>0.1072838896, b=>-1.8509366318
Iteration 2488: Achieved Loss=> 26.0722551937
Gradients: w=>0.1072268542, b=>-1.8499526171
Iteration 2489: Achieved Loss=> 26.0688222841
Gradients: w=>0.1071698491, b=>-1.8489691255
Iteration 2490: Achieved Loss=> 26.0653930237
Gradients: w=>0.1071128743, b=>-1.8479861567
Iteration 2491: Achieved Loss=> 26.0619674085
Gradients: w=>0.1070559298, b=>-1.8470037106
Iteration 2492: Achieved Loss=> 26.0585454347
Gradients: w=>0.1069990156, b=>-1.8460217867
Iteration 2493: Achieved Loss=> 26.0551270984
Gradients: w=>0.1069421316, b=>-1.8450403848
Iteration 2494: Achieved Loss=> 26.0517123957
Gradients: w=>0.1068852779, b=>-1.8440595047
Iteration 2495: Achieved Loss=> 26.0483013227
Gradients: w=>0.1068284544, b=>-1.8430791461
Iteration 2496: Achieved Loss=> 26.0448938756
Gradients: w=>0.1067716611, b=>-1.8420993087
Iteration 2497: Achieved Loss=> 26.0414900506
Gradients: w=>0.1067148980, b=>-1.8411199921
Iteration 2498: Achieved Loss=> 26.0380898438
Gradients: w=>0.1066581651, b=>-1.8401411962
Iteration 2499: Achieved Loss=> 26.0346932513
Gradients: w=>0.1066014624, b=>-1.8391629207
Iteration 2500: Achieved Loss=> 26.0313002693
Gradients: w=>0.1065447898, b=>-1.8381851652
Iteration 2501: Achieved Loss=> 26.0279108940
Gradients: w=>0.1064881473, b=>-1.8372079296
Iteration 2502: Achieved Loss=> 26.0245251216
Gradients: w=>0.1064315349, b=>-1.8362312134
Iteration 2503: Achieved Loss=> 26.0211429481
Gradients: w=>0.1063749526, b=>-1.8352550166
Iteration 2504: Achieved Loss=> 26.0177643698
Gradients: w=>0.1063184005, b=>-1.8342793387
Iteration 2505: Achieved Loss=> 26.0143893829
Gradients: w=>0.1062618783, b=>-1.8333041795
Iteration 2506: Achieved Loss=> 26.0110179835
Gradients: w=>0.1062053863, b=>-1.8323295387
Iteration 2507: Achieved Loss=> 26.0076501678
Gradients: w=>0.1061489242, b=>-1.8313554161
Iteration 2508: Achieved Loss=> 26.0042859320
Gradients: w=>0.1060924922, b=>-1.8303818113
Iteration 2509: Achieved Loss=> 26.0009252724
Gradients: w=>0.1060360902, b=>-1.8294087242
Iteration 2510: Achieved Loss=> 25.9975681851
Gradients: w=>0.1059797181, b=>-1.8284361543
Iteration 2511: Achieved Loss=> 25.9942146663
Gradients: w=>0.1059233760, b=>-1.8274641015
Iteration 2512: Achieved Loss=> 25.9908647122
Gradients: w=>0.1058670639, b=>-1.8264925655
Iteration 2513: Achieved Loss=> 25.9875183190
Gradients: w=>0.1058107817, b=>-1.8255215460
Iteration 2514: Achieved Loss=> 25.9841754830
Gradients: w=>0.1057545295, b=>-1.8245510427
Iteration 2515: Achieved Loss=> 25.9808362003
Gradients: w=>0.1056983071, b=>-1.8235810554
Iteration 2516: Achieved Loss=> 25.9775004672
Gradients: w=>0.1056421147, b=>-1.8226115837
Iteration 2517: Achieved Loss=> 25.9741682800
Gradients: w=>0.1055859521, b=>-1.8216426275
Iteration 2518: Achieved Loss=> 25.9708396348
Gradients: w=>0.1055298193, b=>-1.8206741863
Iteration 2519: Achieved Loss=> 25.9675145278
Gradients: w=>0.1054737165, b=>-1.8197062601
Iteration 2520: Achieved Loss=> 25.9641929554
Gradients: w=>0.1054176434, b=>-1.8187388483
Iteration 2521: Achieved Loss=> 25.9608749138
Gradients: w=>0.1053616001, b=>-1.8177719510
Iteration 2522: Achieved Loss=> 25.9575603991
Gradients: w=>0.1053055867, b=>-1.8168055676
Iteration 2523: Achieved Loss=> 25.9542494077
Gradients: w=>0.1052496030, b=>-1.8158396980
Iteration 2524: Achieved Loss=> 25.9509419358
Gradients: w=>0.1051936491, b=>-1.8148743419
Iteration 2525: Achieved Loss=> 25.9476379797
Gradients: w=>0.1051377249, b=>-1.8139094990
Iteration 2526: Achieved Loss=> 25.9443375357
Gradients: w=>0.1050818305, b=>-1.8129451690
Iteration 2527: Achieved Loss=> 25.9410405999
Gradients: w=>0.1050259657, b=>-1.8119813517
Iteration 2528: Achieved Loss=> 25.9377471687
Gradients: w=>0.1049701307, b=>-1.8110180468
Iteration 2529: Achieved Loss=> 25.9344572384
Gradients: w=>0.1049143254, b=>-1.8100552540
Iteration 2530: Achieved Loss=> 25.9311708051
Gradients: w=>0.1048585497, b=>-1.8090929731
Iteration 2531: Achieved Loss=> 25.9278878653
Gradients: w=>0.1048028037, b=>-1.8081312037
Iteration 2532: Achieved Loss=> 25.9246084152
Gradients: w=>0.1047470873, b=>-1.8071699457
Iteration 2533: Achieved Loss=> 25.9213324511
Gradients: w=>0.1046914005, b=>-1.8062091987
Iteration 2534: Achieved Loss=> 25.9180599692
Gradients: w=>0.1046357433, b=>-1.8052489624
Iteration 2535: Achieved Loss=> 25.9147909659
Gradients: w=>0.1045801158, b=>-1.8042892366
Iteration 2536: Achieved Loss=> 25.9115254376
Gradients: w=>0.1045245178, b=>-1.8033300211
Iteration 2537: Achieved Loss=> 25.9082633803
Gradients: w=>0.1044689493, b=>-1.8023713155
Iteration 2538: Achieved Loss=> 25.9050047906
Gradients: w=>0.1044134104, b=>-1.8014131196
Iteration 2539: Achieved Loss=> 25.9017496647
Gradients: w=>0.1043579011, b=>-1.8004554331
Iteration 2540: Achieved Loss=> 25.8984979990
Gradients: w=>0.1043024212, b=>-1.7994982557
Iteration 2541: Achieved Loss=> 25.8952497897
Gradients: w=>0.1042469708, b=>-1.7985415872
Iteration 2542: Achieved Loss=> 25.8920050331
Gradients: w=>0.1041915499, b=>-1.7975854273
Iteration 2543: Achieved Loss=> 25.8887637257
Gradients: w=>0.1041361585, b=>-1.7966297757
Iteration 2544: Achieved Loss=> 25.8855258637
Gradients: w=>0.1040807965, b=>-1.7956746322
Iteration 2545: Achieved Loss=> 25.8822914435
Gradients: w=>0.1040254640, b=>-1.7947199964
Iteration 2546: Achieved Loss=> 25.8790604614
Gradients: w=>0.1039701609, b=>-1.7937658682
Iteration 2547: Achieved Loss=> 25.8758329138
Gradients: w=>0.1039148871, b=>-1.7928122472
Iteration 2548: Achieved Loss=> 25.8726087970
Gradients: w=>0.1038596428, b=>-1.7918591332
Iteration 2549: Achieved Loss=> 25.8693881073
Gradients: w=>0.1038044278, b=>-1.7909065258
Iteration 2550: Achieved Loss=> 25.8661708412
Gradients: w=>0.1037492422, b=>-1.7899544250
Iteration 2551: Achieved Loss=> 25.8629569950
Gradients: w=>0.1036940859, b=>-1.7890028302
Iteration 2552: Achieved Loss=> 25.8597465650
Gradients: w=>0.1036389589, b=>-1.7880517414
Iteration 2553: Achieved Loss=> 25.8565395476
Gradients: w=>0.1035838613, b=>-1.7871011582
Iteration 2554: Achieved Loss=> 25.8533359393
Gradients: w=>0.1035287929, b=>-1.7861510804
Iteration 2555: Achieved Loss=> 25.8501357363
Gradients: w=>0.1034737538, b=>-1.7852015077
Iteration 2556: Achieved Loss=> 25.8469389350
Gradients: w=>0.1034187440, b=>-1.7842524398
Iteration 2557: Achieved Loss=> 25.8437455319
Gradients: w=>0.1033637634, b=>-1.7833038764
Iteration 2558: Achieved Loss=> 25.8405555233
Gradients: w=>0.1033088121, b=>-1.7823558173
Iteration 2559: Achieved Loss=> 25.8373689056
Gradients: w=>0.1032538900, b=>-1.7814082622
Iteration 2560: Achieved Loss=> 25.8341856752
Gradients: w=>0.1031989970, b=>-1.7804612109
Iteration 2561: Achieved Loss=> 25.8310058285
Gradients: w=>0.1031441333, b=>-1.7795146631
Iteration 2562: Achieved Loss=> 25.8278293620
Gradients: w=>0.1030892987, b=>-1.7785686185
Iteration 2563: Achieved Loss=> 25.8246562719
Gradients: w=>0.1030344932, b=>-1.7776230768
Iteration 2564: Achieved Loss=> 25.8214865548
Gradients: w=>0.1029797169, b=>-1.7766780378
Iteration 2565: Achieved Loss=> 25.8183202070
Gradients: w=>0.1029249698, b=>-1.7757335012
Iteration 2566: Achieved Loss=> 25.8151572250
Gradients: w=>0.1028702517, b=>-1.7747894668
Iteration 2567: Achieved Loss=> 25.8119976051
Gradients: w=>0.1028155627, b=>-1.7738459343
Iteration 2568: Achieved Loss=> 25.8088413439
Gradients: w=>0.1027609028, b=>-1.7729029033
Iteration 2569: Achieved Loss=> 25.8056884377
Gradients: w=>0.1027062720, b=>-1.7719603737
Iteration 2570: Achieved Loss=> 25.8025388830
Gradients: w=>0.1026516701, b=>-1.7710183452
Iteration 2571: Achieved Loss=> 25.7993926762
Gradients: w=>0.1025970974, b=>-1.7700768175
Iteration 2572: Achieved Loss=> 25.7962498137
Gradients: w=>0.1025425536, b=>-1.7691357903
Iteration 2573: Achieved Loss=> 25.7931102920
Gradients: w=>0.1024880388, b=>-1.7681952634
Iteration 2574: Achieved Loss=> 25.7899741076
Gradients: w=>0.1024335530, b=>-1.7672552365
Iteration 2575: Achieved Loss=> 25.7868412569
Gradients: w=>0.1023790962, b=>-1.7663157094
Iteration 2576: Achieved Loss=> 25.7837117363
Gradients: w=>0.1023246684, b=>-1.7653766818
Iteration 2577: Achieved Loss=> 25.7805855424
Gradients: w=>0.1022702694, b=>-1.7644381533
Iteration 2578: Achieved Loss=> 25.7774626715
Gradients: w=>0.1022158994, b=>-1.7635001238
Iteration 2579: Achieved Loss=> 25.7743431202
Gradients: w=>0.1021615583, b=>-1.7625625930
Iteration 2580: Achieved Loss=> 25.7712268848
Gradients: w=>0.1021072461, b=>-1.7616255607
Iteration 2581: Achieved Loss=> 25.7681139620
Gradients: w=>0.1020529627, b=>-1.7606890265
Iteration 2582: Achieved Loss=> 25.7650043482
Gradients: w=>0.1019987082, b=>-1.7597529901
Iteration 2583: Achieved Loss=> 25.7618980398
Gradients: w=>0.1019444826, b=>-1.7588174514
Iteration 2584: Achieved Loss=> 25.7587950333
Gradients: w=>0.1018902858, b=>-1.7578824101
Iteration 2585: Achieved Loss=> 25.7556953253
Gradients: w=>0.1018361178, b=>-1.7569478658
Iteration 2586: Achieved Loss=> 25.7525989122
Gradients: w=>0.1017819786, b=>-1.7560138184
Iteration 2587: Achieved Loss=> 25.7495057905
Gradients: w=>0.1017278681, b=>-1.7550802676
Iteration 2588: Achieved Loss=> 25.7464159567
Gradients: w=>0.1016737865, b=>-1.7541472130
Iteration 2589: Achieved Loss=> 25.7433294074
Gradients: w=>0.1016197336, b=>-1.7532146545
Iteration 2590: Achieved Loss=> 25.7402461390
Gradients: w=>0.1015657094, b=>-1.7522825918
Iteration 2591: Achieved Loss=> 25.7371661481
Gradients: w=>0.1015117140, b=>-1.7513510246
Iteration 2592: Achieved Loss=> 25.7340894311
Gradients: w=>0.1014577472, b=>-1.7504199526
Iteration 2593: Achieved Loss=> 25.7310159846
Gradients: w=>0.1014038092, b=>-1.7494893756
Iteration 2594: Achieved Loss=> 25.7279458051
Gradients: w=>0.1013498998, b=>-1.7485592934
Iteration 2595: Achieved Loss=> 25.7248788891
Gradients: w=>0.1012960191, b=>-1.7476297056
Iteration 2596: Achieved Loss=> 25.7218152332
Gradients: w=>0.1012421670, b=>-1.7467006120
Iteration 2597: Achieved Loss=> 25.7187548340
Gradients: w=>0.1011883436, b=>-1.7457720123
Iteration 2598: Achieved Loss=> 25.7156976878
Gradients: w=>0.1011345487, b=>-1.7448439064
Iteration 2599: Achieved Loss=> 25.7126437913
Gradients: w=>0.1010807825, b=>-1.7439162938
Iteration 2600: Achieved Loss=> 25.7095931411
Gradients: w=>0.1010270449, b=>-1.7429891744
Iteration 2601: Achieved Loss=> 25.7065457336
Gradients: w=>0.1009733358, b=>-1.7420625478
Iteration 2602: Achieved Loss=> 25.7035015655
Gradients: w=>0.1009196553, b=>-1.7411364139
Iteration 2603: Achieved Loss=> 25.7004606333
Gradients: w=>0.1008660033, b=>-1.7402107723
Iteration 2604: Achieved Loss=> 25.6974229335
Gradients: w=>0.1008123798, b=>-1.7392856229
Iteration 2605: Achieved Loss=> 25.6943884627
Gradients: w=>0.1007587848, b=>-1.7383609652
Iteration 2606: Achieved Loss=> 25.6913572175
Gradients: w=>0.1007052184, b=>-1.7374367992
Iteration 2607: Achieved Loss=> 25.6883291944
Gradients: w=>0.1006516804, b=>-1.7365131245
Iteration 2608: Achieved Loss=> 25.6853043901
Gradients: w=>0.1005981709, b=>-1.7355899408
Iteration 2609: Achieved Loss=> 25.6822828010
Gradients: w=>0.1005446898, b=>-1.7346672479
Iteration 2610: Achieved Loss=> 25.6792644239
Gradients: w=>0.1004912372, b=>-1.7337450455
Iteration 2611: Achieved Loss=> 25.6762492552
Gradients: w=>0.1004378129, b=>-1.7328233335
Iteration 2612: Achieved Loss=> 25.6732372916
Gradients: w=>0.1003844171, b=>-1.7319021114
Iteration 2613: Achieved Loss=> 25.6702285296
Gradients: w=>0.1003310497, b=>-1.7309813791
Iteration 2614: Achieved Loss=> 25.6672229659
Gradients: w=>0.1002777106, b=>-1.7300611362
Iteration 2615: Achieved Loss=> 25.6642205970
Gradients: w=>0.1002243999, b=>-1.7291413826
Iteration 2616: Achieved Loss=> 25.6612214196
Gradients: w=>0.1001711175, b=>-1.7282221180
Iteration 2617: Achieved Loss=> 25.6582254303
Gradients: w=>0.1001178635, b=>-1.7273033421
Iteration 2618: Achieved Loss=> 25.6552326256
Gradients: w=>0.1000646378, b=>-1.7263850546
Iteration 2619: Achieved Loss=> 25.6522430022
Gradients: w=>0.1000114403, b=>-1.7254672553
Iteration 2620: Achieved Loss=> 25.6492565567
Gradients: w=>0.0999582712, b=>-1.7245499439
Iteration 2621: Achieved Loss=> 25.6462732857
Gradients: w=>0.0999051303, b=>-1.7236331203
Iteration 2622: Achieved Loss=> 25.6432931859
Gradients: w=>0.0998520177, b=>-1.7227167840
Iteration 2623: Achieved Loss=> 25.6403162539
Gradients: w=>0.0997989333, b=>-1.7218009349
Iteration 2624: Achieved Loss=> 25.6373424863
Gradients: w=>0.0997458771, b=>-1.7208855726
Iteration 2625: Achieved Loss=> 25.6343718798
Gradients: w=>0.0996928491, b=>-1.7199706970
Iteration 2626: Achieved Loss=> 25.6314044309
Gradients: w=>0.0996398493, b=>-1.7190563078
Iteration 2627: Achieved Loss=> 25.6284401364
Gradients: w=>0.0995868777, b=>-1.7181424047
Iteration 2628: Achieved Loss=> 25.6254789928
Gradients: w=>0.0995339343, b=>-1.7172289875
Iteration 2629: Achieved Loss=> 25.6225209969
Gradients: w=>0.0994810190, b=>-1.7163160558
Iteration 2630: Achieved Loss=> 25.6195661453
Gradients: w=>0.0994281318, b=>-1.7154036096
Iteration 2631: Achieved Loss=> 25.6166144346
Gradients: w=>0.0993752728, b=>-1.7144916483
Iteration 2632: Achieved Loss=> 25.6136658616
Gradients: w=>0.0993224418, b=>-1.7135801720
Iteration 2633: Achieved Loss=> 25.6107204228
Gradients: w=>0.0992696390, b=>-1.7126691801
Iteration 2634: Achieved Loss=> 25.6077781149
Gradients: w=>0.0992168642, b=>-1.7117586726
Iteration 2635: Achieved Loss=> 25.6048389346
Gradients: w=>0.0991641175, b=>-1.7108486492
Iteration 2636: Achieved Loss=> 25.6019028787
Gradients: w=>0.0991113988, b=>-1.7099391095
Iteration 2637: Achieved Loss=> 25.5989699437
Gradients: w=>0.0990587081, b=>-1.7090300534
Iteration 2638: Achieved Loss=> 25.5960401263
Gradients: w=>0.0990060455, b=>-1.7081214806
Iteration 2639: Achieved Loss=> 25.5931134233
Gradients: w=>0.0989534108, b=>-1.7072133908
Iteration 2640: Achieved Loss=> 25.5901898313
Gradients: w=>0.0989008041, b=>-1.7063057837
Iteration 2641: Achieved Loss=> 25.5872693470
Gradients: w=>0.0988482254, b=>-1.7053986592
Iteration 2642: Achieved Loss=> 25.5843519672
Gradients: w=>0.0987956747, b=>-1.7044920169
Iteration 2643: Achieved Loss=> 25.5814376884
Gradients: w=>0.0987431519, b=>-1.7035858566
Iteration 2644: Achieved Loss=> 25.5785265075
Gradients: w=>0.0986906570, b=>-1.7026801781
Iteration 2645: Achieved Loss=> 25.5756184211
Gradients: w=>0.0986381900, b=>-1.7017749811
Iteration 2646: Achieved Loss=> 25.5727134259
Gradients: w=>0.0985857509, b=>-1.7008702652
Iteration 2647: Achieved Loss=> 25.5698115186
Gradients: w=>0.0985333397, b=>-1.6999660304
Iteration 2648: Achieved Loss=> 25.5669126961
Gradients: w=>0.0984809563, b=>-1.6990622763
Iteration 2649: Achieved Loss=> 25.5640169549
Gradients: w=>0.0984286008, b=>-1.6981590026
Iteration 2650: Achieved Loss=> 25.5611242918
Gradients: w=>0.0983762732, b=>-1.6972562092
Iteration 2651: Achieved Loss=> 25.5582347036
Gradients: w=>0.0983239733, b=>-1.6963538957
Iteration 2652: Achieved Loss=> 25.5553481869
Gradients: w=>0.0982717013, b=>-1.6954520619
Iteration 2653: Achieved Loss=> 25.5524647386
Gradients: w=>0.0982194570, b=>-1.6945507075
Iteration 2654: Achieved Loss=> 25.5495843553
Gradients: w=>0.0981672406, b=>-1.6936498323
Iteration 2655: Achieved Loss=> 25.5467070337
Gradients: w=>0.0981150518, b=>-1.6927494361
Iteration 2656: Achieved Loss=> 25.5438327708
Gradients: w=>0.0980628909, b=>-1.6918495186
Iteration 2657: Achieved Loss=> 25.5409615630
Gradients: w=>0.0980107576, b=>-1.6909500794
Iteration 2658: Achieved Loss=> 25.5380934074
Gradients: w=>0.0979586521, b=>-1.6900511185
Iteration 2659: Achieved Loss=> 25.5352283005
Gradients: w=>0.0979065743, b=>-1.6891526354
Iteration 2660: Achieved Loss=> 25.5323662391
Gradients: w=>0.0978545241, b=>-1.6882546300
Iteration 2661: Achieved Loss=> 25.5295072201
Gradients: w=>0.0978025017, b=>-1.6873571020
Iteration 2662: Achieved Loss=> 25.5266512402
Gradients: w=>0.0977505069, b=>-1.6864600512
Iteration 2663: Achieved Loss=> 25.5237982960
Gradients: w=>0.0976985397, b=>-1.6855634773
Iteration 2664: Achieved Loss=> 25.5209483846
Gradients: w=>0.0976466001, b=>-1.6846673800
Iteration 2665: Achieved Loss=> 25.5181015025
Gradients: w=>0.0975946882, b=>-1.6837717591
Iteration 2666: Achieved Loss=> 25.5152576465
Gradients: w=>0.0975428039, b=>-1.6828766144
Iteration 2667: Achieved Loss=> 25.5124168136
Gradients: w=>0.0974909471, b=>-1.6819819455
Iteration 2668: Achieved Loss=> 25.5095790004
Gradients: w=>0.0974391180, b=>-1.6810877523
Iteration 2669: Achieved Loss=> 25.5067442037
Gradients: w=>0.0973873163, b=>-1.6801940344
Iteration 2670: Achieved Loss=> 25.5039124203
Gradients: w=>0.0973355422, b=>-1.6793007917
Iteration 2671: Achieved Loss=> 25.5010836471
Gradients: w=>0.0972837957, b=>-1.6784080238
Iteration 2672: Achieved Loss=> 25.4982578808
Gradients: w=>0.0972320766, b=>-1.6775157306
Iteration 2673: Achieved Loss=> 25.4954351182
Gradients: w=>0.0971803851, b=>-1.6766239118
Iteration 2674: Achieved Loss=> 25.4926153562
Gradients: w=>0.0971287210, b=>-1.6757325670
Iteration 2675: Achieved Loss=> 25.4897985915
Gradients: w=>0.0970770844, b=>-1.6748416962
Iteration 2676: Achieved Loss=> 25.4869848210
Gradients: w=>0.0970254752, b=>-1.6739512989
Iteration 2677: Achieved Loss=> 25.4841740415
Gradients: w=>0.0969738935, b=>-1.6730613750
Iteration 2678: Achieved Loss=> 25.4813662497
Gradients: w=>0.0969223392, b=>-1.6721719242
Iteration 2679: Achieved Loss=> 25.4785614426
Gradients: w=>0.0968708123, b=>-1.6712829463
Iteration 2680: Achieved Loss=> 25.4757596169
Gradients: w=>0.0968193128, b=>-1.6703944410
Iteration 2681: Achieved Loss=> 25.4729607695
Gradients: w=>0.0967678407, b=>-1.6695064080
Iteration 2682: Achieved Loss=> 25.4701648972
Gradients: w=>0.0967163959, b=>-1.6686188472
Iteration 2683: Achieved Loss=> 25.4673719969
Gradients: w=>0.0966649785, b=>-1.6677317582
Iteration 2684: Achieved Loss=> 25.4645820654
Gradients: w=>0.0966135885, b=>-1.6668451408
Iteration 2685: Achieved Loss=> 25.4617950995
Gradients: w=>0.0965622257, b=>-1.6659589948
Iteration 2686: Achieved Loss=> 25.4590110961
Gradients: w=>0.0965108903, b=>-1.6650733198
Iteration 2687: Achieved Loss=> 25.4562300520
Gradients: w=>0.0964595821, b=>-1.6641881157
Iteration 2688: Achieved Loss=> 25.4534519641
Gradients: w=>0.0964083013, b=>-1.6633033822
Iteration 2689: Achieved Loss=> 25.4506768293
Gradients: w=>0.0963570476, b=>-1.6624191191
Iteration 2690: Achieved Loss=> 25.4479046444
Gradients: w=>0.0963058213, b=>-1.6615353261
Iteration 2691: Achieved Loss=> 25.4451354062
Gradients: w=>0.0962546221, b=>-1.6606520029
Iteration 2692: Achieved Loss=> 25.4423691117
Gradients: w=>0.0962034502, b=>-1.6597691493
Iteration 2693: Achieved Loss=> 25.4396057577
Gradients: w=>0.0961523055, b=>-1.6588867651
Iteration 2694: Achieved Loss=> 25.4368453411
Gradients: w=>0.0961011880, b=>-1.6580048500
Iteration 2695: Achieved Loss=> 25.4340878588
Gradients: w=>0.0960500977, b=>-1.6571234037
Iteration 2696: Achieved Loss=> 25.4313333076
Gradients: w=>0.0959990345, b=>-1.6562424260
Iteration 2697: Achieved Loss=> 25.4285816844
Gradients: w=>0.0959479985, b=>-1.6553619167
Iteration 2698: Achieved Loss=> 25.4258329861
Gradients: w=>0.0958969896, b=>-1.6544818755
Iteration 2699: Achieved Loss=> 25.4230872097
Gradients: w=>0.0958460078, b=>-1.6536023022
Iteration 2700: Achieved Loss=> 25.4203443520
Gradients: w=>0.0957950531, b=>-1.6527231964
Iteration 2701: Achieved Loss=> 25.4176044098
Gradients: w=>0.0957441255, b=>-1.6518445581
Iteration 2702: Achieved Loss=> 25.4148673802
Gradients: w=>0.0956932250, b=>-1.6509663868
Iteration 2703: Achieved Loss=> 25.4121332599
Gradients: w=>0.0956423515, b=>-1.6500886824
Iteration 2704: Achieved Loss=> 25.4094020460
Gradients: w=>0.0955915051, b=>-1.6492114446
Iteration 2705: Achieved Loss=> 25.4066737353
Gradients: w=>0.0955406857, b=>-1.6483346732
Iteration 2706: Achieved Loss=> 25.4039483247
Gradients: w=>0.0954898934, b=>-1.6474583678
Iteration 2707: Achieved Loss=> 25.4012258112
Gradients: w=>0.0954391280, b=>-1.6465825284
Iteration 2708: Achieved Loss=> 25.3985061917
Gradients: w=>0.0953883897, b=>-1.6457071546
Iteration 2709: Achieved Loss=> 25.3957894630
Gradients: w=>0.0953376783, b=>-1.6448322462
Iteration 2710: Achieved Loss=> 25.3930756222
Gradients: w=>0.0952869938, b=>-1.6439578029
Iteration 2711: Achieved Loss=> 25.3903646661
Gradients: w=>0.0952363363, b=>-1.6430838244
Iteration 2712: Achieved Loss=> 25.3876565918
Gradients: w=>0.0951857058, b=>-1.6422103106
Iteration 2713: Achieved Loss=> 25.3849513960
Gradients: w=>0.0951351021, b=>-1.6413372612
Iteration 2714: Achieved Loss=> 25.3822490758
Gradients: w=>0.0950845254, b=>-1.6404646759
Iteration 2715: Achieved Loss=> 25.3795496282
Gradients: w=>0.0950339755, b=>-1.6395925546
Iteration 2716: Achieved Loss=> 25.3768530499
Gradients: w=>0.0949834526, b=>-1.6387208968
Iteration 2717: Achieved Loss=> 25.3741593381
Gradients: w=>0.0949329564, b=>-1.6378497025
Iteration 2718: Achieved Loss=> 25.3714684897
Gradients: w=>0.0948824872, b=>-1.6369789713
Iteration 2719: Achieved Loss=> 25.3687805015
Gradients: w=>0.0948320447, b=>-1.6361087031
Iteration 2720: Achieved Loss=> 25.3660953707
Gradients: w=>0.0947816291, b=>-1.6352388975
Iteration 2721: Achieved Loss=> 25.3634130940
Gradients: w=>0.0947312403, b=>-1.6343695543
Iteration 2722: Achieved Loss=> 25.3607336686
Gradients: w=>0.0946808782, b=>-1.6335006733
Iteration 2723: Achieved Loss=> 25.3580570914
Gradients: w=>0.0946305430, b=>-1.6326322542
Iteration 2724: Achieved Loss=> 25.3553833592
Gradients: w=>0.0945802345, b=>-1.6317642967
Iteration 2725: Achieved Loss=> 25.3527124693
Gradients: w=>0.0945299527, b=>-1.6308968008
Iteration 2726: Achieved Loss=> 25.3500444184
Gradients: w=>0.0944796977, b=>-1.6300297660
Iteration 2727: Achieved Loss=> 25.3473792036
Gradients: w=>0.0944294694, b=>-1.6291631921
Iteration 2728: Achieved Loss=> 25.3447168218
Gradients: w=>0.0943792678, b=>-1.6282970789
Iteration 2729: Achieved Loss=> 25.3420572701
Gradients: w=>0.0943290929, b=>-1.6274314262
Iteration 2730: Achieved Loss=> 25.3394005455
Gradients: w=>0.0942789446, b=>-1.6265662337
Iteration 2731: Achieved Loss=> 25.3367466449
Gradients: w=>0.0942288230, b=>-1.6257015012
Iteration 2732: Achieved Loss=> 25.3340955653
Gradients: w=>0.0941787281, b=>-1.6248372284
Iteration 2733: Achieved Loss=> 25.3314473038
Gradients: w=>0.0941286598, b=>-1.6239734150
Iteration 2734: Achieved Loss=> 25.3288018574
Gradients: w=>0.0940786181, b=>-1.6231100609
Iteration 2735: Achieved Loss=> 25.3261592229
Gradients: w=>0.0940286030, b=>-1.6222471658
Iteration 2736: Achieved Loss=> 25.3235193976
Gradients: w=>0.0939786146, b=>-1.6213847294
Iteration 2737: Achieved Loss=> 25.3208823783
Gradients: w=>0.0939286526, b=>-1.6205227515
Iteration 2738: Achieved Loss=> 25.3182481621
Gradients: w=>0.0938787173, b=>-1.6196612319
Iteration 2739: Achieved Loss=> 25.3156167461
Gradients: w=>0.0938288085, b=>-1.6188001702
Iteration 2740: Achieved Loss=> 25.3129881271
Gradients: w=>0.0937789262, b=>-1.6179395664
Iteration 2741: Achieved Loss=> 25.3103623024
Gradients: w=>0.0937290704, b=>-1.6170794200
Iteration 2742: Achieved Loss=> 25.3077392688
Gradients: w=>0.0936792412, b=>-1.6162197310
Iteration 2743: Achieved Loss=> 25.3051190235
Gradients: w=>0.0936294384, b=>-1.6153604990
Iteration 2744: Achieved Loss=> 25.3025015634
Gradients: w=>0.0935796621, b=>-1.6145017237
Iteration 2745: Achieved Loss=> 25.2998868857
Gradients: w=>0.0935299123, b=>-1.6136434051
Iteration 2746: Achieved Loss=> 25.2972749872
Gradients: w=>0.0934801890, b=>-1.6127855427
Iteration 2747: Achieved Loss=> 25.2946658652
Gradients: w=>0.0934304920, b=>-1.6119281364
Iteration 2748: Achieved Loss=> 25.2920595166
Gradients: w=>0.0933808215, b=>-1.6110711859
Iteration 2749: Achieved Loss=> 25.2894559386
Gradients: w=>0.0933311774, b=>-1.6102146910
Iteration 2750: Achieved Loss=> 25.2868551280
Gradients: w=>0.0932815597, b=>-1.6093586515
Iteration 2751: Achieved Loss=> 25.2842570821
Gradients: w=>0.0932319683, b=>-1.6085030670
Iteration 2752: Achieved Loss=> 25.2816617978
Gradients: w=>0.0931824034, b=>-1.6076479374
Iteration 2753: Achieved Loss=> 25.2790692723
Gradients: w=>0.0931328647, b=>-1.6067932624
Iteration 2754: Achieved Loss=> 25.2764795025
Gradients: w=>0.0930833524, b=>-1.6059390418
Iteration 2755: Achieved Loss=> 25.2738924857
Gradients: w=>0.0930338665, b=>-1.6050852753
Iteration 2756: Achieved Loss=> 25.2713082188
Gradients: w=>0.0929844068, b=>-1.6042319627
Iteration 2757: Achieved Loss=> 25.2687266989
Gradients: w=>0.0929349734, b=>-1.6033791038
Iteration 2758: Achieved Loss=> 25.2661479231
Gradients: w=>0.0928855664, b=>-1.6025266982
Iteration 2759: Achieved Loss=> 25.2635718884
Gradients: w=>0.0928361855, b=>-1.6016747458
Iteration 2760: Achieved Loss=> 25.2609985921
Gradients: w=>0.0927868310, b=>-1.6008232464
Iteration 2761: Achieved Loss=> 25.2584280311
Gradients: w=>0.0927375027, b=>-1.5999721996
Iteration 2762: Achieved Loss=> 25.2558602026
Gradients: w=>0.0926882005, b=>-1.5991216053
Iteration 2763: Achieved Loss=> 25.2532951036
Gradients: w=>0.0926389247, b=>-1.5982714632
Iteration 2764: Achieved Loss=> 25.2507327312
Gradients: w=>0.0925896750, b=>-1.5974217730
Iteration 2765: Achieved Loss=> 25.2481730827
Gradients: w=>0.0925404514, b=>-1.5965725345
Iteration 2766: Achieved Loss=> 25.2456161549
Gradients: w=>0.0924912541, b=>-1.5957237476
Iteration 2767: Achieved Loss=> 25.2430619451
Gradients: w=>0.0924420829, b=>-1.5948754118
Iteration 2768: Achieved Loss=> 25.2405104504
Gradients: w=>0.0923929379, b=>-1.5940275271
Iteration 2769: Achieved Loss=> 25.2379616679
Gradients: w=>0.0923438189, b=>-1.5931800932
Iteration 2770: Achieved Loss=> 25.2354155947
Gradients: w=>0.0922947261, b=>-1.5923331097
Iteration 2771: Achieved Loss=> 25.2328722279
Gradients: w=>0.0922456594, b=>-1.5914865766
Iteration 2772: Achieved Loss=> 25.2303315646
Gradients: w=>0.0921966188, b=>-1.5906404935
Iteration 2773: Achieved Loss=> 25.2277936020
Gradients: w=>0.0921476042, b=>-1.5897948601
Iteration 2774: Achieved Loss=> 25.2252583372
Gradients: w=>0.0920986158, b=>-1.5889496764
Iteration 2775: Achieved Loss=> 25.2227257674
Gradients: w=>0.0920496533, b=>-1.5881049420
Iteration 2776: Achieved Loss=> 25.2201958896
Gradients: w=>0.0920007169, b=>-1.5872606566
Iteration 2777: Achieved Loss=> 25.2176687010
Gradients: w=>0.0919518065, b=>-1.5864168202
Iteration 2778: Achieved Loss=> 25.2151441988
Gradients: w=>0.0919029221, b=>-1.5855734323
Iteration 2779: Achieved Loss=> 25.2126223800
Gradients: w=>0.0918540637, b=>-1.5847304928
Iteration 2780: Achieved Loss=> 25.2101032419
Gradients: w=>0.0918052312, b=>-1.5838880014
Iteration 2781: Achieved Loss=> 25.2075867816
Gradients: w=>0.0917564247, b=>-1.5830459579
Iteration 2782: Achieved Loss=> 25.2050729963
Gradients: w=>0.0917076442, b=>-1.5822043621
Iteration 2783: Achieved Loss=> 25.2025618830
Gradients: w=>0.0916588896, b=>-1.5813632137
Iteration 2784: Achieved Loss=> 25.2000534390
Gradients: w=>0.0916101609, b=>-1.5805225125
Iteration 2785: Achieved Loss=> 25.1975476614
Gradients: w=>0.0915614582, b=>-1.5796822582
Iteration 2786: Achieved Loss=> 25.1950445474
Gradients: w=>0.0915127813, b=>-1.5788424506
Iteration 2787: Achieved Loss=> 25.1925440942
Gradients: w=>0.0914641303, b=>-1.5780030895
Iteration 2788: Achieved Loss=> 25.1900462989
Gradients: w=>0.0914155051, b=>-1.5771641746
Iteration 2789: Achieved Loss=> 25.1875511587
Gradients: w=>0.0913669058, b=>-1.5763257057
Iteration 2790: Achieved Loss=> 25.1850586707
Gradients: w=>0.0913183324, b=>-1.5754876826
Iteration 2791: Achieved Loss=> 25.1825688323
Gradients: w=>0.0912697848, b=>-1.5746501050
Iteration 2792: Achieved Loss=> 25.1800816405
Gradients: w=>0.0912212629, b=>-1.5738129727
Iteration 2793: Achieved Loss=> 25.1775970925
Gradients: w=>0.0911727669, b=>-1.5729762854
Iteration 2794: Achieved Loss=> 25.1751151855
Gradients: w=>0.0911242967, b=>-1.5721400429
Iteration 2795: Achieved Loss=> 25.1726359168
Gradients: w=>0.0910758522, b=>-1.5713042450
Iteration 2796: Achieved Loss=> 25.1701592834
Gradients: w=>0.0910274335, b=>-1.5704688914
Iteration 2797: Achieved Loss=> 25.1676852827
Gradients: w=>0.0909790405, b=>-1.5696339820
Iteration 2798: Achieved Loss=> 25.1652139118
Gradients: w=>0.0909306733, b=>-1.5687995164
Iteration 2799: Achieved Loss=> 25.1627451679
Gradients: w=>0.0908823317, b=>-1.5679654944
Iteration 2800: Achieved Loss=> 25.1602790482
Gradients: w=>0.0908340159, b=>-1.5671319158
Iteration 2801: Achieved Loss=> 25.1578155499
Gradients: w=>0.0907857257, b=>-1.5662987804
Iteration 2802: Achieved Loss=> 25.1553546703
Gradients: w=>0.0907374613, b=>-1.5654660879
Iteration 2803: Achieved Loss=> 25.1528964066
Gradients: w=>0.0906892224, b=>-1.5646338380
Iteration 2804: Achieved Loss=> 25.1504407559
Gradients: w=>0.0906410093, b=>-1.5638020307
Iteration 2805: Achieved Loss=> 25.1479877156
Gradients: w=>0.0905928217, b=>-1.5629706655
Iteration 2806: Achieved Loss=> 25.1455372828
Gradients: w=>0.0905446598, b=>-1.5621397423
Iteration 2807: Achieved Loss=> 25.1430894547
Gradients: w=>0.0904965235, b=>-1.5613092609
Iteration 2808: Achieved Loss=> 25.1406442286
Gradients: w=>0.0904484127, b=>-1.5604792210
Iteration 2809: Achieved Loss=> 25.1382016018
Gradients: w=>0.0904003276, b=>-1.5596496223
Iteration 2810: Achieved Loss=> 25.1357615714
Gradients: w=>0.0903522680, b=>-1.5588204647
Iteration 2811: Achieved Loss=> 25.1333241347
Gradients: w=>0.0903042340, b=>-1.5579917479
Iteration 2812: Achieved Loss=> 25.1308892889
Gradients: w=>0.0902562255, b=>-1.5571634717
Iteration 2813: Achieved Loss=> 25.1284570314
Gradients: w=>0.0902082425, b=>-1.5563356358
Iteration 2814: Achieved Loss=> 25.1260273592
Gradients: w=>0.0901602850, b=>-1.5555082400
Iteration 2815: Achieved Loss=> 25.1236002698
Gradients: w=>0.0901123530, b=>-1.5546812840
Iteration 2816: Achieved Loss=> 25.1211757603
Gradients: w=>0.0900644465, b=>-1.5538547677
Iteration 2817: Achieved Loss=> 25.1187538280
Gradients: w=>0.0900165655, b=>-1.5530286909
Iteration 2818: Achieved Loss=> 25.1163344702
Gradients: w=>0.0899687099, b=>-1.5522030531
Iteration 2819: Achieved Loss=> 25.1139176841
Gradients: w=>0.0899208798, b=>-1.5513778543
Iteration 2820: Achieved Loss=> 25.1115034670
Gradients: w=>0.0898730751, b=>-1.5505530943
Iteration 2821: Achieved Loss=> 25.1090918162
Gradients: w=>0.0898252958, b=>-1.5497287726
Iteration 2822: Achieved Loss=> 25.1066827288
Gradients: w=>0.0897775420, b=>-1.5489048893
Iteration 2823: Achieved Loss=> 25.1042762023
Gradients: w=>0.0897298135, b=>-1.5480814439
Iteration 2824: Achieved Loss=> 25.1018722339
Gradients: w=>0.0896821103, b=>-1.5472584363
Iteration 2825: Achieved Loss=> 25.0994708209
Gradients: w=>0.0896344326, b=>-1.5464358662
Iteration 2826: Achieved Loss=> 25.0970719605
Gradients: w=>0.0895867802, b=>-1.5456137334
Iteration 2827: Achieved Loss=> 25.0946756500
Gradients: w=>0.0895391531, b=>-1.5447920377
Iteration 2828: Achieved Loss=> 25.0922818868
Gradients: w=>0.0894915513, b=>-1.5439707788
Iteration 2829: Achieved Loss=> 25.0898906681
Gradients: w=>0.0894439749, b=>-1.5431499566
Iteration 2830: Achieved Loss=> 25.0875019912
Gradients: w=>0.0893964237, b=>-1.5423295707
Iteration 2831: Achieved Loss=> 25.0851158534
Gradients: w=>0.0893488978, b=>-1.5415096210
Iteration 2832: Achieved Loss=> 25.0827322520
Gradients: w=>0.0893013972, b=>-1.5406901071
Iteration 2833: Achieved Loss=> 25.0803511844
Gradients: w=>0.0892539218, b=>-1.5398710290
Iteration 2834: Achieved Loss=> 25.0779726478
Gradients: w=>0.0892064717, b=>-1.5390523863
Iteration 2835: Achieved Loss=> 25.0755966395
Gradients: w=>0.0891590468, b=>-1.5382341788
Iteration 2836: Achieved Loss=> 25.0732231568
Gradients: w=>0.0891116471, b=>-1.5374164063
Iteration 2837: Achieved Loss=> 25.0708521971
Gradients: w=>0.0890642726, b=>-1.5365990685
Iteration 2838: Achieved Loss=> 25.0684837577
Gradients: w=>0.0890169233, b=>-1.5357821653
Iteration 2839: Achieved Loss=> 25.0661178359
Gradients: w=>0.0889695992, b=>-1.5349656964
Iteration 2840: Achieved Loss=> 25.0637544290
Gradients: w=>0.0889223002, b=>-1.5341496615
Iteration 2841: Achieved Loss=> 25.0613935344
Gradients: w=>0.0888750264, b=>-1.5333340604
Iteration 2842: Achieved Loss=> 25.0590351493
Gradients: w=>0.0888277777, b=>-1.5325188930
Iteration 2843: Achieved Loss=> 25.0566792712
Gradients: w=>0.0887805541, b=>-1.5317041589
Iteration 2844: Achieved Loss=> 25.0543258973
Gradients: w=>0.0887333557, b=>-1.5308898579
Iteration 2845: Achieved Loss=> 25.0519750250
Gradients: w=>0.0886861823, b=>-1.5300759899
Iteration 2846: Achieved Loss=> 25.0496266516
Gradients: w=>0.0886390340, b=>-1.5292625545
Iteration 2847: Achieved Loss=> 25.0472807745
Gradients: w=>0.0885919108, b=>-1.5284495516
Iteration 2848: Achieved Loss=> 25.0449373911
Gradients: w=>0.0885448126, b=>-1.5276369809
Iteration 2849: Achieved Loss=> 25.0425964985
Gradients: w=>0.0884977395, b=>-1.5268248422
Iteration 2850: Achieved Loss=> 25.0402580943
Gradients: w=>0.0884506913, b=>-1.5260131353
Iteration 2851: Achieved Loss=> 25.0379221758
Gradients: w=>0.0884036682, b=>-1.5252018598
Iteration 2852: Achieved Loss=> 25.0355887403
Gradients: w=>0.0883566701, b=>-1.5243910157
Iteration 2853: Achieved Loss=> 25.0332577852
Gradients: w=>0.0883096970, b=>-1.5235806027
Iteration 2854: Achieved Loss=> 25.0309293079
Gradients: w=>0.0882627489, b=>-1.5227706205
Iteration 2855: Achieved Loss=> 25.0286033056
Gradients: w=>0.0882158257, b=>-1.5219610689
Iteration 2856: Achieved Loss=> 25.0262797759
Gradients: w=>0.0881689274, b=>-1.5211519476
Iteration 2857: Achieved Loss=> 25.0239587160
Gradients: w=>0.0881220541, b=>-1.5203432566
Iteration 2858: Achieved Loss=> 25.0216401234
Gradients: w=>0.0880752057, b=>-1.5195349954
Iteration 2859: Achieved Loss=> 25.0193239954
Gradients: w=>0.0880283823, b=>-1.5187271640
Iteration 2860: Achieved Loss=> 25.0170103294
Gradients: w=>0.0879815837, b=>-1.5179197620
Iteration 2861: Achieved Loss=> 25.0146991227
Gradients: w=>0.0879348100, b=>-1.5171127893
Iteration 2862: Achieved Loss=> 25.0123903728
Gradients: w=>0.0878880611, b=>-1.5163062455
Iteration 2863: Achieved Loss=> 25.0100840771
Gradients: w=>0.0878413371, b=>-1.5155001306
Iteration 2864: Achieved Loss=> 25.0077802329
Gradients: w=>0.0877946380, b=>-1.5146944442
Iteration 2865: Achieved Loss=> 25.0054788377
Gradients: w=>0.0877479637, b=>-1.5138891861
Iteration 2866: Achieved Loss=> 25.0031798888
Gradients: w=>0.0877013141, b=>-1.5130843562
Iteration 2867: Achieved Loss=> 25.0008833836
Gradients: w=>0.0876546894, b=>-1.5122799541
Iteration 2868: Achieved Loss=> 24.9985893195
Gradients: w=>0.0876080895, b=>-1.5114759797
Iteration 2869: Achieved Loss=> 24.9962976940
Gradients: w=>0.0875615144, b=>-1.5106724326
Iteration 2870: Achieved Loss=> 24.9940085045
Gradients: w=>0.0875149640, b=>-1.5098693128
Iteration 2871: Achieved Loss=> 24.9917217483
Gradients: w=>0.0874684383, b=>-1.5090666199
Iteration 2872: Achieved Loss=> 24.9894374229
Gradients: w=>0.0874219374, b=>-1.5082643538
Iteration 2873: Achieved Loss=> 24.9871555256
Gradients: w=>0.0873754612, b=>-1.5074625142
Iteration 2874: Achieved Loss=> 24.9848760540
Gradients: w=>0.0873290098, b=>-1.5066611008
Iteration 2875: Achieved Loss=> 24.9825990054
Gradients: w=>0.0872825830, b=>-1.5058601135
Iteration 2876: Achieved Loss=> 24.9803243773
Gradients: w=>0.0872361809, b=>-1.5050595521
Iteration 2877: Achieved Loss=> 24.9780521670
Gradients: w=>0.0871898034, b=>-1.5042594162
Iteration 2878: Achieved Loss=> 24.9757823721
Gradients: w=>0.0871434507, b=>-1.5034597057
Iteration 2879: Achieved Loss=> 24.9735149899
Gradients: w=>0.0870971225, b=>-1.5026604204
Iteration 2880: Achieved Loss=> 24.9712500179
Gradients: w=>0.0870508190, b=>-1.5018615600
Iteration 2881: Achieved Loss=> 24.9689874534
Gradients: w=>0.0870045401, b=>-1.5010631243
Iteration 2882: Achieved Loss=> 24.9667272941
Gradients: w=>0.0869582859, b=>-1.5002651131
Iteration 2883: Achieved Loss=> 24.9644695372
Gradients: w=>0.0869120562, b=>-1.4994675261
Iteration 2884: Achieved Loss=> 24.9622141803
Gradients: w=>0.0868658510, b=>-1.4986703631
Iteration 2885: Achieved Loss=> 24.9599612208
Gradients: w=>0.0868196705, b=>-1.4978736240
Iteration 2886: Achieved Loss=> 24.9577106562
Gradients: w=>0.0867735145, b=>-1.4970773084
Iteration 2887: Achieved Loss=> 24.9554624838
Gradients: w=>0.0867273830, b=>-1.4962814161
Iteration 2888: Achieved Loss=> 24.9532167012
Gradients: w=>0.0866812761, b=>-1.4954859470
Iteration 2889: Achieved Loss=> 24.9509733059
Gradients: w=>0.0866351937, b=>-1.4946909008
Iteration 2890: Achieved Loss=> 24.9487322952
Gradients: w=>0.0865891357, b=>-1.4938962772
Iteration 2891: Achieved Loss=> 24.9464936666
Gradients: w=>0.0865431023, b=>-1.4931020761
Iteration 2892: Achieved Loss=> 24.9442574177
Gradients: w=>0.0864970933, b=>-1.4923082972
Iteration 2893: Achieved Loss=> 24.9420235459
Gradients: w=>0.0864511088, b=>-1.4915149403
Iteration 2894: Achieved Loss=> 24.9397920486
Gradients: w=>0.0864051488, b=>-1.4907220052
Iteration 2895: Achieved Loss=> 24.9375629233
Gradients: w=>0.0863592131, b=>-1.4899294916
Iteration 2896: Achieved Loss=> 24.9353361676
Gradients: w=>0.0863133019, b=>-1.4891373993
Iteration 2897: Achieved Loss=> 24.9331117788
Gradients: w=>0.0862674151, b=>-1.4883457282
Iteration 2898: Achieved Loss=> 24.9308897546
Gradients: w=>0.0862215527, b=>-1.4875544779
Iteration 2899: Achieved Loss=> 24.9286700923
Gradients: w=>0.0861757147, b=>-1.4867636483
Iteration 2900: Achieved Loss=> 24.9264527894
Gradients: w=>0.0861299010, b=>-1.4859732391
Iteration 2901: Achieved Loss=> 24.9242378435
Gradients: w=>0.0860841117, b=>-1.4851832501
Iteration 2902: Achieved Loss=> 24.9220252520
Gradients: w=>0.0860383468, b=>-1.4843936811
Iteration 2903: Achieved Loss=> 24.9198150125
Gradients: w=>0.0859926061, b=>-1.4836045319
Iteration 2904: Achieved Loss=> 24.9176071224
Gradients: w=>0.0859468898, b=>-1.4828158022
Iteration 2905: Achieved Loss=> 24.9154015793
Gradients: w=>0.0859011978, b=>-1.4820274918
Iteration 2906: Achieved Loss=> 24.9131983806
Gradients: w=>0.0858555301, b=>-1.4812396005
Iteration 2907: Achieved Loss=> 24.9109975238
Gradients: w=>0.0858098867, b=>-1.4804521280
Iteration 2908: Achieved Loss=> 24.9087990065
Gradients: w=>0.0857642675, b=>-1.4796650742
Iteration 2909: Achieved Loss=> 24.9066028262
Gradients: w=>0.0857186726, b=>-1.4788784389
Iteration 2910: Achieved Loss=> 24.9044089804
Gradients: w=>0.0856731019, b=>-1.4780922217
Iteration 2911: Achieved Loss=> 24.9022174666
Gradients: w=>0.0856275555, b=>-1.4773064225
Iteration 2912: Achieved Loss=> 24.9000282823
Gradients: w=>0.0855820332, b=>-1.4765210410
Iteration 2913: Achieved Loss=> 24.8978414251
Gradients: w=>0.0855365352, b=>-1.4757360771
Iteration 2914: Achieved Loss=> 24.8956568925
Gradients: w=>0.0854910613, b=>-1.4749515305
Iteration 2915: Achieved Loss=> 24.8934746820
Gradients: w=>0.0854456117, b=>-1.4741674010
Iteration 2916: Achieved Loss=> 24.8912947911
Gradients: w=>0.0854001862, b=>-1.4733836884
Iteration 2917: Achieved Loss=> 24.8891172174
Gradients: w=>0.0853547848, b=>-1.4726003924
Iteration 2918: Achieved Loss=> 24.8869419584
Gradients: w=>0.0853094076, b=>-1.4718175128
Iteration 2919: Achieved Loss=> 24.8847690117
Gradients: w=>0.0852640545, b=>-1.4710350495
Iteration 2920: Achieved Loss=> 24.8825983748
Gradients: w=>0.0852187255, b=>-1.4702530021
Iteration 2921: Achieved Loss=> 24.8804300452
Gradients: w=>0.0851734206, b=>-1.4694713705
Iteration 2922: Achieved Loss=> 24.8782640205
Gradients: w=>0.0851281398, b=>-1.4686901544
Iteration 2923: Achieved Loss=> 24.8761002982
Gradients: w=>0.0850828831, b=>-1.4679093536
Iteration 2924: Achieved Loss=> 24.8739388759
Gradients: w=>0.0850376504, b=>-1.4671289679
Iteration 2925: Achieved Loss=> 24.8717797512
Gradients: w=>0.0849924418, b=>-1.4663489971
Iteration 2926: Achieved Loss=> 24.8696229216
Gradients: w=>0.0849472572, b=>-1.4655694410
Iteration 2927: Achieved Loss=> 24.8674683846
Gradients: w=>0.0849020966, b=>-1.4647902993
Iteration 2928: Achieved Loss=> 24.8653161379
Gradients: w=>0.0848569600, b=>-1.4640115718
Iteration 2929: Achieved Loss=> 24.8631661789
Gradients: w=>0.0848118475, b=>-1.4632332583
Iteration 2930: Achieved Loss=> 24.8610185053
Gradients: w=>0.0847667589, b=>-1.4624553586
Iteration 2931: Achieved Loss=> 24.8588731147
Gradients: w=>0.0847216943, b=>-1.4616778725
Iteration 2932: Achieved Loss=> 24.8567300045
Gradients: w=>0.0846766536, b=>-1.4609007996
Iteration 2933: Achieved Loss=> 24.8545891725
Gradients: w=>0.0846316369, b=>-1.4601241399
Iteration 2934: Achieved Loss=> 24.8524506161
Gradients: w=>0.0845866442, b=>-1.4593478931
Iteration 2935: Achieved Loss=> 24.8503143329
Gradients: w=>0.0845416753, b=>-1.4585720590
Iteration 2936: Achieved Loss=> 24.8481803206
Gradients: w=>0.0844967303, b=>-1.4577966373
Iteration 2937: Achieved Loss=> 24.8460485766
Gradients: w=>0.0844518093, b=>-1.4570216279
Iteration 2938: Achieved Loss=> 24.8439190987
Gradients: w=>0.0844069121, b=>-1.4562470304
Iteration 2939: Achieved Loss=> 24.8417918844
Gradients: w=>0.0843620388, b=>-1.4554728448
Iteration 2940: Achieved Loss=> 24.8396669312
Gradients: w=>0.0843171894, b=>-1.4546990708
Iteration 2941: Achieved Loss=> 24.8375442368
Gradients: w=>0.0842723638, b=>-1.4539257081
Iteration 2942: Achieved Loss=> 24.8354237988
Gradients: w=>0.0842275620, b=>-1.4531527566
Iteration 2943: Achieved Loss=> 24.8333056148
Gradients: w=>0.0841827840, b=>-1.4523802160
Iteration 2944: Achieved Loss=> 24.8311896824
Gradients: w=>0.0841380299, b=>-1.4516080861
Iteration 2945: Achieved Loss=> 24.8290759991
Gradients: w=>0.0840932995, b=>-1.4508363666
Iteration 2946: Achieved Loss=> 24.8269645627
Gradients: w=>0.0840485929, b=>-1.4500650575
Iteration 2947: Achieved Loss=> 24.8248553706
Gradients: w=>0.0840039101, b=>-1.4492941584
Iteration 2948: Achieved Loss=> 24.8227484206
Gradients: w=>0.0839592511, b=>-1.4485236691
Iteration 2949: Achieved Loss=> 24.8206437103
Gradients: w=>0.0839146158, b=>-1.4477535895
Iteration 2950: Achieved Loss=> 24.8185412372
Gradients: w=>0.0838700042, b=>-1.4469839193
Iteration 2951: Achieved Loss=> 24.8164409990
Gradients: w=>0.0838254163, b=>-1.4462146582
Iteration 2952: Achieved Loss=> 24.8143429932
Gradients: w=>0.0837808521, b=>-1.4454458061
Iteration 2953: Achieved Loss=> 24.8122472177
Gradients: w=>0.0837363117, b=>-1.4446773627
Iteration 2954: Achieved Loss=> 24.8101536699
Gradients: w=>0.0836917949, b=>-1.4439093279
Iteration 2955: Achieved Loss=> 24.8080623475
Gradients: w=>0.0836473017, b=>-1.4431417014
Iteration 2956: Achieved Loss=> 24.8059732481
Gradients: w=>0.0836028323, b=>-1.4423744830
Iteration 2957: Achieved Loss=> 24.8038863694
Gradients: w=>0.0835583864, b=>-1.4416076724
Iteration 2958: Achieved Loss=> 24.8018017090
Gradients: w=>0.0835139642, b=>-1.4408412695
Iteration 2959: Achieved Loss=> 24.7997192645
Gradients: w=>0.0834695656, b=>-1.4400752741
Iteration 2960: Achieved Loss=> 24.7976390337
Gradients: w=>0.0834251907, b=>-1.4393096859
Iteration 2961: Achieved Loss=> 24.7955610140
Gradients: w=>0.0833808393, b=>-1.4385445046
Iteration 2962: Achieved Loss=> 24.7934852033
Gradients: w=>0.0833365115, b=>-1.4377797302
Iteration 2963: Achieved Loss=> 24.7914115991
Gradients: w=>0.0832922072, b=>-1.4370153624
Iteration 2964: Achieved Loss=> 24.7893401991
Gradients: w=>0.0832479265, b=>-1.4362514009
Iteration 2965: Achieved Loss=> 24.7872710010
Gradients: w=>0.0832036694, b=>-1.4354878456
Iteration 2966: Achieved Loss=> 24.7852040024
Gradients: w=>0.0831594357, b=>-1.4347246962
Iteration 2967: Achieved Loss=> 24.7831392009
Gradients: w=>0.0831152256, b=>-1.4339619525
Iteration 2968: Achieved Loss=> 24.7810765943
Gradients: w=>0.0830710390, b=>-1.4331996143
Iteration 2969: Achieved Loss=> 24.7790161802
Gradients: w=>0.0830268759, b=>-1.4324376814
Iteration 2970: Achieved Loss=> 24.7769579563
Gradients: w=>0.0829827363, b=>-1.4316761536
Iteration 2971: Achieved Loss=> 24.7749019202
Gradients: w=>0.0829386201, b=>-1.4309150306
Iteration 2972: Achieved Loss=> 24.7728480697
Gradients: w=>0.0828945274, b=>-1.4301543122
Iteration 2973: Achieved Loss=> 24.7707964023
Gradients: w=>0.0828504581, b=>-1.4293939983
Iteration 2974: Achieved Loss=> 24.7687469159
Gradients: w=>0.0828064123, b=>-1.4286340886
Iteration 2975: Achieved Loss=> 24.7666996080
Gradients: w=>0.0827623898, b=>-1.4278745828
Iteration 2976: Achieved Loss=> 24.7646544763
Gradients: w=>0.0827183908, b=>-1.4271154809
Iteration 2977: Achieved Loss=> 24.7626115186
Gradients: w=>0.0826744152, b=>-1.4263567825
Iteration 2978: Achieved Loss=> 24.7605707325
Gradients: w=>0.0826304629, b=>-1.4255984874
Iteration 2979: Achieved Loss=> 24.7585321157
Gradients: w=>0.0825865340, b=>-1.4248405955
Iteration 2980: Achieved Loss=> 24.7564956659
Gradients: w=>0.0825426285, b=>-1.4240831065
Iteration 2981: Achieved Loss=> 24.7544613808
Gradients: w=>0.0824987463, b=>-1.4233260202
Iteration 2982: Achieved Loss=> 24.7524292581
Gradients: w=>0.0824548874, b=>-1.4225693364
Iteration 2983: Achieved Loss=> 24.7503992955
Gradients: w=>0.0824110519, b=>-1.4218130549
Iteration 2984: Achieved Loss=> 24.7483714907
Gradients: w=>0.0823672396, b=>-1.4210571754
Iteration 2985: Achieved Loss=> 24.7463458414
Gradients: w=>0.0823234507, b=>-1.4203016978
Iteration 2986: Achieved Loss=> 24.7443223454
Gradients: w=>0.0822796850, b=>-1.4195466219
Iteration 2987: Achieved Loss=> 24.7423010003
Gradients: w=>0.0822359426, b=>-1.4187919473
Iteration 2988: Achieved Loss=> 24.7402818038
Gradients: w=>0.0821922235, b=>-1.4180376740
Iteration 2989: Achieved Loss=> 24.7382647537
Gradients: w=>0.0821485275, b=>-1.4172838016
Iteration 2990: Achieved Loss=> 24.7362498477
Gradients: w=>0.0821048549, b=>-1.4165303300
Iteration 2991: Achieved Loss=> 24.7342370835
Gradients: w=>0.0820612054, b=>-1.4157772590
Iteration 2992: Achieved Loss=> 24.7322264588
Gradients: w=>0.0820175791, b=>-1.4150245884
Iteration 2993: Achieved Loss=> 24.7302179714
Gradients: w=>0.0819739761, b=>-1.4142723179
Iteration 2994: Achieved Loss=> 24.7282116189
Gradients: w=>0.0819303962, b=>-1.4135204473
Iteration 2995: Achieved Loss=> 24.7262073991
Gradients: w=>0.0818868395, b=>-1.4127689764
Iteration 2996: Achieved Loss=> 24.7242053098
Gradients: w=>0.0818433059, b=>-1.4120179051
Iteration 2997: Achieved Loss=> 24.7222053487
Gradients: w=>0.0817997955, b=>-1.4112672330
Iteration 2998: Achieved Loss=> 24.7202075135
Gradients: w=>0.0817563082, b=>-1.4105169600
Iteration 2999: Achieved Loss=> 24.7182118019
Gradients: w=>0.0817128441, b=>-1.4097670859
Iteration 3000: Achieved Loss=> 24.7162182118
Gradients: w=>0.0816694030, b=>-1.4090176105
Iteration 3001: Achieved Loss=> 24.7142267408
Gradients: w=>0.0816259851, b=>-1.4082685335
Iteration 3002: Achieved Loss=> 24.7122373866
Gradients: w=>0.0815825902, b=>-1.4075198547
Iteration 3003: Achieved Loss=> 24.7102501472
Gradients: w=>0.0815392184, b=>-1.4067715739
Iteration 3004: Achieved Loss=> 24.7082650201
Gradients: w=>0.0814958696, b=>-1.4060236910
Iteration 3005: Achieved Loss=> 24.7062820031
Gradients: w=>0.0814525439, b=>-1.4052762056
Iteration 3006: Achieved Loss=> 24.7043010941
Gradients: w=>0.0814092412, b=>-1.4045291177
Iteration 3007: Achieved Loss=> 24.7023222907
Gradients: w=>0.0813659616, b=>-1.4037824269
Iteration 3008: Achieved Loss=> 24.7003455908
Gradients: w=>0.0813227049, b=>-1.4030361330
Iteration 3009: Achieved Loss=> 24.6983709920
Gradients: w=>0.0812794713, b=>-1.4022902360
Iteration 3010: Achieved Loss=> 24.6963984922
Gradients: w=>0.0812362606, b=>-1.4015447354
Iteration 3011: Achieved Loss=> 24.6944280892
Gradients: w=>0.0811930729, b=>-1.4007996312
Iteration 3012: Achieved Loss=> 24.6924597806
Gradients: w=>0.0811499082, b=>-1.4000549232
Iteration 3013: Achieved Loss=> 24.6904935643
Gradients: w=>0.0811067664, b=>-1.3993106110
Iteration 3014: Achieved Loss=> 24.6885294380
Gradients: w=>0.0810636476, b=>-1.3985666945
Iteration 3015: Achieved Loss=> 24.6865673996
Gradients: w=>0.0810205517, b=>-1.3978231735
Iteration 3016: Achieved Loss=> 24.6846074468
Gradients: w=>0.0809774787, b=>-1.3970800478
Iteration 3017: Achieved Loss=> 24.6826495773
Gradients: w=>0.0809344285, b=>-1.3963373172
Iteration 3018: Achieved Loss=> 24.6806937890
Gradients: w=>0.0808914013, b=>-1.3955949814
Iteration 3019: Achieved Loss=> 24.6787400797
Gradients: w=>0.0808483970, b=>-1.3948530403
Iteration 3020: Achieved Loss=> 24.6767884472
Gradients: w=>0.0808054155, b=>-1.3941114936
Iteration 3021: Achieved Loss=> 24.6748388892
Gradients: w=>0.0807624568, b=>-1.3933703411
Iteration 3022: Achieved Loss=> 24.6728914035
Gradients: w=>0.0807195210, b=>-1.3926295827
Iteration 3023: Achieved Loss=> 24.6709459880
Gradients: w=>0.0806766081, b=>-1.3918892181
Iteration 3024: Achieved Loss=> 24.6690026404
Gradients: w=>0.0806337179, b=>-1.3911492470
Iteration 3025: Achieved Loss=> 24.6670613585
Gradients: w=>0.0805908505, b=>-1.3904096694
Iteration 3026: Achieved Loss=> 24.6651221402
Gradients: w=>0.0805480060, b=>-1.3896704849
Iteration 3027: Achieved Loss=> 24.6631849832
Gradients: w=>0.0805051842, b=>-1.3889316934
Iteration 3028: Achieved Loss=> 24.6612498854
Gradients: w=>0.0804623851, b=>-1.3881932947
Iteration 3029: Achieved Loss=> 24.6593168445
Gradients: w=>0.0804196089, b=>-1.3874552885
Iteration 3030: Achieved Loss=> 24.6573858585
Gradients: w=>0.0803768553, b=>-1.3867176747
Iteration 3031: Achieved Loss=> 24.6554569250
Gradients: w=>0.0803341245, b=>-1.3859804530
Iteration 3032: Achieved Loss=> 24.6535300419
Gradients: w=>0.0802914164, b=>-1.3852436233
Iteration 3033: Achieved Loss=> 24.6516052071
Gradients: w=>0.0802487311, b=>-1.3845071853
Iteration 3034: Achieved Loss=> 24.6496824183
Gradients: w=>0.0802060684, b=>-1.3837711388
Iteration 3035: Achieved Loss=> 24.6477616735
Gradients: w=>0.0801634284, b=>-1.3830354835
Iteration 3036: Achieved Loss=> 24.6458429703
Gradients: w=>0.0801208110, b=>-1.3823002194
Iteration 3037: Achieved Loss=> 24.6439263067
Gradients: w=>0.0800782164, b=>-1.3815653462
Iteration 3038: Achieved Loss=> 24.6420116804
Gradients: w=>0.0800356443, b=>-1.3808308637
Iteration 3039: Achieved Loss=> 24.6400990894
Gradients: w=>0.0799930949, b=>-1.3800967716
Iteration 3040: Achieved Loss=> 24.6381885314
Gradients: w=>0.0799505681, b=>-1.3793630698
Iteration 3041: Achieved Loss=> 24.6362800042
Gradients: w=>0.0799080639, b=>-1.3786297580
Iteration 3042: Achieved Loss=> 24.6343735058
Gradients: w=>0.0798655824, b=>-1.3778968361
Iteration 3043: Achieved Loss=> 24.6324690340
Gradients: w=>0.0798231234, b=>-1.3771643039
Iteration 3044: Achieved Loss=> 24.6305665866
Gradients: w=>0.0797806869, b=>-1.3764321611
Iteration 3045: Achieved Loss=> 24.6286661614
Gradients: w=>0.0797382731, b=>-1.3757004075
Iteration 3046: Achieved Loss=> 24.6267677564
Gradients: w=>0.0796958818, b=>-1.3749690429
Iteration 3047: Achieved Loss=> 24.6248713693
Gradients: w=>0.0796535130, b=>-1.3742380672
Iteration 3048: Achieved Loss=> 24.6229769981
Gradients: w=>0.0796111667, b=>-1.3735074800
Iteration 3049: Achieved Loss=> 24.6210846405
Gradients: w=>0.0795688430, b=>-1.3727772813
Iteration 3050: Achieved Loss=> 24.6191942944
Gradients: w=>0.0795265418, b=>-1.3720474708
Iteration 3051: Achieved Loss=> 24.6173059578
Gradients: w=>0.0794842630, b=>-1.3713180482
Iteration 3052: Achieved Loss=> 24.6154196284
Gradients: w=>0.0794420067, b=>-1.3705890134
Iteration 3053: Achieved Loss=> 24.6135353041
Gradients: w=>0.0793997729, b=>-1.3698603662
Iteration 3054: Achieved Loss=> 24.6116529829
Gradients: w=>0.0793575616, b=>-1.3691321064
Iteration 3055: Achieved Loss=> 24.6097726625
Gradients: w=>0.0793153727, b=>-1.3684042338
Iteration 3056: Achieved Loss=> 24.6078943408
Gradients: w=>0.0792732062, b=>-1.3676767481
Iteration 3057: Achieved Loss=> 24.6060180158
Gradients: w=>0.0792310621, b=>-1.3669496491
Iteration 3058: Achieved Loss=> 24.6041436852
Gradients: w=>0.0791889404, b=>-1.3662229368
Iteration 3059: Achieved Loss=> 24.6022713471
Gradients: w=>0.0791468412, b=>-1.3654966107
Iteration 3060: Achieved Loss=> 24.6004009991
Gradients: w=>0.0791047643, b=>-1.3647706708
Iteration 3061: Achieved Loss=> 24.5985326394
Gradients: w=>0.0790627098, b=>-1.3640451168
Iteration 3062: Achieved Loss=> 24.5966662656
Gradients: w=>0.0790206776, b=>-1.3633199486
Iteration 3063: Achieved Loss=> 24.5948018758
Gradients: w=>0.0789786678, b=>-1.3625951658
Iteration 3064: Achieved Loss=> 24.5929394677
Gradients: w=>0.0789366803, b=>-1.3618707684
Iteration 3065: Achieved Loss=> 24.5910790394
Gradients: w=>0.0788947151, b=>-1.3611467561
Iteration 3066: Achieved Loss=> 24.5892205887
Gradients: w=>0.0788527723, b=>-1.3604231287
Iteration 3067: Achieved Loss=> 24.5873641135
Gradients: w=>0.0788108517, b=>-1.3596998860
Iteration 3068: Achieved Loss=> 24.5855096116
Gradients: w=>0.0787689535, b=>-1.3589770278
Iteration 3069: Achieved Loss=> 24.5836570811
Gradients: w=>0.0787270775, b=>-1.3582545539
Iteration 3070: Achieved Loss=> 24.5818065197
Gradients: w=>0.0786852237, b=>-1.3575324641
Iteration 3071: Achieved Loss=> 24.5799579255
Gradients: w=>0.0786433923, b=>-1.3568107582
Iteration 3072: Achieved Loss=> 24.5781112962
Gradients: w=>0.0786015830, b=>-1.3560894359
Iteration 3073: Achieved Loss=> 24.5762666300
Gradients: w=>0.0785597960, b=>-1.3553684972
Iteration 3074: Achieved Loss=> 24.5744239245
Gradients: w=>0.0785180312, b=>-1.3546479417
Iteration 3075: Achieved Loss=> 24.5725831778
Gradients: w=>0.0784762886, b=>-1.3539277692
Iteration 3076: Achieved Loss=> 24.5707443878
Gradients: w=>0.0784345682, b=>-1.3532079797
Iteration 3077: Achieved Loss=> 24.5689075523
Gradients: w=>0.0783928700, b=>-1.3524885728
Iteration 3078: Achieved Loss=> 24.5670726694
Gradients: w=>0.0783511939, b=>-1.3517695483
Iteration 3079: Achieved Loss=> 24.5652397370
Gradients: w=>0.0783095400, b=>-1.3510509062
Iteration 3080: Achieved Loss=> 24.5634087529
Gradients: w=>0.0782679083, b=>-1.3503326460
Iteration 3081: Achieved Loss=> 24.5615797151
Gradients: w=>0.0782262986, b=>-1.3496147678
Iteration 3082: Achieved Loss=> 24.5597526215
Gradients: w=>0.0781847111, b=>-1.3488972711
Iteration 3083: Achieved Loss=> 24.5579274701
Gradients: w=>0.0781431457, b=>-1.3481801559
Iteration 3084: Achieved Loss=> 24.5561042587
Gradients: w=>0.0781016025, b=>-1.3474634220
Iteration 3085: Achieved Loss=> 24.5542829855
Gradients: w=>0.0780600812, b=>-1.3467470691
Iteration 3086: Achieved Loss=> 24.5524636482
Gradients: w=>0.0780185821, b=>-1.3460310970
Iteration 3087: Achieved Loss=> 24.5506462448
Gradients: w=>0.0779771050, b=>-1.3453155055
Iteration 3088: Achieved Loss=> 24.5488307732
Gradients: w=>0.0779356500, b=>-1.3446002945
Iteration 3089: Achieved Loss=> 24.5470172315
Gradients: w=>0.0778942170, b=>-1.3438854637
Iteration 3090: Achieved Loss=> 24.5452056175
Gradients: w=>0.0778528061, b=>-1.3431710130
Iteration 3091: Achieved Loss=> 24.5433959293
Gradients: w=>0.0778114171, b=>-1.3424569420
Iteration 3092: Achieved Loss=> 24.5415881647
Gradients: w=>0.0777700502, b=>-1.3417432507
Iteration 3093: Achieved Loss=> 24.5397823217
Gradients: w=>0.0777287052, b=>-1.3410299388
Iteration 3094: Achieved Loss=> 24.5379783983
Gradients: w=>0.0776873823, b=>-1.3403170061
Iteration 3095: Achieved Loss=> 24.5361763924
Gradients: w=>0.0776460813, b=>-1.3396044525
Iteration 3096: Achieved Loss=> 24.5343763020
Gradients: w=>0.0776048022, b=>-1.3388922776
Iteration 3097: Achieved Loss=> 24.5325781251
Gradients: w=>0.0775635451, b=>-1.3381804814
Iteration 3098: Achieved Loss=> 24.5307818596
Gradients: w=>0.0775223100, b=>-1.3374690636
Iteration 3099: Achieved Loss=> 24.5289875035
Gradients: w=>0.0774810967, b=>-1.3367580240
Iteration 3100: Achieved Loss=> 24.5271950547
Gradients: w=>0.0774399054, b=>-1.3360473624
Iteration 3101: Achieved Loss=> 24.5254045113
Gradients: w=>0.0773987360, b=>-1.3353370786
Iteration 3102: Achieved Loss=> 24.5236158712
Gradients: w=>0.0773575884, b=>-1.3346271724
Iteration 3103: Achieved Loss=> 24.5218291324
Gradients: w=>0.0773164628, b=>-1.3339176436
Iteration 3104: Achieved Loss=> 24.5200442929
Gradients: w=>0.0772753590, b=>-1.3332084921
Iteration 3105: Achieved Loss=> 24.5182613506
Gradients: w=>0.0772342770, b=>-1.3324997175
Iteration 3106: Achieved Loss=> 24.5164803035
Gradients: w=>0.0771932169, b=>-1.3317913198
Iteration 3107: Achieved Loss=> 24.5147011496
Gradients: w=>0.0771521786, b=>-1.3310832986
Iteration 3108: Achieved Loss=> 24.5129238870
Gradients: w=>0.0771111622, b=>-1.3303756539
Iteration 3109: Achieved Loss=> 24.5111485135
Gradients: w=>0.0770701675, b=>-1.3296683853
Iteration 3110: Achieved Loss=> 24.5093750272
Gradients: w=>0.0770291946, b=>-1.3289614928
Iteration 3111: Achieved Loss=> 24.5076034261
Gradients: w=>0.0769882435, b=>-1.3282549761
Iteration 3112: Achieved Loss=> 24.5058337082
Gradients: w=>0.0769473142, b=>-1.3275488350
Iteration 3113: Achieved Loss=> 24.5040658714
Gradients: w=>0.0769064067, b=>-1.3268430693
Iteration 3114: Achieved Loss=> 24.5022999139
Gradients: w=>0.0768655209, b=>-1.3261376788
Iteration 3115: Achieved Loss=> 24.5005358334
Gradients: w=>0.0768246568, b=>-1.3254326633
Iteration 3116: Achieved Loss=> 24.4987736282
Gradients: w=>0.0767838145, b=>-1.3247280226
Iteration 3117: Achieved Loss=> 24.4970132962
Gradients: w=>0.0767429938, b=>-1.3240237565
Iteration 3118: Achieved Loss=> 24.4952548353
Gradients: w=>0.0767021949, b=>-1.3233198648
Iteration 3119: Achieved Loss=> 24.4934982437
Gradients: w=>0.0766614176, b=>-1.3226163474
Iteration 3120: Achieved Loss=> 24.4917435193
Gradients: w=>0.0766206621, b=>-1.3219132039
Iteration 3121: Achieved Loss=> 24.4899906601
Gradients: w=>0.0765799282, b=>-1.3212104343
Iteration 3122: Achieved Loss=> 24.4882396642
Gradients: w=>0.0765392159, b=>-1.3205080383
Iteration 3123: Achieved Loss=> 24.4864905295
Gradients: w=>0.0764985254, b=>-1.3198060157
Iteration 3124: Achieved Loss=> 24.4847432541
Gradients: w=>0.0764578564, b=>-1.3191043663
Iteration 3125: Achieved Loss=> 24.4829978361
Gradients: w=>0.0764172090, b=>-1.3184030899
Iteration 3126: Achieved Loss=> 24.4812542734
Gradients: w=>0.0763765833, b=>-1.3177021864
Iteration 3127: Achieved Loss=> 24.4795125640
Gradients: w=>0.0763359792, b=>-1.3170016554
Iteration 3128: Achieved Loss=> 24.4777727061
Gradients: w=>0.0762953966, b=>-1.3163014969
Iteration 3129: Achieved Loss=> 24.4760346976
Gradients: w=>0.0762548357, b=>-1.3156017106
Iteration 3130: Achieved Loss=> 24.4742985366
Gradients: w=>0.0762142962, b=>-1.3149022964
Iteration 3131: Achieved Loss=> 24.4725642210
Gradients: w=>0.0761737784, b=>-1.3142032540
Iteration 3132: Achieved Loss=> 24.4708317490
Gradients: w=>0.0761332821, b=>-1.3135045832
Iteration 3133: Achieved Loss=> 24.4691011186
Gradients: w=>0.0760928073, b=>-1.3128062838
Iteration 3134: Achieved Loss=> 24.4673723278
Gradients: w=>0.0760523540, b=>-1.3121083557
Iteration 3135: Achieved Loss=> 24.4656453747
Gradients: w=>0.0760119222, b=>-1.3114107986
Iteration 3136: Achieved Loss=> 24.4639202573
Gradients: w=>0.0759715120, b=>-1.3107136124
Iteration 3137: Achieved Loss=> 24.4621969736
Gradients: w=>0.0759311232, b=>-1.3100167968
Iteration 3138: Achieved Loss=> 24.4604755218
Gradients: w=>0.0758907559, b=>-1.3093203517
Iteration 3139: Achieved Loss=> 24.4587558998
Gradients: w=>0.0758504100, b=>-1.3086242768
Iteration 3140: Achieved Loss=> 24.4570381058
Gradients: w=>0.0758100856, b=>-1.3079285719
Iteration 3141: Achieved Loss=> 24.4553221377
Gradients: w=>0.0757697826, b=>-1.3072332370
Iteration 3142: Achieved Loss=> 24.4536079937
Gradients: w=>0.0757295011, b=>-1.3065382717
Iteration 3143: Achieved Loss=> 24.4518956718
Gradients: w=>0.0756892410, b=>-1.3058436758
Iteration 3144: Achieved Loss=> 24.4501851700
Gradients: w=>0.0756490022, b=>-1.3051494492
Iteration 3145: Achieved Loss=> 24.4484764864
Gradients: w=>0.0756087849, b=>-1.3044555917
Iteration 3146: Achieved Loss=> 24.4467696192
Gradients: w=>0.0755685890, b=>-1.3037621031
Iteration 3147: Achieved Loss=> 24.4450645663
Gradients: w=>0.0755284144, b=>-1.3030689831
Iteration 3148: Achieved Loss=> 24.4433613259
Gradients: w=>0.0754882611, b=>-1.3023762317
Iteration 3149: Achieved Loss=> 24.4416598959
Gradients: w=>0.0754481293, b=>-1.3016838485
Iteration 3150: Achieved Loss=> 24.4399602746
Gradients: w=>0.0754080187, b=>-1.3009918334
Iteration 3151: Achieved Loss=> 24.4382624599
Gradients: w=>0.0753679295, b=>-1.3003001862
Iteration 3152: Achieved Loss=> 24.4365664499
Gradients: w=>0.0753278616, b=>-1.2996089067
Iteration 3153: Achieved Loss=> 24.4348722428
Gradients: w=>0.0752878150, b=>-1.2989179947
Iteration 3154: Achieved Loss=> 24.4331798366
Gradients: w=>0.0752477897, b=>-1.2982274500
Iteration 3155: Achieved Loss=> 24.4314892293
Gradients: w=>0.0752077857, b=>-1.2975372725
Iteration 3156: Achieved Loss=> 24.4298004192
Gradients: w=>0.0751678029, b=>-1.2968474618
Iteration 3157: Achieved Loss=> 24.4281134042
Gradients: w=>0.0751278414, b=>-1.2961580179
Iteration 3158: Achieved Loss=> 24.4264281825
Gradients: w=>0.0750879011, b=>-1.2954689405
Iteration 3159: Achieved Loss=> 24.4247447521
Gradients: w=>0.0750479821, b=>-1.2947802295
Iteration 3160: Achieved Loss=> 24.4230631112
Gradients: w=>0.0750080843, b=>-1.2940918846
Iteration 3161: Achieved Loss=> 24.4213832578
Gradients: w=>0.0749682077, b=>-1.2934039056
Iteration 3162: Achieved Loss=> 24.4197051901
Gradients: w=>0.0749283523, b=>-1.2927162924
Iteration 3163: Achieved Loss=> 24.4180289061
Gradients: w=>0.0748885181, b=>-1.2920290447
Iteration 3164: Achieved Loss=> 24.4163544040
Gradients: w=>0.0748487050, b=>-1.2913421624
Iteration 3165: Achieved Loss=> 24.4146816819
Gradients: w=>0.0748089132, b=>-1.2906556453
Iteration 3166: Achieved Loss=> 24.4130107378
Gradients: w=>0.0747691424, b=>-1.2899694931
Iteration 3167: Achieved Loss=> 24.4113415699
Gradients: w=>0.0747293929, b=>-1.2892837058
Iteration 3168: Achieved Loss=> 24.4096741762
Gradients: w=>0.0746896644, b=>-1.2885982830
Iteration 3169: Achieved Loss=> 24.4080085550
Gradients: w=>0.0746499571, b=>-1.2879132246
Iteration 3170: Achieved Loss=> 24.4063447043
Gradients: w=>0.0746102709, b=>-1.2872285304
Iteration 3171: Achieved Loss=> 24.4046826223
Gradients: w=>0.0745706058, b=>-1.2865442002
Iteration 3172: Achieved Loss=> 24.4030223070
Gradients: w=>0.0745309618, b=>-1.2858602338
Iteration 3173: Achieved Loss=> 24.4013637566
Gradients: w=>0.0744913388, b=>-1.2851766310
Iteration 3174: Achieved Loss=> 24.3997069691
Gradients: w=>0.0744517369, b=>-1.2844933917
Iteration 3175: Achieved Loss=> 24.3980519429
Gradients: w=>0.0744121561, b=>-1.2838105156
Iteration 3176: Achieved Loss=> 24.3963986758
Gradients: w=>0.0743725963, b=>-1.2831280025
Iteration 3177: Achieved Loss=> 24.3947471662
Gradients: w=>0.0743330576, b=>-1.2824458522
Iteration 3178: Achieved Loss=> 24.3930974121
Gradients: w=>0.0742935398, b=>-1.2817640647
Iteration 3179: Achieved Loss=> 24.3914494116
Gradients: w=>0.0742540431, b=>-1.2810826395
Iteration 3180: Achieved Loss=> 24.3898031629
Gradients: w=>0.0742145674, b=>-1.2804015767
Iteration 3181: Achieved Loss=> 24.3881586642
Gradients: w=>0.0741751126, b=>-1.2797208759
Iteration 3182: Achieved Loss=> 24.3865159135
Gradients: w=>0.0741356789, b=>-1.2790405370
Iteration 3183: Achieved Loss=> 24.3848749090
Gradients: w=>0.0740962661, b=>-1.2783605598
Iteration 3184: Achieved Loss=> 24.3832356489
Gradients: w=>0.0740568742, b=>-1.2776809441
Iteration 3185: Achieved Loss=> 24.3815981313
Gradients: w=>0.0740175033, b=>-1.2770016897
Iteration 3186: Achieved Loss=> 24.3799623543
Gradients: w=>0.0739781533, b=>-1.2763227964
Iteration 3187: Achieved Loss=> 24.3783283161
Gradients: w=>0.0739388243, b=>-1.2756442640
Iteration 3188: Achieved Loss=> 24.3766960149
Gradients: w=>0.0738995161, b=>-1.2749660924
Iteration 3189: Achieved Loss=> 24.3750654487
Gradients: w=>0.0738602289, b=>-1.2742882813
Iteration 3190: Achieved Loss=> 24.3734366159
Gradients: w=>0.0738209625, b=>-1.2736108305
Iteration 3191: Achieved Loss=> 24.3718095144
Gradients: w=>0.0737817170, b=>-1.2729337399
Iteration 3192: Achieved Loss=> 24.3701841425
Gradients: w=>0.0737424924, b=>-1.2722570092
Iteration 3193: Achieved Loss=> 24.3685604984
Gradients: w=>0.0737032886, b=>-1.2715806384
Iteration 3194: Achieved Loss=> 24.3669385801
Gradients: w=>0.0736641057, b=>-1.2709046271
Iteration 3195: Achieved Loss=> 24.3653183859
Gradients: w=>0.0736249436, b=>-1.2702289751
Iteration 3196: Achieved Loss=> 24.3636999140
Gradients: w=>0.0735858023, b=>-1.2695536824
Iteration 3197: Achieved Loss=> 24.3620831624
Gradients: w=>0.0735466818, b=>-1.2688787487
Iteration 3198: Achieved Loss=> 24.3604681295
Gradients: w=>0.0735075822, b=>-1.2682041738
Iteration 3199: Achieved Loss=> 24.3588548132
Gradients: w=>0.0734685033, b=>-1.2675299576
Iteration 3200: Achieved Loss=> 24.3572432119
Gradients: w=>0.0734294452, b=>-1.2668560997
Iteration 3201: Achieved Loss=> 24.3556333237
Gradients: w=>0.0733904078, b=>-1.2661826002
Iteration 3202: Achieved Loss=> 24.3540251468
Gradients: w=>0.0733513912, b=>-1.2655094586
Iteration 3203: Achieved Loss=> 24.3524186793
Gradients: w=>0.0733123954, b=>-1.2648366749
Iteration 3204: Achieved Loss=> 24.3508139195
Gradients: w=>0.0732734203, b=>-1.2641642489
Iteration 3205: Achieved Loss=> 24.3492108654
Gradients: w=>0.0732344659, b=>-1.2634921804
Iteration 3206: Achieved Loss=> 24.3476095154
Gradients: w=>0.0731955322, b=>-1.2628204692
Iteration 3207: Achieved Loss=> 24.3460098676
Gradients: w=>0.0731566192, b=>-1.2621491151
Iteration 3208: Achieved Loss=> 24.3444119202
Gradients: w=>0.0731177269, b=>-1.2614781179
Iteration 3209: Achieved Loss=> 24.3428156714
Gradients: w=>0.0730788552, b=>-1.2608074774
Iteration 3210: Achieved Loss=> 24.3412211194
Gradients: w=>0.0730400043, b=>-1.2601371934
Iteration 3211: Achieved Loss=> 24.3396282623
Gradients: w=>0.0730011740, b=>-1.2594672658
Iteration 3212: Achieved Loss=> 24.3380370984
Gradients: w=>0.0729623643, b=>-1.2587976944
Iteration 3213: Achieved Loss=> 24.3364476259
Gradients: w=>0.0729235753, b=>-1.2581284789
Iteration 3214: Achieved Loss=> 24.3348598429
Gradients: w=>0.0728848069, b=>-1.2574596191
Iteration 3215: Achieved Loss=> 24.3332737478
Gradients: w=>0.0728460591, b=>-1.2567911150
Iteration 3216: Achieved Loss=> 24.3316893386
Gradients: w=>0.0728073319, b=>-1.2561229663
Iteration 3217: Achieved Loss=> 24.3301066136
Gradients: w=>0.0727686253, b=>-1.2554551728
Iteration 3218: Achieved Loss=> 24.3285255710
Gradients: w=>0.0727299392, b=>-1.2547877342
Iteration 3219: Achieved Loss=> 24.3269462090
Gradients: w=>0.0726912738, b=>-1.2541206506
Iteration 3220: Achieved Loss=> 24.3253685259
Gradients: w=>0.0726526288, b=>-1.2534539215
Iteration 3221: Achieved Loss=> 24.3237925198
Gradients: w=>0.0726140045, b=>-1.2527875470
Iteration 3222: Achieved Loss=> 24.3222181889
Gradients: w=>0.0725754006, b=>-1.2521215266
Iteration 3223: Achieved Loss=> 24.3206455316
Gradients: w=>0.0725368173, b=>-1.2514558604
Iteration 3224: Achieved Loss=> 24.3190745459
Gradients: w=>0.0724982545, b=>-1.2507905481
Iteration 3225: Achieved Loss=> 24.3175052302
Gradients: w=>0.0724597122, b=>-1.2501255894
Iteration 3226: Achieved Loss=> 24.3159375826
Gradients: w=>0.0724211904, b=>-1.2494609843
Iteration 3227: Achieved Loss=> 24.3143716014
Gradients: w=>0.0723826891, b=>-1.2487967325
Iteration 3228: Achieved Loss=> 24.3128072848
Gradients: w=>0.0723442082, b=>-1.2481328338
Iteration 3229: Achieved Loss=> 24.3112446310
Gradients: w=>0.0723057478, b=>-1.2474692881
Iteration 3230: Achieved Loss=> 24.3096836383
Gradients: w=>0.0722673079, b=>-1.2468060951
Iteration 3231: Achieved Loss=> 24.3081243049
Gradients: w=>0.0722288884, b=>-1.2461432547
Iteration 3232: Achieved Loss=> 24.3065666291
Gradients: w=>0.0721904893, b=>-1.2454807667
Iteration 3233: Achieved Loss=> 24.3050106090
Gradients: w=>0.0721521106, b=>-1.2448186309
Iteration 3234: Achieved Loss=> 24.3034562429
Gradients: w=>0.0721137523, b=>-1.2441568471
Iteration 3235: Achieved Loss=> 24.3019035291
Gradients: w=>0.0720754144, b=>-1.2434954151
Iteration 3236: Achieved Loss=> 24.3003524658
Gradients: w=>0.0720370969, b=>-1.2428343348
Iteration 3237: Achieved Loss=> 24.2988030513
Gradients: w=>0.0719987998, b=>-1.2421736059
Iteration 3238: Achieved Loss=> 24.2972552837
Gradients: w=>0.0719605230, b=>-1.2415132283
Iteration 3239: Achieved Loss=> 24.2957091614
Gradients: w=>0.0719222666, b=>-1.2408532018
Iteration 3240: Achieved Loss=> 24.2941646825
Gradients: w=>0.0718840305, b=>-1.2401935261
Iteration 3241: Achieved Loss=> 24.2926218455
Gradients: w=>0.0718458148, b=>-1.2395342012
Iteration 3242: Achieved Loss=> 24.2910806484
Gradients: w=>0.0718076193, b=>-1.2388752267
Iteration 3243: Achieved Loss=> 24.2895410896
Gradients: w=>0.0717694442, b=>-1.2382166026
Iteration 3244: Achieved Loss=> 24.2880031673
Gradients: w=>0.0717312893, b=>-1.2375583287
Iteration 3245: Achieved Loss=> 24.2864668798
Gradients: w=>0.0716931548, b=>-1.2369004047
Iteration 3246: Achieved Loss=> 24.2849322253
Gradients: w=>0.0716550405, b=>-1.2362428305
Iteration 3247: Achieved Loss=> 24.2833992021
Gradients: w=>0.0716169465, b=>-1.2355856058
Iteration 3248: Achieved Loss=> 24.2818678085
Gradients: w=>0.0715788727, b=>-1.2349287306
Iteration 3249: Achieved Loss=> 24.2803380428
Gradients: w=>0.0715408192, b=>-1.2342722046
Iteration 3250: Achieved Loss=> 24.2788099031
Gradients: w=>0.0715027859, b=>-1.2336160276
Iteration 3251: Achieved Loss=> 24.2772833878
Gradients: w=>0.0714647728, b=>-1.2329601995
Iteration 3252: Achieved Loss=> 24.2757584952
Gradients: w=>0.0714267800, b=>-1.2323047200
Iteration 3253: Achieved Loss=> 24.2742352235
Gradients: w=>0.0713888073, b=>-1.2316495890
Iteration 3254: Achieved Loss=> 24.2727135710
Gradients: w=>0.0713508548, b=>-1.2309948062
Iteration 3255: Achieved Loss=> 24.2711935360
Gradients: w=>0.0713129225, b=>-1.2303403716
Iteration 3256: Achieved Loss=> 24.2696751168
Gradients: w=>0.0712750104, b=>-1.2296862849
Iteration 3257: Achieved Loss=> 24.2681583116
Gradients: w=>0.0712371184, b=>-1.2290325459
Iteration 3258: Achieved Loss=> 24.2666431187
Gradients: w=>0.0711992465, b=>-1.2283791545
Iteration 3259: Achieved Loss=> 24.2651295365
Gradients: w=>0.0711613948, b=>-1.2277261104
Iteration 3260: Achieved Loss=> 24.2636175632
Gradients: w=>0.0711235633, b=>-1.2270734135
Iteration 3261: Achieved Loss=> 24.2621071970
Gradients: w=>0.0710857518, b=>-1.2264210637
Iteration 3262: Achieved Loss=> 24.2605984364
Gradients: w=>0.0710479604, b=>-1.2257690606
Iteration 3263: Achieved Loss=> 24.2590912795
Gradients: w=>0.0710101891, b=>-1.2251174041
Iteration 3264: Achieved Loss=> 24.2575857247
Gradients: w=>0.0709724379, b=>-1.2244660941
Iteration 3265: Achieved Loss=> 24.2560817703
Gradients: w=>0.0709347068, b=>-1.2238151303
Iteration 3266: Achieved Loss=> 24.2545794145
Gradients: w=>0.0708969957, b=>-1.2231645127
Iteration 3267: Achieved Loss=> 24.2530786558
Gradients: w=>0.0708593047, b=>-1.2225142409
Iteration 3268: Achieved Loss=> 24.2515794923
Gradients: w=>0.0708216337, b=>-1.2218643148
Iteration 3269: Achieved Loss=> 24.2500819223
Gradients: w=>0.0707839828, b=>-1.2212147342
Iteration 3270: Achieved Loss=> 24.2485859443
Gradients: w=>0.0707463519, b=>-1.2205654989
Iteration 3271: Achieved Loss=> 24.2470915565
Gradients: w=>0.0707087409, b=>-1.2199166089
Iteration 3272: Achieved Loss=> 24.2455987571
Gradients: w=>0.0706711500, b=>-1.2192680637
Iteration 3273: Achieved Loss=> 24.2441075446
Gradients: w=>0.0706335790, b=>-1.2186198634
Iteration 3274: Achieved Loss=> 24.2426179172
Gradients: w=>0.0705960281, b=>-1.2179720077
Iteration 3275: Achieved Loss=> 24.2411298732
Gradients: w=>0.0705584970, b=>-1.2173244964
Iteration 3276: Achieved Loss=> 24.2396434110
Gradients: w=>0.0705209860, b=>-1.2166773293
Iteration 3277: Achieved Loss=> 24.2381585289
Gradients: w=>0.0704834949, b=>-1.2160305063
Iteration 3278: Achieved Loss=> 24.2366752252
Gradients: w=>0.0704460237, b=>-1.2153840272
Iteration 3279: Achieved Loss=> 24.2351934982
Gradients: w=>0.0704085724, b=>-1.2147378917
Iteration 3280: Achieved Loss=> 24.2337133462
Gradients: w=>0.0703711410, b=>-1.2140920998
Iteration 3281: Achieved Loss=> 24.2322347676
Gradients: w=>0.0703337296, b=>-1.2134466511
Iteration 3282: Achieved Loss=> 24.2307577607
Gradients: w=>0.0702963380, b=>-1.2128015456
Iteration 3283: Achieved Loss=> 24.2292823238
Gradients: w=>0.0702589663, b=>-1.2121567831
Iteration 3284: Achieved Loss=> 24.2278084553
Gradients: w=>0.0702216145, b=>-1.2115123634
Iteration 3285: Achieved Loss=> 24.2263361535
Gradients: w=>0.0701842825, b=>-1.2108682862
Iteration 3286: Achieved Loss=> 24.2248654167
Gradients: w=>0.0701469704, b=>-1.2102245515
Iteration 3287: Achieved Loss=> 24.2233962433
Gradients: w=>0.0701096781, b=>-1.2095811589
Iteration 3288: Achieved Loss=> 24.2219286316
Gradients: w=>0.0700724057, b=>-1.2089381085
Iteration 3289: Achieved Loss=> 24.2204625799
Gradients: w=>0.0700351530, b=>-1.2082953999
Iteration 3290: Achieved Loss=> 24.2189980866
Gradients: w=>0.0699979202, b=>-1.2076530329
Iteration 3291: Achieved Loss=> 24.2175351500
Gradients: w=>0.0699607072, b=>-1.2070110075
Iteration 3292: Achieved Loss=> 24.2160737685
Gradients: w=>0.0699235139, b=>-1.2063693234
Iteration 3293: Achieved Loss=> 24.2146139404
Gradients: w=>0.0698863404, b=>-1.2057279805
Iteration 3294: Achieved Loss=> 24.2131556641
Gradients: w=>0.0698491867, b=>-1.2050869785
Iteration 3295: Achieved Loss=> 24.2116989378
Gradients: w=>0.0698120527, b=>-1.2044463172
Iteration 3296: Achieved Loss=> 24.2102437601
Gradients: w=>0.0697749385, b=>-1.2038059966
Iteration 3297: Achieved Loss=> 24.2087901292
Gradients: w=>0.0697378440, b=>-1.2031660164
Iteration 3298: Achieved Loss=> 24.2073380434
Gradients: w=>0.0697007692, b=>-1.2025263764
Iteration 3299: Achieved Loss=> 24.2058875012
Gradients: w=>0.0696637141, b=>-1.2018870765
Iteration 3300: Achieved Loss=> 24.2044385009
Gradients: w=>0.0696266788, b=>-1.2012481164
Iteration 3301: Achieved Loss=> 24.2029910409
Gradients: w=>0.0695896631, b=>-1.2006094960
Iteration 3302: Achieved Loss=> 24.2015451194
Gradients: w=>0.0695526671, b=>-1.1999712152
Iteration 3303: Achieved Loss=> 24.2001007350
Gradients: w=>0.0695156908, b=>-1.1993332736
Iteration 3304: Achieved Loss=> 24.1986578859
Gradients: w=>0.0694787341, b=>-1.1986956713
Iteration 3305: Achieved Loss=> 24.1972165705
Gradients: w=>0.0694417971, b=>-1.1980584078
Iteration 3306: Achieved Loss=> 24.1957767872
Gradients: w=>0.0694048797, b=>-1.1974214832
Iteration 3307: Achieved Loss=> 24.1943385344
Gradients: w=>0.0693679819, b=>-1.1967848972
Iteration 3308: Achieved Loss=> 24.1929018104
Gradients: w=>0.0693311037, b=>-1.1961486496
Iteration 3309: Achieved Loss=> 24.1914666136
Gradients: w=>0.0692942452, b=>-1.1955127403
Iteration 3310: Achieved Loss=> 24.1900329424
Gradients: w=>0.0692574063, b=>-1.1948771690
Iteration 3311: Achieved Loss=> 24.1886007951
Gradients: w=>0.0692205869, b=>-1.1942419356
Iteration 3312: Achieved Loss=> 24.1871701702
Gradients: w=>0.0691837871, b=>-1.1936070400
Iteration 3313: Achieved Loss=> 24.1857410660
Gradients: w=>0.0691470069, b=>-1.1929724818
Iteration 3314: Achieved Loss=> 24.1843134810
Gradients: w=>0.0691102462, b=>-1.1923382610
Iteration 3315: Achieved Loss=> 24.1828874134
Gradients: w=>0.0690735051, b=>-1.1917043774
Iteration 3316: Achieved Loss=> 24.1814628617
Gradients: w=>0.0690367835, b=>-1.1910708308
Iteration 3317: Achieved Loss=> 24.1800398242
Gradients: w=>0.0690000814, b=>-1.1904376210
Iteration 3318: Achieved Loss=> 24.1786182995
Gradients: w=>0.0689633989, b=>-1.1898047478
Iteration 3319: Achieved Loss=> 24.1771982857
Gradients: w=>0.0689267358, b=>-1.1891722111
Iteration 3320: Achieved Loss=> 24.1757797815
Gradients: w=>0.0688900922, b=>-1.1885400106
Iteration 3321: Achieved Loss=> 24.1743627850
Gradients: w=>0.0688534681, b=>-1.1879081463
Iteration 3322: Achieved Loss=> 24.1729472948
Gradients: w=>0.0688168635, b=>-1.1872766178
Iteration 3323: Achieved Loss=> 24.1715333092
Gradients: w=>0.0687802784, b=>-1.1866454251
Iteration 3324: Achieved Loss=> 24.1701208267
Gradients: w=>0.0687437127, b=>-1.1860145680
Iteration 3325: Achieved Loss=> 24.1687098456
Gradients: w=>0.0687071664, b=>-1.1853840463
Iteration 3326: Achieved Loss=> 24.1673003644
Gradients: w=>0.0686706395, b=>-1.1847538597
Iteration 3327: Achieved Loss=> 24.1658923814
Gradients: w=>0.0686341321, b=>-1.1841240082
Iteration 3328: Achieved Loss=> 24.1644858950
Gradients: w=>0.0685976441, b=>-1.1834944915
Iteration 3329: Achieved Loss=> 24.1630809037
Gradients: w=>0.0685611755, b=>-1.1828653095
Iteration 3330: Achieved Loss=> 24.1616774059
Gradients: w=>0.0685247263, b=>-1.1822364620
Iteration 3331: Achieved Loss=> 24.1602754000
Gradients: w=>0.0684882964, b=>-1.1816079488
Iteration 3332: Achieved Loss=> 24.1588748844
Gradients: w=>0.0684518859, b=>-1.1809797698
Iteration 3333: Achieved Loss=> 24.1574758575
Gradients: w=>0.0684154948, b=>-1.1803519247
Iteration 3334: Achieved Loss=> 24.1560783177
Gradients: w=>0.0683791230, b=>-1.1797244134
Iteration 3335: Achieved Loss=> 24.1546822635
Gradients: w=>0.0683427706, b=>-1.1790972357
Iteration 3336: Achieved Loss=> 24.1532876933
Gradients: w=>0.0683064375, b=>-1.1784703914
Iteration 3337: Achieved Loss=> 24.1518946055
Gradients: w=>0.0682701237, b=>-1.1778438803
Iteration 3338: Achieved Loss=> 24.1505029985
Gradients: w=>0.0682338292, b=>-1.1772177024
Iteration 3339: Achieved Loss=> 24.1491128707
Gradients: w=>0.0681975540, b=>-1.1765918573
Iteration 3340: Achieved Loss=> 24.1477242206
Gradients: w=>0.0681612981, b=>-1.1759663450
Iteration 3341: Achieved Loss=> 24.1463370467
Gradients: w=>0.0681250615, b=>-1.1753411652
Iteration 3342: Achieved Loss=> 24.1449513472
Gradients: w=>0.0680888441, b=>-1.1747163177
Iteration 3343: Achieved Loss=> 24.1435671207
Gradients: w=>0.0680526460, b=>-1.1740918025
Iteration 3344: Achieved Loss=> 24.1421843657
Gradients: w=>0.0680164671, b=>-1.1734676192
Iteration 3345: Achieved Loss=> 24.1408030804
Gradients: w=>0.0679803074, b=>-1.1728437678
Iteration 3346: Achieved Loss=> 24.1394232635
Gradients: w=>0.0679441670, b=>-1.1722202481
Iteration 3347: Achieved Loss=> 24.1380449133
Gradients: w=>0.0679080458, b=>-1.1715970598
Iteration 3348: Achieved Loss=> 24.1366680282
Gradients: w=>0.0678719438, b=>-1.1709742029
Iteration 3349: Achieved Loss=> 24.1352926067
Gradients: w=>0.0678358610, b=>-1.1703516770
Iteration 3350: Achieved Loss=> 24.1339186473
Gradients: w=>0.0677997974, b=>-1.1697294822
Iteration 3351: Achieved Loss=> 24.1325461483
Gradients: w=>0.0677637529, b=>-1.1691076181
Iteration 3352: Achieved Loss=> 24.1311751083
Gradients: w=>0.0677277276, b=>-1.1684860846
Iteration 3353: Achieved Loss=> 24.1298055257
Gradients: w=>0.0676917215, b=>-1.1678648815
Iteration 3354: Achieved Loss=> 24.1284373989
Gradients: w=>0.0676557345, b=>-1.1672440087
Iteration 3355: Achieved Loss=> 24.1270707264
Gradients: w=>0.0676197666, b=>-1.1666234659
Iteration 3356: Achieved Loss=> 24.1257055067
Gradients: w=>0.0675838179, b=>-1.1660032531
Iteration 3357: Achieved Loss=> 24.1243417381
Gradients: w=>0.0675478883, b=>-1.1653833700
Iteration 3358: Achieved Loss=> 24.1229794192
Gradients: w=>0.0675119777, b=>-1.1647638164
Iteration 3359: Achieved Loss=> 24.1216185484
Gradients: w=>0.0674760863, b=>-1.1641445922
Iteration 3360: Achieved Loss=> 24.1202591242
Gradients: w=>0.0674402139, b=>-1.1635256972
Iteration 3361: Achieved Loss=> 24.1189011451
Gradients: w=>0.0674043606, b=>-1.1629071312
Iteration 3362: Achieved Loss=> 24.1175446094
Gradients: w=>0.0673685264, b=>-1.1622888941
Iteration 3363: Achieved Loss=> 24.1161895157
Gradients: w=>0.0673327112, b=>-1.1616709856
Iteration 3364: Achieved Loss=> 24.1148358625
Gradients: w=>0.0672969151, b=>-1.1610534057
Iteration 3365: Achieved Loss=> 24.1134836481
Gradients: w=>0.0672611380, b=>-1.1604361541
Iteration 3366: Achieved Loss=> 24.1121328711
Gradients: w=>0.0672253799, b=>-1.1598192306
Iteration 3367: Achieved Loss=> 24.1107835300
Gradients: w=>0.0671896408, b=>-1.1592026351
Iteration 3368: Achieved Loss=> 24.1094356232
Gradients: w=>0.0671539208, b=>-1.1585863674
Iteration 3369: Achieved Loss=> 24.1080891492
Gradients: w=>0.0671182197, b=>-1.1579704273
Iteration 3370: Achieved Loss=> 24.1067441065
Gradients: w=>0.0670825376, b=>-1.1573548147
Iteration 3371: Achieved Loss=> 24.1054004935
Gradients: w=>0.0670468744, b=>-1.1567395293
Iteration 3372: Achieved Loss=> 24.1040583087
Gradients: w=>0.0670112303, b=>-1.1561245711
Iteration 3373: Achieved Loss=> 24.1027175507
Gradients: w=>0.0669756050, b=>-1.1555099398
Iteration 3374: Achieved Loss=> 24.1013782178
Gradients: w=>0.0669399987, b=>-1.1548956352
Iteration 3375: Achieved Loss=> 24.1000403087
Gradients: w=>0.0669044114, b=>-1.1542816573
Iteration 3376: Achieved Loss=> 24.0987038217
Gradients: w=>0.0668688429, b=>-1.1536680057
Iteration 3377: Achieved Loss=> 24.0973687553
Gradients: w=>0.0668332934, b=>-1.1530546804
Iteration 3378: Achieved Loss=> 24.0960351082
Gradients: w=>0.0667977628, b=>-1.1524416811
Iteration 3379: Achieved Loss=> 24.0947028786
Gradients: w=>0.0667622510, b=>-1.1518290078
Iteration 3380: Achieved Loss=> 24.0933720652
Gradients: w=>0.0667267582, b=>-1.1512166601
Iteration 3381: Achieved Loss=> 24.0920426664
Gradients: w=>0.0666912842, b=>-1.1506046380
Iteration 3382: Achieved Loss=> 24.0907146807
Gradients: w=>0.0666558291, b=>-1.1499929412
Iteration 3383: Achieved Loss=> 24.0893881067
Gradients: w=>0.0666203928, b=>-1.1493815697
Iteration 3384: Achieved Loss=> 24.0880629428
Gradients: w=>0.0665849753, b=>-1.1487705232
Iteration 3385: Achieved Loss=> 24.0867391875
Gradients: w=>0.0665495767, b=>-1.1481598015
Iteration 3386: Achieved Loss=> 24.0854168393
Gradients: w=>0.0665141969, b=>-1.1475494045
Iteration 3387: Achieved Loss=> 24.0840958967
Gradients: w=>0.0664788359, b=>-1.1469393320
Iteration 3388: Achieved Loss=> 24.0827763583
Gradients: w=>0.0664434937, b=>-1.1463295839
Iteration 3389: Achieved Loss=> 24.0814582225
Gradients: w=>0.0664081703, b=>-1.1457201599
Iteration 3390: Achieved Loss=> 24.0801414879
Gradients: w=>0.0663728657, b=>-1.1451110599
Iteration 3391: Achieved Loss=> 24.0788261529
Gradients: w=>0.0663375799, b=>-1.1445022837
Iteration 3392: Achieved Loss=> 24.0775122161
Gradients: w=>0.0663023128, b=>-1.1438938311
Iteration 3393: Achieved Loss=> 24.0761996760
Gradients: w=>0.0662670644, b=>-1.1432857021
Iteration 3394: Achieved Loss=> 24.0748885311
Gradients: w=>0.0662318348, b=>-1.1426778963
Iteration 3395: Achieved Loss=> 24.0735787799
Gradients: w=>0.0661966239, b=>-1.1420704136
Iteration 3396: Achieved Loss=> 24.0722704210
Gradients: w=>0.0661614318, b=>-1.1414632540
Iteration 3397: Achieved Loss=> 24.0709634528
Gradients: w=>0.0661262583, b=>-1.1408564171
Iteration 3398: Achieved Loss=> 24.0696578739
Gradients: w=>0.0660911036, b=>-1.1402499028
Iteration 3399: Achieved Loss=> 24.0683536828
Gradients: w=>0.0660559675, b=>-1.1396437109
Iteration 3400: Achieved Loss=> 24.0670508780
Gradients: w=>0.0660208501, b=>-1.1390378413
Iteration 3401: Achieved Loss=> 24.0657494581
Gradients: w=>0.0659857514, b=>-1.1384322939
Iteration 3402: Achieved Loss=> 24.0644494215
Gradients: w=>0.0659506714, b=>-1.1378270683
Iteration 3403: Achieved Loss=> 24.0631507669
Gradients: w=>0.0659156100, b=>-1.1372221645
Iteration 3404: Achieved Loss=> 24.0618534927
Gradients: w=>0.0658805672, b=>-1.1366175823
Iteration 3405: Achieved Loss=> 24.0605575975
Gradients: w=>0.0658455431, b=>-1.1360133215
Iteration 3406: Achieved Loss=> 24.0592630798
Gradients: w=>0.0658105376, b=>-1.1354093820
Iteration 3407: Achieved Loss=> 24.0579699381
Gradients: w=>0.0657755507, b=>-1.1348057635
Iteration 3408: Achieved Loss=> 24.0566781710
Gradients: w=>0.0657405824, b=>-1.1342024659
Iteration 3409: Achieved Loss=> 24.0553877771
Gradients: w=>0.0657056327, b=>-1.1335994891
Iteration 3410: Achieved Loss=> 24.0540987548
Gradients: w=>0.0656707015, b=>-1.1329968328
Iteration 3411: Achieved Loss=> 24.0528111027
Gradients: w=>0.0656357890, b=>-1.1323944969
Iteration 3412: Achieved Loss=> 24.0515248193
Gradients: w=>0.0656008950, b=>-1.1317924812
Iteration 3413: Achieved Loss=> 24.0502399033
Gradients: w=>0.0655660195, b=>-1.1311907856
Iteration 3414: Achieved Loss=> 24.0489563530
Gradients: w=>0.0655311626, b=>-1.1305894098
Iteration 3415: Achieved Loss=> 24.0476741672
Gradients: w=>0.0654963242, b=>-1.1299883538
Iteration 3416: Achieved Loss=> 24.0463933443
Gradients: w=>0.0654615044, b=>-1.1293876173
Iteration 3417: Achieved Loss=> 24.0451138829
Gradients: w=>0.0654267030, b=>-1.1287872002
Iteration 3418: Achieved Loss=> 24.0438357815
Gradients: w=>0.0653919202, b=>-1.1281871023
Iteration 3419: Achieved Loss=> 24.0425590388
Gradients: w=>0.0653571558, b=>-1.1275873234
Iteration 3420: Achieved Loss=> 24.0412836531
Gradients: w=>0.0653224100, b=>-1.1269878633
Iteration 3421: Achieved Loss=> 24.0400096232
Gradients: w=>0.0652876826, b=>-1.1263887220
Iteration 3422: Achieved Loss=> 24.0387369476
Gradients: w=>0.0652529736, b=>-1.1257898992
Iteration 3423: Achieved Loss=> 24.0374656248
Gradients: w=>0.0652182832, b=>-1.1251913947
Iteration 3424: Achieved Loss=> 24.0361956533
Gradients: w=>0.0651836111, b=>-1.1245932084
Iteration 3425: Achieved Loss=> 24.0349270319
Gradients: w=>0.0651489575, b=>-1.1239953402
Iteration 3426: Achieved Loss=> 24.0336597589
Gradients: w=>0.0651143223, b=>-1.1233977897
Iteration 3427: Achieved Loss=> 24.0323938330
Gradients: w=>0.0650797055, b=>-1.1228005570
Iteration 3428: Achieved Loss=> 24.0311292528
Gradients: w=>0.0650451072, b=>-1.1222036418
Iteration 3429: Achieved Loss=> 24.0298660168
Gradients: w=>0.0650105272, b=>-1.1216070439
Iteration 3430: Achieved Loss=> 24.0286041236
Gradients: w=>0.0649759656, b=>-1.1210107631
Iteration 3431: Achieved Loss=> 24.0273435717
Gradients: w=>0.0649414224, b=>-1.1204147994
Iteration 3432: Achieved Loss=> 24.0260843598
Gradients: w=>0.0649068975, b=>-1.1198191525
Iteration 3433: Achieved Loss=> 24.0248264864
Gradients: w=>0.0648723910, b=>-1.1192238223
Iteration 3434: Achieved Loss=> 24.0235699501
Gradients: w=>0.0648379029, b=>-1.1186288085
Iteration 3435: Achieved Loss=> 24.0223147495
Gradients: w=>0.0648034331, b=>-1.1180341111
Iteration 3436: Achieved Loss=> 24.0210608831
Gradients: w=>0.0647689816, b=>-1.1174397299
Iteration 3437: Achieved Loss=> 24.0198083496
Gradients: w=>0.0647345484, b=>-1.1168456646
Iteration 3438: Achieved Loss=> 24.0185571475
Gradients: w=>0.0647001335, b=>-1.1162519152
Iteration 3439: Achieved Loss=> 24.0173072754
Gradients: w=>0.0646657369, b=>-1.1156584814
Iteration 3440: Achieved Loss=> 24.0160587318
Gradients: w=>0.0646313587, b=>-1.1150653631
Iteration 3441: Achieved Loss=> 24.0148115155
Gradients: w=>0.0645969986, b=>-1.1144725601
Iteration 3442: Achieved Loss=> 24.0135656249
Gradients: w=>0.0645626569, b=>-1.1138800723
Iteration 3443: Achieved Loss=> 24.0123210586
Gradients: w=>0.0645283334, b=>-1.1132878995
Iteration 3444: Achieved Loss=> 24.0110778153
Gradients: w=>0.0644940282, b=>-1.1126960414
Iteration 3445: Achieved Loss=> 24.0098358936
Gradients: w=>0.0644597412, b=>-1.1121044981
Iteration 3446: Achieved Loss=> 24.0085952920
Gradients: w=>0.0644254724, b=>-1.1115132692
Iteration 3447: Achieved Loss=> 24.0073560091
Gradients: w=>0.0643912218, b=>-1.1109223546
Iteration 3448: Achieved Loss=> 24.0061180435
Gradients: w=>0.0643569895, b=>-1.1103317542
Iteration 3449: Achieved Loss=> 24.0048813939
Gradients: w=>0.0643227753, b=>-1.1097414677
Iteration 3450: Achieved Loss=> 24.0036460588
Gradients: w=>0.0642885794, b=>-1.1091514951
Iteration 3451: Achieved Loss=> 24.0024120369
Gradients: w=>0.0642544016, b=>-1.1085618361
Iteration 3452: Achieved Loss=> 24.0011793267
Gradients: w=>0.0642202420, b=>-1.1079724906
Iteration 3453: Achieved Loss=> 23.9999479268
Gradients: w=>0.0641861005, b=>-1.1073834585
Iteration 3454: Achieved Loss=> 23.9987178359
Gradients: w=>0.0641519772, b=>-1.1067947394
Iteration 3455: Achieved Loss=> 23.9974890525
Gradients: w=>0.0641178720, b=>-1.1062063334
Iteration 3456: Achieved Loss=> 23.9962615754
Gradients: w=>0.0640837850, b=>-1.1056182401
Iteration 3457: Achieved Loss=> 23.9950354030
Gradients: w=>0.0640497161, b=>-1.1050304595
Iteration 3458: Achieved Loss=> 23.9938105339
Gradients: w=>0.0640156653, b=>-1.1044429914
Iteration 3459: Achieved Loss=> 23.9925869669
Gradients: w=>0.0639816326, b=>-1.1038558356
Iteration 3460: Achieved Loss=> 23.9913647006
Gradients: w=>0.0639476180, b=>-1.1032689920
Iteration 3461: Achieved Loss=> 23.9901437334
Gradients: w=>0.0639136215, b=>-1.1026824603
Iteration 3462: Achieved Loss=> 23.9889240642
Gradients: w=>0.0638796431, b=>-1.1020962405
Iteration 3463: Achieved Loss=> 23.9877056914
Gradients: w=>0.0638456827, b=>-1.1015103323
Iteration 3464: Achieved Loss=> 23.9864886137
Gradients: w=>0.0638117404, b=>-1.1009247356
Iteration 3465: Achieved Loss=> 23.9852728298
Gradients: w=>0.0637778161, b=>-1.1003394502
Iteration 3466: Achieved Loss=> 23.9840583382
Gradients: w=>0.0637439098, b=>-1.0997544760
Iteration 3467: Achieved Loss=> 23.9828451375
Gradients: w=>0.0637100216, b=>-1.0991698127
Iteration 3468: Achieved Loss=> 23.9816332265
Gradients: w=>0.0636761514, b=>-1.0985854603
Iteration 3469: Achieved Loss=> 23.9804226038
Gradients: w=>0.0636422992, b=>-1.0980014186
Iteration 3470: Achieved Loss=> 23.9792132678
Gradients: w=>0.0636084650, b=>-1.0974176873
Iteration 3471: Achieved Loss=> 23.9780052174
Gradients: w=>0.0635746488, b=>-1.0968342664
Iteration 3472: Achieved Loss=> 23.9767984511
Gradients: w=>0.0635408506, b=>-1.0962511556
Iteration 3473: Achieved Loss=> 23.9755929676
Gradients: w=>0.0635070703, b=>-1.0956683549
Iteration 3474: Achieved Loss=> 23.9743887655
Gradients: w=>0.0634733080, b=>-1.0950858639
Iteration 3475: Achieved Loss=> 23.9731858435
Gradients: w=>0.0634395636, b=>-1.0945036827
Iteration 3476: Achieved Loss=> 23.9719842001
Gradients: w=>0.0634058372, b=>-1.0939218109
Iteration 3477: Achieved Loss=> 23.9707838340
Gradients: w=>0.0633721287, b=>-1.0933402485
Iteration 3478: Achieved Loss=> 23.9695847439
Gradients: w=>0.0633384382, b=>-1.0927589953
Iteration 3479: Achieved Loss=> 23.9683869284
Gradients: w=>0.0633047655, b=>-1.0921780510
Iteration 3480: Achieved Loss=> 23.9671903861
Gradients: w=>0.0632711107, b=>-1.0915974157
Iteration 3481: Achieved Loss=> 23.9659951158
Gradients: w=>0.0632374739, b=>-1.0910170890
Iteration 3482: Achieved Loss=> 23.9648011160
Gradients: w=>0.0632038549, b=>-1.0904370708
Iteration 3483: Achieved Loss=> 23.9636083854
Gradients: w=>0.0631702538, b=>-1.0898573610
Iteration 3484: Achieved Loss=> 23.9624169226
Gradients: w=>0.0631366705, b=>-1.0892779594
Iteration 3485: Achieved Loss=> 23.9612267264
Gradients: w=>0.0631031051, b=>-1.0886988658
Iteration 3486: Achieved Loss=> 23.9600377953
Gradients: w=>0.0630695576, b=>-1.0881200800
Iteration 3487: Achieved Loss=> 23.9588501280
Gradients: w=>0.0630360279, b=>-1.0875416020
Iteration 3488: Achieved Loss=> 23.9576637232
Gradients: w=>0.0630025160, b=>-1.0869634315
Iteration 3489: Achieved Loss=> 23.9564785795
Gradients: w=>0.0629690219, b=>-1.0863855684
Iteration 3490: Achieved Loss=> 23.9552946955
Gradients: w=>0.0629355457, b=>-1.0858080125
Iteration 3491: Achieved Loss=> 23.9541120701
Gradients: w=>0.0629020872, b=>-1.0852307636
Iteration 3492: Achieved Loss=> 23.9529307017
Gradients: w=>0.0628686465, b=>-1.0846538216
Iteration 3493: Achieved Loss=> 23.9517505891
Gradients: w=>0.0628352236, b=>-1.0840771863
Iteration 3494: Achieved Loss=> 23.9505717309
Gradients: w=>0.0628018185, b=>-1.0835008576
Iteration 3495: Achieved Loss=> 23.9493941258
Gradients: w=>0.0627684311, b=>-1.0829248353
Iteration 3496: Achieved Loss=> 23.9482177725
Gradients: w=>0.0627350615, b=>-1.0823491192
Iteration 3497: Achieved Loss=> 23.9470426697
Gradients: w=>0.0627017096, b=>-1.0817737092
Iteration 3498: Achieved Loss=> 23.9458688159
Gradients: w=>0.0626683754, b=>-1.0811986051
Iteration 3499: Achieved Loss=> 23.9446962100
Gradients: w=>0.0626350590, b=>-1.0806238067
Iteration 3500: Achieved Loss=> 23.9435248505
Gradients: w=>0.0626017603, b=>-1.0800493140
Iteration 3501: Achieved Loss=> 23.9423547361
Gradients: w=>0.0625684793, b=>-1.0794751266
Iteration 3502: Achieved Loss=> 23.9411858655
Gradients: w=>0.0625352160, b=>-1.0789012445
Iteration 3503: Achieved Loss=> 23.9400182374
Gradients: w=>0.0625019703, b=>-1.0783276675
Iteration 3504: Achieved Loss=> 23.9388518505
Gradients: w=>0.0624687423, b=>-1.0777543954
Iteration 3505: Achieved Loss=> 23.9376867034
Gradients: w=>0.0624355320, b=>-1.0771814281
Iteration 3506: Achieved Loss=> 23.9365227948
Gradients: w=>0.0624023394, b=>-1.0766087654
Iteration 3507: Achieved Loss=> 23.9353601235
Gradients: w=>0.0623691644, b=>-1.0760364071
Iteration 3508: Achieved Loss=> 23.9341986880
Gradients: w=>0.0623360070, b=>-1.0754643531
Iteration 3509: Achieved Loss=> 23.9330384872
Gradients: w=>0.0623028673, b=>-1.0748926032
Iteration 3510: Achieved Loss=> 23.9318795196
Gradients: w=>0.0622697452, b=>-1.0743211573
Iteration 3511: Achieved Loss=> 23.9307217839
Gradients: w=>0.0622366407, b=>-1.0737500152
Iteration 3512: Achieved Loss=> 23.9295652789
Gradients: w=>0.0622035538, b=>-1.0731791768
Iteration 3513: Achieved Loss=> 23.9284100033
Gradients: w=>0.0621704845, b=>-1.0726086418
Iteration 3514: Achieved Loss=> 23.9272559556
Gradients: w=>0.0621374327, b=>-1.0720384101
Iteration 3515: Achieved Loss=> 23.9261031347
Gradients: w=>0.0621043985, b=>-1.0714684816
Iteration 3516: Achieved Loss=> 23.9249515393
Gradients: w=>0.0620713819, b=>-1.0708988560
Iteration 3517: Achieved Loss=> 23.9238011679
Gradients: w=>0.0620383829, b=>-1.0703295333
Iteration 3518: Achieved Loss=> 23.9226520194
Gradients: w=>0.0620054014, b=>-1.0697605133
Iteration 3519: Achieved Loss=> 23.9215040924
Gradients: w=>0.0619724374, b=>-1.0691917958
Iteration 3520: Achieved Loss=> 23.9203573856
Gradients: w=>0.0619394910, b=>-1.0686233806
Iteration 3521: Achieved Loss=> 23.9192118977
Gradients: w=>0.0619065620, b=>-1.0680552676
Iteration 3522: Achieved Loss=> 23.9180676275
Gradients: w=>0.0618736506, b=>-1.0674874566
Iteration 3523: Achieved Loss=> 23.9169245736
Gradients: w=>0.0618407567, b=>-1.0669199476
Iteration 3524: Achieved Loss=> 23.9157827348
Gradients: w=>0.0618078802, b=>-1.0663527402
Iteration 3525: Achieved Loss=> 23.9146421097
Gradients: w=>0.0617750213, b=>-1.0657858343
Iteration 3526: Achieved Loss=> 23.9135026970
Gradients: w=>0.0617421798, b=>-1.0652192298
Iteration 3527: Achieved Loss=> 23.9123644955
Gradients: w=>0.0617093557, b=>-1.0646529266
Iteration 3528: Achieved Loss=> 23.9112275039
Gradients: w=>0.0616765491, b=>-1.0640869244
Iteration 3529: Achieved Loss=> 23.9100917210
Gradients: w=>0.0616437600, b=>-1.0635212232
Iteration 3530: Achieved Loss=> 23.9089571453
Gradients: w=>0.0616109883, b=>-1.0629558226
Iteration 3531: Achieved Loss=> 23.9078237756
Gradients: w=>0.0615782340, b=>-1.0623907227
Iteration 3532: Achieved Loss=> 23.9066916107
Gradients: w=>0.0615454971, b=>-1.0618259232
Iteration 3533: Achieved Loss=> 23.9055606493
Gradients: w=>0.0615127776, b=>-1.0612614239
Iteration 3534: Achieved Loss=> 23.9044308901
Gradients: w=>0.0614800755, b=>-1.0606972248
Iteration 3535: Achieved Loss=> 23.9033023317
Gradients: w=>0.0614473908, b=>-1.0601333256
Iteration 3536: Achieved Loss=> 23.9021749730
Gradients: w=>0.0614147235, b=>-1.0595697262
Iteration 3537: Achieved Loss=> 23.9010488127
Gradients: w=>0.0613820736, b=>-1.0590064264
Iteration 3538: Achieved Loss=> 23.8999238494
Gradients: w=>0.0613494410, b=>-1.0584434260
Iteration 3539: Achieved Loss=> 23.8988000820
Gradients: w=>0.0613168257, b=>-1.0578807250
Iteration 3540: Achieved Loss=> 23.8976775091
Gradients: w=>0.0612842278, b=>-1.0573183232
Iteration 3541: Achieved Loss=> 23.8965561294
Gradients: w=>0.0612516472, b=>-1.0567562203
Iteration 3542: Achieved Loss=> 23.8954359418
Gradients: w=>0.0612190840, b=>-1.0561944162
Iteration 3543: Achieved Loss=> 23.8943169449
Gradients: w=>0.0611865380, b=>-1.0556329109
Iteration 3544: Achieved Loss=> 23.8931991375
Gradients: w=>0.0611540094, b=>-1.0550717040
Iteration 3545: Achieved Loss=> 23.8920825183
Gradients: w=>0.0611214981, b=>-1.0545107955
Iteration 3546: Achieved Loss=> 23.8909670860
Gradients: w=>0.0610890040, b=>-1.0539501852
Iteration 3547: Achieved Loss=> 23.8898528394
Gradients: w=>0.0610565272, b=>-1.0533898729
Iteration 3548: Achieved Loss=> 23.8887397772
Gradients: w=>0.0610240677, b=>-1.0528298585
Iteration 3549: Achieved Loss=> 23.8876278982
Gradients: w=>0.0609916254, b=>-1.0522701419
Iteration 3550: Achieved Loss=> 23.8865172011
Gradients: w=>0.0609592004, b=>-1.0517107228
Iteration 3551: Achieved Loss=> 23.8854076846
Gradients: w=>0.0609267926, b=>-1.0511516011
Iteration 3552: Achieved Loss=> 23.8842993476
Gradients: w=>0.0608944021, b=>-1.0505927766
Iteration 3553: Achieved Loss=> 23.8831921886
Gradients: w=>0.0608620287, b=>-1.0500342492
Iteration 3554: Achieved Loss=> 23.8820862066
Gradients: w=>0.0608296726, b=>-1.0494760188
Iteration 3555: Achieved Loss=> 23.8809814002
Gradients: w=>0.0607973337, b=>-1.0489180851
Iteration 3556: Achieved Loss=> 23.8798777681
Gradients: w=>0.0607650120, b=>-1.0483604481
Iteration 3557: Achieved Loss=> 23.8787753092
Gradients: w=>0.0607327074, b=>-1.0478031075
Iteration 3558: Achieved Loss=> 23.8776740223
Gradients: w=>0.0607004200, b=>-1.0472460632
Iteration 3559: Achieved Loss=> 23.8765739059
Gradients: w=>0.0606681498, b=>-1.0466893151
Iteration 3560: Achieved Loss=> 23.8754749589
Gradients: w=>0.0606358968, b=>-1.0461328629
Iteration 3561: Achieved Loss=> 23.8743771802
Gradients: w=>0.0606036609, b=>-1.0455767065
Iteration 3562: Achieved Loss=> 23.8732805683
Gradients: w=>0.0605714421, b=>-1.0450208459
Iteration 3563: Achieved Loss=> 23.8721851221
Gradients: w=>0.0605392405, b=>-1.0444652807
Iteration 3564: Achieved Loss=> 23.8710908403
Gradients: w=>0.0605070560, b=>-1.0439100109
Iteration 3565: Achieved Loss=> 23.8699977217
Gradients: w=>0.0604748885, b=>-1.0433550363
Iteration 3566: Achieved Loss=> 23.8689057651
Gradients: w=>0.0604427382, b=>-1.0428003567
Iteration 3567: Achieved Loss=> 23.8678149692
Gradients: w=>0.0604106050, b=>-1.0422459721
Iteration 3568: Achieved Loss=> 23.8667253329
Gradients: w=>0.0603784889, b=>-1.0416918821
Iteration 3569: Achieved Loss=> 23.8656368547
Gradients: w=>0.0603463898, b=>-1.0411380867
Iteration 3570: Achieved Loss=> 23.8645495336
Gradients: w=>0.0603143078, b=>-1.0405845858
Iteration 3571: Achieved Loss=> 23.8634633683
Gradients: w=>0.0602822429, b=>-1.0400313791
Iteration 3572: Achieved Loss=> 23.8623783576
Gradients: w=>0.0602501950, b=>-1.0394784665
Iteration 3573: Achieved Loss=> 23.8612945002
Gradients: w=>0.0602181642, b=>-1.0389258478
Iteration 3574: Achieved Loss=> 23.8602117949
Gradients: w=>0.0601861503, b=>-1.0383735229
Iteration 3575: Achieved Loss=> 23.8591302406
Gradients: w=>0.0601541535, b=>-1.0378214917
Iteration 3576: Achieved Loss=> 23.8580498359
Gradients: w=>0.0601221737, b=>-1.0372697539
Iteration 3577: Achieved Loss=> 23.8569705796
Gradients: w=>0.0600902109, b=>-1.0367183095
Iteration 3578: Achieved Loss=> 23.8558924706
Gradients: w=>0.0600582651, b=>-1.0361671582
Iteration 3579: Achieved Loss=> 23.8548155075
Gradients: w=>0.0600263363, b=>-1.0356163000
Iteration 3580: Achieved Loss=> 23.8537396893
Gradients: w=>0.0599944245, b=>-1.0350657345
Iteration 3581: Achieved Loss=> 23.8526650146
Gradients: w=>0.0599625296, b=>-1.0345154618
Iteration 3582: Achieved Loss=> 23.8515914823
Gradients: w=>0.0599306517, b=>-1.0339654817
Iteration 3583: Achieved Loss=> 23.8505190912
Gradients: w=>0.0598987907, b=>-1.0334157939
Iteration 3584: Achieved Loss=> 23.8494478399
Gradients: w=>0.0598669467, b=>-1.0328663983
Iteration 3585: Achieved Loss=> 23.8483777274
Gradients: w=>0.0598351196, b=>-1.0323172948
Iteration 3586: Achieved Loss=> 23.8473087524
Gradients: w=>0.0598033094, b=>-1.0317684833
Iteration 3587: Achieved Loss=> 23.8462409137
Gradients: w=>0.0597715161, b=>-1.0312199635
Iteration 3588: Achieved Loss=> 23.8451742101
Gradients: w=>0.0597397397, b=>-1.0306717353
Iteration 3589: Achieved Loss=> 23.8441086403
Gradients: w=>0.0597079803, b=>-1.0301237986
Iteration 3590: Achieved Loss=> 23.8430442033
Gradients: w=>0.0596762377, b=>-1.0295761531
Iteration 3591: Achieved Loss=> 23.8419808977
Gradients: w=>0.0596445119, b=>-1.0290287988
Iteration 3592: Achieved Loss=> 23.8409187223
Gradients: w=>0.0596128031, b=>-1.0284817355
Iteration 3593: Achieved Loss=> 23.8398576761
Gradients: w=>0.0595811111, b=>-1.0279349631
Iteration 3594: Achieved Loss=> 23.8387977577
Gradients: w=>0.0595494359, b=>-1.0273884813
Iteration 3595: Achieved Loss=> 23.8377389660
Gradients: w=>0.0595177776, b=>-1.0268422901
Iteration 3596: Achieved Loss=> 23.8366812998
Gradients: w=>0.0594861362, b=>-1.0262963892
Iteration 3597: Achieved Loss=> 23.8356247578
Gradients: w=>0.0594545115, b=>-1.0257507785
Iteration 3598: Achieved Loss=> 23.8345693389
Gradients: w=>0.0594229037, b=>-1.0252054579
Iteration 3599: Achieved Loss=> 23.8335150419
Gradients: w=>0.0593913126, b=>-1.0246604272
Iteration 3600: Achieved Loss=> 23.8324618656
Gradients: w=>0.0593597384, b=>-1.0241156863
Iteration 3601: Achieved Loss=> 23.8314098089
Gradients: w=>0.0593281809, b=>-1.0235712350
Iteration 3602: Achieved Loss=> 23.8303588704
Gradients: w=>0.0592966402, b=>-1.0230270731
Iteration 3603: Achieved Loss=> 23.8293090490
Gradients: w=>0.0592651163, b=>-1.0224832005
Iteration 3604: Achieved Loss=> 23.8282603436
Gradients: w=>0.0592336092, b=>-1.0219396170
Iteration 3605: Achieved Loss=> 23.8272127530
Gradients: w=>0.0592021188, b=>-1.0213963225
Iteration 3606: Achieved Loss=> 23.8261662759
Gradients: w=>0.0591706451, b=>-1.0208533169
Iteration 3607: Achieved Loss=> 23.8251209111
Gradients: w=>0.0591391882, b=>-1.0203106000
Iteration 3608: Achieved Loss=> 23.8240766576
Gradients: w=>0.0591077480, b=>-1.0197681715
Iteration 3609: Achieved Loss=> 23.8230335141
Gradients: w=>0.0590763245, b=>-1.0192260315
Iteration 3610: Achieved Loss=> 23.8219914795
Gradients: w=>0.0590449177, b=>-1.0186841796
Iteration 3611: Achieved Loss=> 23.8209505525
Gradients: w=>0.0590135276, b=>-1.0181426158
Iteration 3612: Achieved Loss=> 23.8199107320
Gradients: w=>0.0589821542, b=>-1.0176013400
Iteration 3613: Achieved Loss=> 23.8188720168
Gradients: w=>0.0589507975, b=>-1.0170603519
Iteration 3614: Achieved Loss=> 23.8178344057
Gradients: w=>0.0589194574, b=>-1.0165196514
Iteration 3615: Achieved Loss=> 23.8167978976
Gradients: w=>0.0588881340, b=>-1.0159792383
Iteration 3616: Achieved Loss=> 23.8157624913
Gradients: w=>0.0588568273, b=>-1.0154391126
Iteration 3617: Achieved Loss=> 23.8147281856
Gradients: w=>0.0588255372, b=>-1.0148992740
Iteration 3618: Achieved Loss=> 23.8136949793
Gradients: w=>0.0587942637, b=>-1.0143597224
Iteration 3619: Achieved Loss=> 23.8126628713
Gradients: w=>0.0587630069, b=>-1.0138204576
Iteration 3620: Achieved Loss=> 23.8116318604
Gradients: w=>0.0587317667, b=>-1.0132814795
Iteration 3621: Achieved Loss=> 23.8106019455
Gradients: w=>0.0587005431, b=>-1.0127427880
Iteration 3622: Achieved Loss=> 23.8095731253
Gradients: w=>0.0586693360, b=>-1.0122043828
Iteration 3623: Achieved Loss=> 23.8085453988
Gradients: w=>0.0586381456, b=>-1.0116662639
Iteration 3624: Achieved Loss=> 23.8075187647
Gradients: w=>0.0586069718, b=>-1.0111284311
Iteration 3625: Achieved Loss=> 23.8064932219
Gradients: w=>0.0585758145, b=>-1.0105908842
Iteration 3626: Achieved Loss=> 23.8054687692
Gradients: w=>0.0585446738, b=>-1.0100536230
Iteration 3627: Achieved Loss=> 23.8044454055
Gradients: w=>0.0585135497, b=>-1.0095166475
Iteration 3628: Achieved Loss=> 23.8034231296
Gradients: w=>0.0584824421, b=>-1.0089799575
Iteration 3629: Achieved Loss=> 23.8024019404
Gradients: w=>0.0584513510, b=>-1.0084435528
Iteration 3630: Achieved Loss=> 23.8013818367
Gradients: w=>0.0584202765, b=>-1.0079074332
Iteration 3631: Achieved Loss=> 23.8003628173
Gradients: w=>0.0583892185, b=>-1.0073715987
Iteration 3632: Achieved Loss=> 23.7993448811
Gradients: w=>0.0583581770, b=>-1.0068360490
Iteration 3633: Achieved Loss=> 23.7983280270
Gradients: w=>0.0583271520, b=>-1.0063007841
Iteration 3634: Achieved Loss=> 23.7973122537
Gradients: w=>0.0582961435, b=>-1.0057658037
Iteration 3635: Achieved Loss=> 23.7962975602
Gradients: w=>0.0582651514, b=>-1.0052311077
Iteration 3636: Achieved Loss=> 23.7952839453
Gradients: w=>0.0582341759, b=>-1.0046966960
Iteration 3637: Achieved Loss=> 23.7942714079
Gradients: w=>0.0582032168, b=>-1.0041625684
Iteration 3638: Achieved Loss=> 23.7932599467
Gradients: w=>0.0581722742, b=>-1.0036287248
Iteration 3639: Achieved Loss=> 23.7922495607
Gradients: w=>0.0581413480, b=>-1.0030951649
Iteration 3640: Achieved Loss=> 23.7912402488
Gradients: w=>0.0581104383, b=>-1.0025618887
Iteration 3641: Achieved Loss=> 23.7902320097
Gradients: w=>0.0580795450, b=>-1.0020288961
Iteration 3642: Achieved Loss=> 23.7892248423
Gradients: w=>0.0580486682, b=>-1.0014961868
Iteration 3643: Achieved Loss=> 23.7882187456
Gradients: w=>0.0580178077, b=>-1.0009637606
Iteration 3644: Achieved Loss=> 23.7872137183
Gradients: w=>0.0579869637, b=>-1.0004316176
Iteration 3645: Achieved Loss=> 23.7862097593
Gradients: w=>0.0579561360, b=>-0.9998997574
Iteration 3646: Achieved Loss=> 23.7852068675
Gradients: w=>0.0579253247, b=>-0.9993681800
Iteration 3647: Achieved Loss=> 23.7842050418
Gradients: w=>0.0578945299, b=>-0.9988368852
Iteration 3648: Achieved Loss=> 23.7832042810
Gradients: w=>0.0578637514, b=>-0.9983058729
Iteration 3649: Achieved Loss=> 23.7822045840
Gradients: w=>0.0578329892, b=>-0.9977751428
Iteration 3650: Achieved Loss=> 23.7812059496
Gradients: w=>0.0578022434, b=>-0.9972446949
Iteration 3651: Achieved Loss=> 23.7802083768
Gradients: w=>0.0577715140, b=>-0.9967145291
Iteration 3652: Achieved Loss=> 23.7792118643
Gradients: w=>0.0577408009, b=>-0.9961846450
Iteration 3653: Achieved Loss=> 23.7782164112
Gradients: w=>0.0577101041, b=>-0.9956550427
Iteration 3654: Achieved Loss=> 23.7772220161
Gradients: w=>0.0576794236, b=>-0.9951257219
Iteration 3655: Achieved Loss=> 23.7762286781
Gradients: w=>0.0576487595, b=>-0.9945966825
Iteration 3656: Achieved Loss=> 23.7752363960
Gradients: w=>0.0576181116, b=>-0.9940679244
Iteration 3657: Achieved Loss=> 23.7742451687
Gradients: w=>0.0575874801, b=>-0.9935394474
Iteration 3658: Achieved Loss=> 23.7732549950
Gradients: w=>0.0575568648, b=>-0.9930112513
Iteration 3659: Achieved Loss=> 23.7722658739
Gradients: w=>0.0575262658, b=>-0.9924833360
Iteration 3660: Achieved Loss=> 23.7712778041
Gradients: w=>0.0574956831, b=>-0.9919557014
Iteration 3661: Achieved Loss=> 23.7702907847
Gradients: w=>0.0574651166, b=>-0.9914283473
Iteration 3662: Achieved Loss=> 23.7693048145
Gradients: w=>0.0574345664, b=>-0.9909012736
Iteration 3663: Achieved Loss=> 23.7683198923
Gradients: w=>0.0574040324, b=>-0.9903744801
Iteration 3664: Achieved Loss=> 23.7673360170
Gradients: w=>0.0573735146, b=>-0.9898479666
Iteration 3665: Achieved Loss=> 23.7663531876
Gradients: w=>0.0573430131, b=>-0.9893217330
Iteration 3666: Achieved Loss=> 23.7653714030
Gradients: w=>0.0573125278, b=>-0.9887957792
Iteration 3667: Achieved Loss=> 23.7643906619
Gradients: w=>0.0572820587, b=>-0.9882701050
Iteration 3668: Achieved Loss=> 23.7634109634
Gradients: w=>0.0572516058, b=>-0.9877447103
Iteration 3669: Achieved Loss=> 23.7624323062
Gradients: w=>0.0572211691, b=>-0.9872195949
Iteration 3670: Achieved Loss=> 23.7614546894
Gradients: w=>0.0571907486, b=>-0.9866947587
Iteration 3671: Achieved Loss=> 23.7604781117
Gradients: w=>0.0571603442, b=>-0.9861702015
Iteration 3672: Achieved Loss=> 23.7595025721
Gradients: w=>0.0571299560, b=>-0.9856459231
Iteration 3673: Achieved Loss=> 23.7585280695
Gradients: w=>0.0570995840, b=>-0.9851219235
Iteration 3674: Achieved Loss=> 23.7575546028
Gradients: w=>0.0570692281, b=>-0.9845982024
Iteration 3675: Achieved Loss=> 23.7565821708
Gradients: w=>0.0570388883, b=>-0.9840747598
Iteration 3676: Achieved Loss=> 23.7556107725
Gradients: w=>0.0570085647, b=>-0.9835515955
Iteration 3677: Achieved Loss=> 23.7546404068
Gradients: w=>0.0569782572, b=>-0.9830287092
Iteration 3678: Achieved Loss=> 23.7536710726
Gradients: w=>0.0569479658, b=>-0.9825061010
Iteration 3679: Achieved Loss=> 23.7527027687
Gradients: w=>0.0569176905, b=>-0.9819837706
Iteration 3680: Achieved Loss=> 23.7517354942
Gradients: w=>0.0568874313, b=>-0.9814617179
Iteration 3681: Achieved Loss=> 23.7507692478
Gradients: w=>0.0568571882, b=>-0.9809399427
Iteration 3682: Achieved Loss=> 23.7498040285
Gradients: w=>0.0568269612, b=>-0.9804184449
Iteration 3683: Achieved Loss=> 23.7488398352
Gradients: w=>0.0567967503, b=>-0.9798972244
Iteration 3684: Achieved Loss=> 23.7478766669
Gradients: w=>0.0567665554, b=>-0.9793762809
Iteration 3685: Achieved Loss=> 23.7469145224
Gradients: w=>0.0567363765, b=>-0.9788556144
Iteration 3686: Achieved Loss=> 23.7459534006
Gradients: w=>0.0567062137, b=>-0.9783352248
Iteration 3687: Achieved Loss=> 23.7449933005
Gradients: w=>0.0566760670, b=>-0.9778151117
Iteration 3688: Achieved Loss=> 23.7440342209
Gradients: w=>0.0566459362, b=>-0.9772952752
Iteration 3689: Achieved Loss=> 23.7430761608
Gradients: w=>0.0566158215, b=>-0.9767757150
Iteration 3690: Achieved Loss=> 23.7421191191
Gradients: w=>0.0565857228, b=>-0.9762564311
Iteration 3691: Achieved Loss=> 23.7411630948
Gradients: w=>0.0565556401, b=>-0.9757374232
Iteration 3692: Achieved Loss=> 23.7402080866
Gradients: w=>0.0565255734, b=>-0.9752186912
Iteration 3693: Achieved Loss=> 23.7392540936
Gradients: w=>0.0564955226, b=>-0.9747002350
Iteration 3694: Achieved Loss=> 23.7383011147
Gradients: w=>0.0564654879, b=>-0.9741820545
Iteration 3695: Achieved Loss=> 23.7373491488
Gradients: w=>0.0564354691, b=>-0.9736641494
Iteration 3696: Achieved Loss=> 23.7363981948
Gradients: w=>0.0564054663, b=>-0.9731465196
Iteration 3697: Achieved Loss=> 23.7354482517
Gradients: w=>0.0563754794, b=>-0.9726291651
Iteration 3698: Achieved Loss=> 23.7344993183
Gradients: w=>0.0563455085, b=>-0.9721120856
Iteration 3699: Achieved Loss=> 23.7335513936
Gradients: w=>0.0563155535, b=>-0.9715952809
Iteration 3700: Achieved Loss=> 23.7326044766
Gradients: w=>0.0562856144, b=>-0.9710787511
Iteration 3701: Achieved Loss=> 23.7316585661
Gradients: w=>0.0562556912, b=>-0.9705624958
Iteration 3702: Achieved Loss=> 23.7307136610
Gradients: w=>0.0562257840, b=>-0.9700465150
Iteration 3703: Achieved Loss=> 23.7297697604
Gradients: w=>0.0561958926, b=>-0.9695308085
Iteration 3704: Achieved Loss=> 23.7288268632
Gradients: w=>0.0561660172, b=>-0.9690153761
Iteration 3705: Achieved Loss=> 23.7278849682
Gradients: w=>0.0561361576, b=>-0.9685002178
Iteration 3706: Achieved Loss=> 23.7269440744
Gradients: w=>0.0561063139, b=>-0.9679853334
Iteration 3707: Achieved Loss=> 23.7260041808
Gradients: w=>0.0560764860, b=>-0.9674707227
Iteration 3708: Achieved Loss=> 23.7250652863
Gradients: w=>0.0560466741, b=>-0.9669563855
Iteration 3709: Achieved Loss=> 23.7241273897
Gradients: w=>0.0560168779, b=>-0.9664423218
Iteration 3710: Achieved Loss=> 23.7231904902
Gradients: w=>0.0559870976, b=>-0.9659285314
Iteration 3711: Achieved Loss=> 23.7222545866
Gradients: w=>0.0559573332, b=>-0.9654150142
Iteration 3712: Achieved Loss=> 23.7213196778
Gradients: w=>0.0559275845, b=>-0.9649017699
Iteration 3713: Achieved Loss=> 23.7203857628
Gradients: w=>0.0558978517, b=>-0.9643887985
Iteration 3714: Achieved Loss=> 23.7194528405
Gradients: w=>0.0558681347, b=>-0.9638760998
Iteration 3715: Achieved Loss=> 23.7185209099
Gradients: w=>0.0558384335, b=>-0.9633636737
Iteration 3716: Achieved Loss=> 23.7175899699
Gradients: w=>0.0558087481, b=>-0.9628515200
Iteration 3717: Achieved Loss=> 23.7166600195
Gradients: w=>0.0557790784, b=>-0.9623396386
Iteration 3718: Achieved Loss=> 23.7157310576
Gradients: w=>0.0557494245, b=>-0.9618280293
Iteration 3719: Achieved Loss=> 23.7148030832
Gradients: w=>0.0557197864, b=>-0.9613166920
Iteration 3720: Achieved Loss=> 23.7138760952
Gradients: w=>0.0556901641, b=>-0.9608056265
Iteration 3721: Achieved Loss=> 23.7129500926
Gradients: w=>0.0556605575, b=>-0.9602948328
Iteration 3722: Achieved Loss=> 23.7120250743
Gradients: w=>0.0556309666, b=>-0.9597843106
Iteration 3723: Achieved Loss=> 23.7111010392
Gradients: w=>0.0556013915, b=>-0.9592740598
Iteration 3724: Achieved Loss=> 23.7101779864
Gradients: w=>0.0555718321, b=>-0.9587640802
Iteration 3725: Achieved Loss=> 23.7092559148
Gradients: w=>0.0555422884, b=>-0.9582543718
Iteration 3726: Achieved Loss=> 23.7083348233
Gradients: w=>0.0555127604, b=>-0.9577449344
Iteration 3727: Achieved Loss=> 23.7074147109
Gradients: w=>0.0554832481, b=>-0.9572357678
Iteration 3728: Achieved Loss=> 23.7064955766
Gradients: w=>0.0554537515, b=>-0.9567268719
Iteration 3729: Achieved Loss=> 23.7055774193
Gradients: w=>0.0554242706, b=>-0.9562182465
Iteration 3730: Achieved Loss=> 23.7046602380
Gradients: w=>0.0553948054, b=>-0.9557098915
Iteration 3731: Achieved Loss=> 23.7037440316
Gradients: w=>0.0553653558, b=>-0.9552018068
Iteration 3732: Achieved Loss=> 23.7028287991
Gradients: w=>0.0553359219, b=>-0.9546939922
Iteration 3733: Achieved Loss=> 23.7019145395
Gradients: w=>0.0553065036, b=>-0.9541864476
Iteration 3734: Achieved Loss=> 23.7010012518
Gradients: w=>0.0552771010, b=>-0.9536791728
Iteration 3735: Achieved Loss=> 23.7000889348
Gradients: w=>0.0552477140, b=>-0.9531721677
Iteration 3736: Achieved Loss=> 23.6991775877
Gradients: w=>0.0552183426, b=>-0.9526654321
Iteration 3737: Achieved Loss=> 23.6982672092
Gradients: w=>0.0551889868, b=>-0.9521589659
Iteration 3738: Achieved Loss=> 23.6973577985
Gradients: w=>0.0551596467, b=>-0.9516527690
Iteration 3739: Achieved Loss=> 23.6964493545
Gradients: w=>0.0551303221, b=>-0.9511468411
Iteration 3740: Achieved Loss=> 23.6955418761
Gradients: w=>0.0551010131, b=>-0.9506411823
Iteration 3741: Achieved Loss=> 23.6946353623
Gradients: w=>0.0550717197, b=>-0.9501357922
Iteration 3742: Achieved Loss=> 23.6937298122
Gradients: w=>0.0550424419, b=>-0.9496306709
Iteration 3743: Achieved Loss=> 23.6928252246
Gradients: w=>0.0550131797, b=>-0.9491258181
Iteration 3744: Achieved Loss=> 23.6919215986
Gradients: w=>0.0549839330, b=>-0.9486212337
Iteration 3745: Achieved Loss=> 23.6910189331
Gradients: w=>0.0549547018, b=>-0.9481169175
Iteration 3746: Achieved Loss=> 23.6901172272
Gradients: w=>0.0549254862, b=>-0.9476128694
Iteration 3747: Achieved Loss=> 23.6892164797
Gradients: w=>0.0548962861, b=>-0.9471090893
Iteration 3748: Achieved Loss=> 23.6883166897
Gradients: w=>0.0548671016, b=>-0.9466055771
Iteration 3749: Achieved Loss=> 23.6874178562
Gradients: w=>0.0548379326, b=>-0.9461023325
Iteration 3750: Achieved Loss=> 23.6865199781
Gradients: w=>0.0548087790, b=>-0.9455993555
Iteration 3751: Achieved Loss=> 23.6856230545
Gradients: w=>0.0547796410, b=>-0.9450966458
Iteration 3752: Achieved Loss=> 23.6847270842
Gradients: w=>0.0547505185, b=>-0.9445942034
Iteration 3753: Achieved Loss=> 23.6838320663
Gradients: w=>0.0547214114, b=>-0.9440920282
Iteration 3754: Achieved Loss=> 23.6829379999
Gradients: w=>0.0546923198, b=>-0.9435901199
Iteration 3755: Achieved Loss=> 23.6820448838
Gradients: w=>0.0546632437, b=>-0.9430884784
Iteration 3756: Achieved Loss=> 23.6811527170
Gradients: w=>0.0546341830, b=>-0.9425871036
Iteration 3757: Achieved Loss=> 23.6802614987
Gradients: w=>0.0546051378, b=>-0.9420859954
Iteration 3758: Achieved Loss=> 23.6793712276
Gradients: w=>0.0545761081, b=>-0.9415851535
Iteration 3759: Achieved Loss=> 23.6784819029
Gradients: w=>0.0545470937, b=>-0.9410845780
Iteration 3760: Achieved Loss=> 23.6775935236
Gradients: w=>0.0545180948, b=>-0.9405842685
Iteration 3761: Achieved Loss=> 23.6767060885
Gradients: w=>0.0544891113, b=>-0.9400842251
Iteration 3762: Achieved Loss=> 23.6758195968
Gradients: w=>0.0544601432, b=>-0.9395844474
Iteration 3763: Achieved Loss=> 23.6749340475
Gradients: w=>0.0544311906, b=>-0.9390849355
Iteration 3764: Achieved Loss=> 23.6740494394
Gradients: w=>0.0544022533, b=>-0.9385856891
Iteration 3765: Achieved Loss=> 23.6731657716
Gradients: w=>0.0543733314, b=>-0.9380867082
Iteration 3766: Achieved Loss=> 23.6722830432
Gradients: w=>0.0543444248, b=>-0.9375879925
Iteration 3767: Achieved Loss=> 23.6714012531
Gradients: w=>0.0543155337, b=>-0.9370895419
Iteration 3768: Achieved Loss=> 23.6705204003
Gradients: w=>0.0542866579, b=>-0.9365913563
Iteration 3769: Achieved Loss=> 23.6696404839
Gradients: w=>0.0542577974, b=>-0.9360934356
Iteration 3770: Achieved Loss=> 23.6687615028
Gradients: w=>0.0542289523, b=>-0.9355957796
Iteration 3771: Achieved Loss=> 23.6678834560
Gradients: w=>0.0542001225, b=>-0.9350983882
Iteration 3772: Achieved Loss=> 23.6670063425
Gradients: w=>0.0541713081, b=>-0.9346012612
Iteration 3773: Achieved Loss=> 23.6661301615
Gradients: w=>0.0541425089, b=>-0.9341043985
Iteration 3774: Achieved Loss=> 23.6652549117
Gradients: w=>0.0541137251, b=>-0.9336077999
Iteration 3775: Achieved Loss=> 23.6643805924
Gradients: w=>0.0540849566, b=>-0.9331114653
Iteration 3776: Achieved Loss=> 23.6635072024
Gradients: w=>0.0540562034, b=>-0.9326153946
Iteration 3777: Achieved Loss=> 23.6626347408
Gradients: w=>0.0540274654, b=>-0.9321195877
Iteration 3778: Achieved Loss=> 23.6617632067
Gradients: w=>0.0539987428, b=>-0.9316240443
Iteration 3779: Achieved Loss=> 23.6608925989
Gradients: w=>0.0539700354, b=>-0.9311287643
Iteration 3780: Achieved Loss=> 23.6600229166
Gradients: w=>0.0539413433, b=>-0.9306337477
Iteration 3781: Achieved Loss=> 23.6591541588
Gradients: w=>0.0539126664, b=>-0.9301389942
Iteration 3782: Achieved Loss=> 23.6582863244
Gradients: w=>0.0538840048, b=>-0.9296445038
Iteration 3783: Achieved Loss=> 23.6574194125
Gradients: w=>0.0538553584, b=>-0.9291502763
Iteration 3784: Achieved Loss=> 23.6565534221
Gradients: w=>0.0538267272, b=>-0.9286563114
Iteration 3785: Achieved Loss=> 23.6556883523
Gradients: w=>0.0537981113, b=>-0.9281626092
Iteration 3786: Achieved Loss=> 23.6548242020
Gradients: w=>0.0537695106, b=>-0.9276691695
Iteration 3787: Achieved Loss=> 23.6539609703
Gradients: w=>0.0537409250, b=>-0.9271759921
Iteration 3788: Achieved Loss=> 23.6530986561
Gradients: w=>0.0537123547, b=>-0.9266830769
Iteration 3789: Achieved Loss=> 23.6522372586
Gradients: w=>0.0536837996, b=>-0.9261904237
Iteration 3790: Achieved Loss=> 23.6513767768
Gradients: w=>0.0536552596, b=>-0.9256980325
Iteration 3791: Achieved Loss=> 23.6505172096
Gradients: w=>0.0536267348, b=>-0.9252059030
Iteration 3792: Achieved Loss=> 23.6496585561
Gradients: w=>0.0535982252, b=>-0.9247140351
Iteration 3793: Achieved Loss=> 23.6488008153
Gradients: w=>0.0535697307, b=>-0.9242224288
Iteration 3794: Achieved Loss=> 23.6479439863
Gradients: w=>0.0535412514, b=>-0.9237310837
Iteration 3795: Achieved Loss=> 23.6470880681
Gradients: w=>0.0535127873, b=>-0.9232399999
Iteration 3796: Achieved Loss=> 23.6462330597
Gradients: w=>0.0534843382, b=>-0.9227491772
Iteration 3797: Achieved Loss=> 23.6453789602
Gradients: w=>0.0534559043, b=>-0.9222586154
Iteration 3798: Achieved Loss=> 23.6445257686
Gradients: w=>0.0534274855, b=>-0.9217683145
Iteration 3799: Achieved Loss=> 23.6436734839
Gradients: w=>0.0533990818, b=>-0.9212782741
Iteration 3800: Achieved Loss=> 23.6428221051
Gradients: w=>0.0533706932, b=>-0.9207884943
Iteration 3801: Achieved Loss=> 23.6419716314
Gradients: w=>0.0533423197, b=>-0.9202989749
Iteration 3802: Achieved Loss=> 23.6411220617
Gradients: w=>0.0533139613, b=>-0.9198097157
Iteration 3803: Achieved Loss=> 23.6402733950
Gradients: w=>0.0532856180, b=>-0.9193207166
Iteration 3804: Achieved Loss=> 23.6394256305
Gradients: w=>0.0532572897, b=>-0.9188319775
Iteration 3805: Achieved Loss=> 23.6385787671
Gradients: w=>0.0532289765, b=>-0.9183434982
Iteration 3806: Achieved Loss=> 23.6377328039
Gradients: w=>0.0532006783, b=>-0.9178552786
Iteration 3807: Achieved Loss=> 23.6368877400
Gradients: w=>0.0531723952, b=>-0.9173673186
Iteration 3808: Achieved Loss=> 23.6360435744
Gradients: w=>0.0531441271, b=>-0.9168796180
Iteration 3809: Achieved Loss=> 23.6352003060
Gradients: w=>0.0531158741, b=>-0.9163921766
Iteration 3810: Achieved Loss=> 23.6343579341
Gradients: w=>0.0530876361, b=>-0.9159049944
Iteration 3811: Achieved Loss=> 23.6335164576
Gradients: w=>0.0530594130, b=>-0.9154180712
Iteration 3812: Achieved Loss=> 23.6326758755
Gradients: w=>0.0530312050, b=>-0.9149314069
Iteration 3813: Achieved Loss=> 23.6318361870
Gradients: w=>0.0530030120, b=>-0.9144450012
Iteration 3814: Achieved Loss=> 23.6309973910
Gradients: w=>0.0529748340, b=>-0.9139588542
Iteration 3815: Achieved Loss=> 23.6301594867
Gradients: w=>0.0529466709, b=>-0.9134729656
Iteration 3816: Achieved Loss=> 23.6293224731
Gradients: w=>0.0529185229, b=>-0.9129873354
Iteration 3817: Achieved Loss=> 23.6284863491
Gradients: w=>0.0528903898, b=>-0.9125019633
Iteration 3818: Achieved Loss=> 23.6276511140
Gradients: w=>0.0528622716, b=>-0.9120168492
Iteration 3819: Achieved Loss=> 23.6268167667
Gradients: w=>0.0528341684, b=>-0.9115319931
Iteration 3820: Achieved Loss=> 23.6259833062
Gradients: w=>0.0528060801, b=>-0.9110473947
Iteration 3821: Achieved Loss=> 23.6251507318
Gradients: w=>0.0527780068, b=>-0.9105630539
Iteration 3822: Achieved Loss=> 23.6243190423
Gradients: w=>0.0527499484, b=>-0.9100789707
Iteration 3823: Achieved Loss=> 23.6234882369
Gradients: w=>0.0527219049, b=>-0.9095951447
Iteration 3824: Achieved Loss=> 23.6226583147
Gradients: w=>0.0526938763, b=>-0.9091115760
Iteration 3825: Achieved Loss=> 23.6218292746
Gradients: w=>0.0526658626, b=>-0.9086282644
Iteration 3826: Achieved Loss=> 23.6210011158
Gradients: w=>0.0526378638, b=>-0.9081452097
Iteration 3827: Achieved Loss=> 23.6201738373
Gradients: w=>0.0526098799, b=>-0.9076624119
Iteration 3828: Achieved Loss=> 23.6193474381
Gradients: w=>0.0525819109, b=>-0.9071798707
Iteration 3829: Achieved Loss=> 23.6185219175
Gradients: w=>0.0525539568, b=>-0.9066975860
Iteration 3830: Achieved Loss=> 23.6176972743
Gradients: w=>0.0525260175, b=>-0.9062155577
Iteration 3831: Achieved Loss=> 23.6168735077
Gradients: w=>0.0524980930, b=>-0.9057337857
Iteration 3832: Achieved Loss=> 23.6160506168
Gradients: w=>0.0524701834, b=>-0.9052522699
Iteration 3833: Achieved Loss=> 23.6152286005
Gradients: w=>0.0524422887, b=>-0.9047710100
Iteration 3834: Achieved Loss=> 23.6144074581
Gradients: w=>0.0524144087, b=>-0.9042900059
Iteration 3835: Achieved Loss=> 23.6135871885
Gradients: w=>0.0523865436, b=>-0.9038092576
Iteration 3836: Achieved Loss=> 23.6127677908
Gradients: w=>0.0523586933, b=>-0.9033287649
Iteration 3837: Achieved Loss=> 23.6119492642
Gradients: w=>0.0523308578, b=>-0.9028485276
Iteration 3838: Achieved Loss=> 23.6111316076
Gradients: w=>0.0523030371, b=>-0.9023685456
Iteration 3839: Achieved Loss=> 23.6103148202
Gradients: w=>0.0522752312, b=>-0.9018888187
Iteration 3840: Achieved Loss=> 23.6094989010
Gradients: w=>0.0522474401, b=>-0.9014093470
Iteration 3841: Achieved Loss=> 23.6086838491
Gradients: w=>0.0522196638, b=>-0.9009301301
Iteration 3842: Achieved Loss=> 23.6078696636
Gradients: w=>0.0521919022, b=>-0.9004511680
Iteration 3843: Achieved Loss=> 23.6070563435
Gradients: w=>0.0521641554, b=>-0.8999724605
Iteration 3844: Achieved Loss=> 23.6062438880
Gradients: w=>0.0521364233, b=>-0.8994940075
Iteration 3845: Achieved Loss=> 23.6054322961
Gradients: w=>0.0521087060, b=>-0.8990158089
Iteration 3846: Achieved Loss=> 23.6046215669
Gradients: w=>0.0520810034, b=>-0.8985378645
Iteration 3847: Achieved Loss=> 23.6038116996
Gradients: w=>0.0520533156, b=>-0.8980601742
Iteration 3848: Achieved Loss=> 23.6030026930
Gradients: w=>0.0520256424, b=>-0.8975827378
Iteration 3849: Achieved Loss=> 23.6021945465
Gradients: w=>0.0519979840, b=>-0.8971055553
Iteration 3850: Achieved Loss=> 23.6013872589
Gradients: w=>0.0519703403, b=>-0.8966286264
Iteration 3851: Achieved Loss=> 23.6005808296
Gradients: w=>0.0519427112, b=>-0.8961519511
Iteration 3852: Achieved Loss=> 23.5997752574
Gradients: w=>0.0519150969, b=>-0.8956755293
Iteration 3853: Achieved Loss=> 23.5989705415
Gradients: w=>0.0518874972, b=>-0.8951993607
Iteration 3854: Achieved Loss=> 23.5981666810
Gradients: w=>0.0518599122, b=>-0.8947234452
Iteration 3855: Achieved Loss=> 23.5973636751
Gradients: w=>0.0518323419, b=>-0.8942477828
Iteration 3856: Achieved Loss=> 23.5965615226
Gradients: w=>0.0518047863, b=>-0.8937723732
Iteration 3857: Achieved Loss=> 23.5957602229
Gradients: w=>0.0517772452, b=>-0.8932972164
Iteration 3858: Achieved Loss=> 23.5949597749
Gradients: w=>0.0517497189, b=>-0.8928223121
Iteration 3859: Achieved Loss=> 23.5941601778
Gradients: w=>0.0517222071, b=>-0.8923476604
Iteration 3860: Achieved Loss=> 23.5933614307
Gradients: w=>0.0516947100, b=>-0.8918732610
Iteration 3861: Achieved Loss=> 23.5925635326
Gradients: w=>0.0516672275, b=>-0.8913991138
Iteration 3862: Achieved Loss=> 23.5917664826
Gradients: w=>0.0516397596, b=>-0.8909252187
Iteration 3863: Achieved Loss=> 23.5909702799
Gradients: w=>0.0516123064, b=>-0.8904515755
Iteration 3864: Achieved Loss=> 23.5901749235
Gradients: w=>0.0515848677, b=>-0.8899781841
Iteration 3865: Achieved Loss=> 23.5893804126
Gradients: w=>0.0515574436, b=>-0.8895050443
Iteration 3866: Achieved Loss=> 23.5885867462
Gradients: w=>0.0515300341, b=>-0.8890321562
Iteration 3867: Achieved Loss=> 23.5877939235
Gradients: w=>0.0515026391, b=>-0.8885595194
Iteration 3868: Achieved Loss=> 23.5870019436
Gradients: w=>0.0514752587, b=>-0.8880871339
Iteration 3869: Achieved Loss=> 23.5862108054
Gradients: w=>0.0514478929, b=>-0.8876149995
Iteration 3870: Achieved Loss=> 23.5854205083
Gradients: w=>0.0514205416, b=>-0.8871431161
Iteration 3871: Achieved Loss=> 23.5846310512
Gradients: w=>0.0513932049, b=>-0.8866714836
Iteration 3872: Achieved Loss=> 23.5838424333
Gradients: w=>0.0513658827, b=>-0.8862001018
Iteration 3873: Achieved Loss=> 23.5830546537
Gradients: w=>0.0513385750, b=>-0.8857289706
Iteration 3874: Achieved Loss=> 23.5822677115
Gradients: w=>0.0513112818, b=>-0.8852580899
Iteration 3875: Achieved Loss=> 23.5814816058
Gradients: w=>0.0512840032, b=>-0.8847874596
Iteration 3876: Achieved Loss=> 23.5806963357
Gradients: w=>0.0512567390, b=>-0.8843170794
Iteration 3877: Achieved Loss=> 23.5799119003
Gradients: w=>0.0512294894, b=>-0.8838469493
Iteration 3878: Achieved Loss=> 23.5791282987
Gradients: w=>0.0512022542, b=>-0.8833770691
Iteration 3879: Achieved Loss=> 23.5783455302
Gradients: w=>0.0511750335, b=>-0.8829074388
Iteration 3880: Achieved Loss=> 23.5775635936
Gradients: w=>0.0511478273, b=>-0.8824380581
Iteration 3881: Achieved Loss=> 23.5767824883
Gradients: w=>0.0511206355, b=>-0.8819689269
Iteration 3882: Achieved Loss=> 23.5760022133
Gradients: w=>0.0510934582, b=>-0.8815000451
Iteration 3883: Achieved Loss=> 23.5752227676
Gradients: w=>0.0510662954, b=>-0.8810314127
Iteration 3884: Achieved Loss=> 23.5744441505
Gradients: w=>0.0510391470, b=>-0.8805630293
Iteration 3885: Achieved Loss=> 23.5736663611
Gradients: w=>0.0510120130, b=>-0.8800948950
Iteration 3886: Achieved Loss=> 23.5728893984
Gradients: w=>0.0509848935, b=>-0.8796270095
Iteration 3887: Achieved Loss=> 23.5721132617
Gradients: w=>0.0509577883, b=>-0.8791593728
Iteration 3888: Achieved Loss=> 23.5713379499
Gradients: w=>0.0509306976, b=>-0.8786919847
Iteration 3889: Achieved Loss=> 23.5705634623
Gradients: w=>0.0509036213, b=>-0.8782248451
Iteration 3890: Achieved Loss=> 23.5697897979
Gradients: w=>0.0508765593, b=>-0.8777579538
Iteration 3891: Achieved Loss=> 23.5690169560
Gradients: w=>0.0508495118, b=>-0.8772913107
Iteration 3892: Achieved Loss=> 23.5682449355
Gradients: w=>0.0508224786, b=>-0.8768249157
Iteration 3893: Achieved Loss=> 23.5674737357
Gradients: w=>0.0507954599, b=>-0.8763587687
Iteration 3894: Achieved Loss=> 23.5667033557
Gradients: w=>0.0507684554, b=>-0.8758928695
Iteration 3895: Achieved Loss=> 23.5659337945
Gradients: w=>0.0507414654, b=>-0.8754272179
Iteration 3896: Achieved Loss=> 23.5651650514
Gradients: w=>0.0507144896, b=>-0.8749618140
Iteration 3897: Achieved Loss=> 23.5643971255
Gradients: w=>0.0506875283, b=>-0.8744966574
Iteration 3898: Achieved Loss=> 23.5636300158
Gradients: w=>0.0506605812, b=>-0.8740317481
Iteration 3899: Achieved Loss=> 23.5628637216
Gradients: w=>0.0506336485, b=>-0.8735670860
Iteration 3900: Achieved Loss=> 23.5620982419
Gradients: w=>0.0506067301, b=>-0.8731026709
Iteration 3901: Achieved Loss=> 23.5613335759
Gradients: w=>0.0505798260, b=>-0.8726385028
Iteration 3902: Achieved Loss=> 23.5605697227
Gradients: w=>0.0505529362, b=>-0.8721745814
Iteration 3903: Achieved Loss=> 23.5598066815
Gradients: w=>0.0505260607, b=>-0.8717109066
Iteration 3904: Achieved Loss=> 23.5590444514
Gradients: w=>0.0504991995, b=>-0.8712474783
Iteration 3905: Achieved Loss=> 23.5582830315
Gradients: w=>0.0504723526, b=>-0.8707842964
Iteration 3906: Achieved Loss=> 23.5575224210
Gradients: w=>0.0504455199, b=>-0.8703213607
Iteration 3907: Achieved Loss=> 23.5567626190
Gradients: w=>0.0504187016, b=>-0.8698586712
Iteration 3908: Achieved Loss=> 23.5560036246
Gradients: w=>0.0503918974, b=>-0.8693962276
Iteration 3909: Achieved Loss=> 23.5552454371
Gradients: w=>0.0503651075, b=>-0.8689340299
Iteration 3910: Achieved Loss=> 23.5544880554
Gradients: w=>0.0503383319, b=>-0.8684720779
Iteration 3911: Achieved Loss=> 23.5537314789
Gradients: w=>0.0503115705, b=>-0.8680103715
Iteration 3912: Achieved Loss=> 23.5529757066
Gradients: w=>0.0502848233, b=>-0.8675489105
Iteration 3913: Achieved Loss=> 23.5522207377
Gradients: w=>0.0502580904, b=>-0.8670876949
Iteration 3914: Achieved Loss=> 23.5514665712
Gradients: w=>0.0502313716, b=>-0.8666267245
Iteration 3915: Achieved Loss=> 23.5507132065
Gradients: w=>0.0502046671, b=>-0.8661659991
Iteration 3916: Achieved Loss=> 23.5499606425
Gradients: w=>0.0501779767, b=>-0.8657055187
Iteration 3917: Achieved Loss=> 23.5492088785
Gradients: w=>0.0501513006, b=>-0.8652452830
Iteration 3918: Achieved Loss=> 23.5484579136
Gradients: w=>0.0501246386, b=>-0.8647852921
Iteration 3919: Achieved Loss=> 23.5477077470
Gradients: w=>0.0500979908, b=>-0.8643255457
Iteration 3920: Achieved Loss=> 23.5469583778
Gradients: w=>0.0500713572, b=>-0.8638660437
Iteration 3921: Achieved Loss=> 23.5462098052
Gradients: w=>0.0500447377, b=>-0.8634067860
Iteration 3922: Achieved Loss=> 23.5454620282
Gradients: w=>0.0500181324, b=>-0.8629477724
Iteration 3923: Achieved Loss=> 23.5447150462
Gradients: w=>0.0499915412, b=>-0.8624890029
Iteration 3924: Achieved Loss=> 23.5439688581
Gradients: w=>0.0499649642, b=>-0.8620304773
Iteration 3925: Achieved Loss=> 23.5432234633
Gradients: w=>0.0499384013, b=>-0.8615721954
Iteration 3926: Achieved Loss=> 23.5424788608
Gradients: w=>0.0499118525, b=>-0.8611141572
Iteration 3927: Achieved Loss=> 23.5417350498
Gradients: w=>0.0498853178, b=>-0.8606563625
Iteration 3928: Achieved Loss=> 23.5409920294
Gradients: w=>0.0498587973, b=>-0.8601988111
Iteration 3929: Achieved Loss=> 23.5402497989
Gradients: w=>0.0498322908, b=>-0.8597415030
Iteration 3930: Achieved Loss=> 23.5395083573
Gradients: w=>0.0498057984, b=>-0.8592844381
Iteration 3931: Achieved Loss=> 23.5387677039
Gradients: w=>0.0497793201, b=>-0.8588276161
Iteration 3932: Achieved Loss=> 23.5380278377
Gradients: w=>0.0497528559, b=>-0.8583710370
Iteration 3933: Achieved Loss=> 23.5372887580
Gradients: w=>0.0497264058, b=>-0.8579147006
Iteration 3934: Achieved Loss=> 23.5365504640
Gradients: w=>0.0496999697, b=>-0.8574586068
Iteration 3935: Achieved Loss=> 23.5358129547
Gradients: w=>0.0496735477, b=>-0.8570027555
Iteration 3936: Achieved Loss=> 23.5350762294
Gradients: w=>0.0496471397, b=>-0.8565471465
Iteration 3937: Achieved Loss=> 23.5343402873
Gradients: w=>0.0496207457, b=>-0.8560917798
Iteration 3938: Achieved Loss=> 23.5336051274
Gradients: w=>0.0495943658, b=>-0.8556366551
Iteration 3939: Achieved Loss=> 23.5328707490
Gradients: w=>0.0495679999, b=>-0.8551817724
Iteration 3940: Achieved Loss=> 23.5321371512
Gradients: w=>0.0495416481, b=>-0.8547271315
Iteration 3941: Achieved Loss=> 23.5314043332
Gradients: w=>0.0495153102, b=>-0.8542727323
Iteration 3942: Achieved Loss=> 23.5306722941
Gradients: w=>0.0494889863, b=>-0.8538185747
Iteration 3943: Achieved Loss=> 23.5299410333
Gradients: w=>0.0494626765, b=>-0.8533646585
Iteration 3944: Achieved Loss=> 23.5292105497
Gradients: w=>0.0494363806, b=>-0.8529109837
Iteration 3945: Achieved Loss=> 23.5284808426
Gradients: w=>0.0494100987, b=>-0.8524575500
Iteration 3946: Achieved Loss=> 23.5277519112
Gradients: w=>0.0493838308, b=>-0.8520043574
Iteration 3947: Achieved Loss=> 23.5270237546
Gradients: w=>0.0493575768, b=>-0.8515514058
Iteration 3948: Achieved Loss=> 23.5262963720
Gradients: w=>0.0493313368, b=>-0.8510986949
Iteration 3949: Achieved Loss=> 23.5255697627
Gradients: w=>0.0493051108, b=>-0.8506462247
Iteration 3950: Achieved Loss=> 23.5248439257
Gradients: w=>0.0492788987, b=>-0.8501939951
Iteration 3951: Achieved Loss=> 23.5241188602
Gradients: w=>0.0492527005, b=>-0.8497420059
Iteration 3952: Achieved Loss=> 23.5233945655
Gradients: w=>0.0492265162, b=>-0.8492902569
Iteration 3953: Achieved Loss=> 23.5226710407
Gradients: w=>0.0492003459, b=>-0.8488387481
Iteration 3954: Achieved Loss=> 23.5219482849
Gradients: w=>0.0491741895, b=>-0.8483874794
Iteration 3955: Achieved Loss=> 23.5212262975
Gradients: w=>0.0491480470, b=>-0.8479364506
Iteration 3956: Achieved Loss=> 23.5205050775
Gradients: w=>0.0491219184, b=>-0.8474856615
Iteration 3957: Achieved Loss=> 23.5197846241
Gradients: w=>0.0490958037, b=>-0.8470351122
Iteration 3958: Achieved Loss=> 23.5190649366
Gradients: w=>0.0490697028, b=>-0.8465848023
Iteration 3959: Achieved Loss=> 23.5183460141
Gradients: w=>0.0490436159, b=>-0.8461347318
Iteration 3960: Achieved Loss=> 23.5176278558
Gradients: w=>0.0490175428, b=>-0.8456849006
Iteration 3961: Achieved Loss=> 23.5169104609
Gradients: w=>0.0489914835, b=>-0.8452353086
Iteration 3962: Achieved Loss=> 23.5161938285
Gradients: w=>0.0489654382, b=>-0.8447859555
Iteration 3963: Achieved Loss=> 23.5154779579
Gradients: w=>0.0489394066, b=>-0.8443368414
Iteration 3964: Achieved Loss=> 23.5147628483
Gradients: w=>0.0489133889, b=>-0.8438879660
Iteration 3965: Achieved Loss=> 23.5140484988
Gradients: w=>0.0488873851, b=>-0.8434393293
Iteration 3966: Achieved Loss=> 23.5133349087
Gradients: w=>0.0488613950, b=>-0.8429909310
Iteration 3967: Achieved Loss=> 23.5126220771
Gradients: w=>0.0488354188, b=>-0.8425427712
Iteration 3968: Achieved Loss=> 23.5119100032
Gradients: w=>0.0488094564, b=>-0.8420948496
Iteration 3969: Achieved Loss=> 23.5111986862
Gradients: w=>0.0487835078, b=>-0.8416471661
Iteration 3970: Achieved Loss=> 23.5104881253
Gradients: w=>0.0487575730, b=>-0.8411997206
Iteration 3971: Achieved Loss=> 23.5097783198
Gradients: w=>0.0487316520, b=>-0.8407525131
Iteration 3972: Achieved Loss=> 23.5090692688
Gradients: w=>0.0487057447, b=>-0.8403055432
Iteration 3973: Achieved Loss=> 23.5083609714
Gradients: w=>0.0486798513, b=>-0.8398588110
Iteration 3974: Achieved Loss=> 23.5076534270
Gradients: w=>0.0486539716, b=>-0.8394123163
Iteration 3975: Achieved Loss=> 23.5069466347
Gradients: w=>0.0486281056, b=>-0.8389660589
Iteration 3976: Achieved Loss=> 23.5062405937
Gradients: w=>0.0486022534, b=>-0.8385200388
Iteration 3977: Achieved Loss=> 23.5055353032
Gradients: w=>0.0485764150, b=>-0.8380742558
Iteration 3978: Achieved Loss=> 23.5048307624
Gradients: w=>0.0485505903, b=>-0.8376287098
Iteration 3979: Achieved Loss=> 23.5041269705
Gradients: w=>0.0485247793, b=>-0.8371834007
Iteration 3980: Achieved Loss=> 23.5034239267
Gradients: w=>0.0484989820, b=>-0.8367383283
Iteration 3981: Achieved Loss=> 23.5027216303
Gradients: w=>0.0484731985, b=>-0.8362934925
Iteration 3982: Achieved Loss=> 23.5020200803
Gradients: w=>0.0484474286, b=>-0.8358488932
Iteration 3983: Achieved Loss=> 23.5013192762
Gradients: w=>0.0484216725, b=>-0.8354045303
Iteration 3984: Achieved Loss=> 23.5006192169
Gradients: w=>0.0483959300, b=>-0.8349604036
Iteration 3985: Achieved Loss=> 23.4999199018
Gradients: w=>0.0483702013, b=>-0.8345165130
Iteration 3986: Achieved Loss=> 23.4992213300
Gradients: w=>0.0483444862, b=>-0.8340728584
Iteration 3987: Achieved Loss=> 23.4985235009
Gradients: w=>0.0483187848, b=>-0.8336294397
Iteration 3988: Achieved Loss=> 23.4978264135
Gradients: w=>0.0482930970, b=>-0.8331862567
Iteration 3989: Achieved Loss=> 23.4971300671
Gradients: w=>0.0482674229, b=>-0.8327433093
Iteration 3990: Achieved Loss=> 23.4964344608
Gradients: w=>0.0482417625, b=>-0.8323005974
Iteration 3991: Achieved Loss=> 23.4957395940
Gradients: w=>0.0482161157, b=>-0.8318581209
Iteration 3992: Achieved Loss=> 23.4950454659
Gradients: w=>0.0481904825, b=>-0.8314158795
Iteration 3993: Achieved Loss=> 23.4943520756
Gradients: w=>0.0481648630, b=>-0.8309738733
Iteration 3994: Achieved Loss=> 23.4936594223
Gradients: w=>0.0481392571, b=>-0.8305321021
Iteration 3995: Achieved Loss=> 23.4929675053
Gradients: w=>0.0481136648, b=>-0.8300905658
Iteration 3996: Achieved Loss=> 23.4922763238
Gradients: w=>0.0480880861, b=>-0.8296492641
Iteration 3997: Achieved Loss=> 23.4915858770
Gradients: w=>0.0480625210, b=>-0.8292081971
Iteration 3998: Achieved Loss=> 23.4908961642
Gradients: w=>0.0480369694, b=>-0.8287673646
Iteration 3999: Achieved Loss=> 23.4902071845
Gradients: w=>0.0480114315, b=>-0.8283267664
Iteration 4000: Achieved Loss=> 23.4895189371
Gradients: w=>0.0479859072, b=>-0.8278864025
Iteration 4001: Achieved Loss=> 23.4888314214
Gradients: w=>0.0479603964, b=>-0.8274462727
Iteration 4002: Achieved Loss=> 23.4881446365
Gradients: w=>0.0479348992, b=>-0.8270063768
Iteration 4003: Achieved Loss=> 23.4874585816
Gradients: w=>0.0479094155, b=>-0.8265667149
Iteration 4004: Achieved Loss=> 23.4867732559
Gradients: w=>0.0478839454, b=>-0.8261272866
Iteration 4005: Achieved Loss=> 23.4860886588
Gradients: w=>0.0478584888, b=>-0.8256880920
Iteration 4006: Achieved Loss=> 23.4854047894
Gradients: w=>0.0478330458, b=>-0.8252491309
Iteration 4007: Achieved Loss=> 23.4847216469
Gradients: w=>0.0478076163, b=>-0.8248104031
Iteration 4008: Achieved Loss=> 23.4840392306
Gradients: w=>0.0477822003, b=>-0.8243719086
Iteration 4009: Achieved Loss=> 23.4833575396
Gradients: w=>0.0477567978, b=>-0.8239336472
Iteration 4010: Achieved Loss=> 23.4826765733
Gradients: w=>0.0477314088, b=>-0.8234956187
Iteration 4011: Achieved Loss=> 23.4819963309
Gradients: w=>0.0477060333, b=>-0.8230578232
Iteration 4012: Achieved Loss=> 23.4813168115
Gradients: w=>0.0476806713, b=>-0.8226202604
Iteration 4013: Achieved Loss=> 23.4806380144
Gradients: w=>0.0476553228, b=>-0.8221829302
Iteration 4014: Achieved Loss=> 23.4799599389
Gradients: w=>0.0476299878, b=>-0.8217458325
Iteration 4015: Achieved Loss=> 23.4792825842
Gradients: w=>0.0476046662, b=>-0.8213089672
Iteration 4016: Achieved Loss=> 23.4786059495
Gradients: w=>0.0475793581, b=>-0.8208723342
Iteration 4017: Achieved Loss=> 23.4779300340
Gradients: w=>0.0475540635, b=>-0.8204359332
Iteration 4018: Achieved Loss=> 23.4772548370
Gradients: w=>0.0475287823, b=>-0.8199997643
Iteration 4019: Achieved Loss=> 23.4765803578
Gradients: w=>0.0475035145, b=>-0.8195638273
Iteration 4020: Achieved Loss=> 23.4759065955
Gradients: w=>0.0474782602, b=>-0.8191281220
Iteration 4021: Achieved Loss=> 23.4752335493
Gradients: w=>0.0474530193, b=>-0.8186926483
Iteration 4022: Achieved Loss=> 23.4745612187
Gradients: w=>0.0474277918, b=>-0.8182574062
Iteration 4023: Achieved Loss=> 23.4738896027
Gradients: w=>0.0474025777, b=>-0.8178223954
Iteration 4024: Achieved Loss=> 23.4732187006
Gradients: w=>0.0473773771, b=>-0.8173876159
Iteration 4025: Achieved Loss=> 23.4725485116
Gradients: w=>0.0473521898, b=>-0.8169530676
Iteration 4026: Achieved Loss=> 23.4718790351
Gradients: w=>0.0473270159, b=>-0.8165187503
Iteration 4027: Achieved Loss=> 23.4712102702
Gradients: w=>0.0473018554, b=>-0.8160846638
Iteration 4028: Achieved Loss=> 23.4705422162
Gradients: w=>0.0472767083, b=>-0.8156508082
Iteration 4029: Achieved Loss=> 23.4698748723
Gradients: w=>0.0472515746, b=>-0.8152171832
Iteration 4030: Achieved Loss=> 23.4692082378
Gradients: w=>0.0472264542, b=>-0.8147837887
Iteration 4031: Achieved Loss=> 23.4685423119
Gradients: w=>0.0472013471, b=>-0.8143506246
Iteration 4032: Achieved Loss=> 23.4678770938
Gradients: w=>0.0471762535, b=>-0.8139176908
Iteration 4033: Achieved Loss=> 23.4672125829
Gradients: w=>0.0471511731, b=>-0.8134849872
Iteration 4034: Achieved Loss=> 23.4665487783
Gradients: w=>0.0471261061, b=>-0.8130525136
Iteration 4035: Achieved Loss=> 23.4658856794
Gradients: w=>0.0471010524, b=>-0.8126202700
Iteration 4036: Achieved Loss=> 23.4652232853
Gradients: w=>0.0470760121, b=>-0.8121882561
Iteration 4037: Achieved Loss=> 23.4645615953
Gradients: w=>0.0470509850, b=>-0.8117564719
Iteration 4038: Achieved Loss=> 23.4639006087
Gradients: w=>0.0470259713, b=>-0.8113249172
Iteration 4039: Achieved Loss=> 23.4632403247
Gradients: w=>0.0470009708, b=>-0.8108935920
Iteration 4040: Achieved Loss=> 23.4625807426
Gradients: w=>0.0469759836, b=>-0.8104624961
Iteration 4041: Achieved Loss=> 23.4619218616
Gradients: w=>0.0469510098, b=>-0.8100316293
Iteration 4042: Achieved Loss=> 23.4612636809
Gradients: w=>0.0469260492, b=>-0.8096009916
Iteration 4043: Achieved Loss=> 23.4606061999
Gradients: w=>0.0469011018, b=>-0.8091705829
Iteration 4044: Achieved Loss=> 23.4599494178
Gradients: w=>0.0468761678, b=>-0.8087404030
Iteration 4045: Achieved Loss=> 23.4592933338
Gradients: w=>0.0468512470, b=>-0.8083104518
Iteration 4046: Achieved Loss=> 23.4586379473
Gradients: w=>0.0468263394, b=>-0.8078807291
Iteration 4047: Achieved Loss=> 23.4579832574
Gradients: w=>0.0468014451, b=>-0.8074512349
Iteration 4048: Achieved Loss=> 23.4573292634
Gradients: w=>0.0467765640, b=>-0.8070219691
Iteration 4049: Achieved Loss=> 23.4566759646
Gradients: w=>0.0467516961, b=>-0.8065929314
Iteration 4050: Achieved Loss=> 23.4560233602
Gradients: w=>0.0467268415, b=>-0.8061641218
Iteration 4051: Achieved Loss=> 23.4553714496
Gradients: w=>0.0467020001, b=>-0.8057355403
Iteration 4052: Achieved Loss=> 23.4547202319
Gradients: w=>0.0466771719, b=>-0.8053071865
Iteration 4053: Achieved Loss=> 23.4540697064
Gradients: w=>0.0466523568, b=>-0.8048790605
Iteration 4054: Achieved Loss=> 23.4534198724
Gradients: w=>0.0466275550, b=>-0.8044511621
Iteration 4055: Achieved Loss=> 23.4527707292
Gradients: w=>0.0466027664, b=>-0.8040234911
Iteration 4056: Achieved Loss=> 23.4521222761
Gradients: w=>0.0465779909, b=>-0.8035960476
Iteration 4057: Achieved Loss=> 23.4514745122
Gradients: w=>0.0465532286, b=>-0.8031688313
Iteration 4058: Achieved Loss=> 23.4508274369
Gradients: w=>0.0465284795, b=>-0.8027418420
Iteration 4059: Achieved Loss=> 23.4501810494
Gradients: w=>0.0465037435, b=>-0.8023150798
Iteration 4060: Achieved Loss=> 23.4495353489
Gradients: w=>0.0464790207, b=>-0.8018885445
Iteration 4061: Achieved Loss=> 23.4488903349
Gradients: w=>0.0464543110, b=>-0.8014622360
Iteration 4062: Achieved Loss=> 23.4482460065
Gradients: w=>0.0464296145, b=>-0.8010361540
Iteration 4063: Achieved Loss=> 23.4476023630
Gradients: w=>0.0464049311, b=>-0.8006102986
Iteration 4064: Achieved Loss=> 23.4469594037
Gradients: w=>0.0463802608, b=>-0.8001846696
Iteration 4065: Achieved Loss=> 23.4463171278
Gradients: w=>0.0463556036, b=>-0.7997592669
Iteration 4066: Achieved Loss=> 23.4456755347
Gradients: w=>0.0463309596, b=>-0.7993340903
Iteration 4067: Achieved Loss=> 23.4450346236
Gradients: w=>0.0463063286, b=>-0.7989091398
Iteration 4068: Achieved Loss=> 23.4443943937
Gradients: w=>0.0462817108, b=>-0.7984844152
Iteration 4069: Achieved Loss=> 23.4437548444
Gradients: w=>0.0462571060, b=>-0.7980599163
Iteration 4070: Achieved Loss=> 23.4431159749
Gradients: w=>0.0462325143, b=>-0.7976356432
Iteration 4071: Achieved Loss=> 23.4424777845
Gradients: w=>0.0462079357, b=>-0.7972115956
Iteration 4072: Achieved Loss=> 23.4418402725
Gradients: w=>0.0461833701, b=>-0.7967877734
Iteration 4073: Achieved Loss=> 23.4412034382
Gradients: w=>0.0461588176, b=>-0.7963641766
Iteration 4074: Achieved Loss=> 23.4405672808
Gradients: w=>0.0461342782, b=>-0.7959408050
Iteration 4075: Achieved Loss=> 23.4399317996
Gradients: w=>0.0461097518, b=>-0.7955176584
Iteration 4076: Achieved Loss=> 23.4392969940
Gradients: w=>0.0460852384, b=>-0.7950947368
Iteration 4077: Achieved Loss=> 23.4386628631
Gradients: w=>0.0460607381, b=>-0.7946720400
Iteration 4078: Achieved Loss=> 23.4380294063
Gradients: w=>0.0460362508, b=>-0.7942495680
Iteration 4079: Achieved Loss=> 23.4373966228
Gradients: w=>0.0460117765, b=>-0.7938273205
Iteration 4080: Achieved Loss=> 23.4367645120
Gradients: w=>0.0459873152, b=>-0.7934052976
Iteration 4081: Achieved Loss=> 23.4361330731
Gradients: w=>0.0459628670, b=>-0.7929834989
Iteration 4082: Achieved Loss=> 23.4355023054
Gradients: w=>0.0459384317, b=>-0.7925619246
Iteration 4083: Achieved Loss=> 23.4348722082
Gradients: w=>0.0459140094, b=>-0.7921405743
Iteration 4084: Achieved Loss=> 23.4342427807
Gradients: w=>0.0458896001, b=>-0.7917194481
Iteration 4085: Achieved Loss=> 23.4336140224
Gradients: w=>0.0458652038, b=>-0.7912985457
Iteration 4086: Achieved Loss=> 23.4329859324
Gradients: w=>0.0458408204, b=>-0.7908778671
Iteration 4087: Achieved Loss=> 23.4323585100
Gradients: w=>0.0458164500, b=>-0.7904574122
Iteration 4088: Achieved Loss=> 23.4317317546
Gradients: w=>0.0457920926, b=>-0.7900371808
Iteration 4089: Achieved Loss=> 23.4311056654
Gradients: w=>0.0457677481, b=>-0.7896171727
Iteration 4090: Achieved Loss=> 23.4304802417
Gradients: w=>0.0457434166, b=>-0.7891973880
Iteration 4091: Achieved Loss=> 23.4298554829
Gradients: w=>0.0457190980, b=>-0.7887778265
Iteration 4092: Achieved Loss=> 23.4292313881
Gradients: w=>0.0456947923, b=>-0.7883584880
Iteration 4093: Achieved Loss=> 23.4286079567
Gradients: w=>0.0456704996, b=>-0.7879393724
Iteration 4094: Achieved Loss=> 23.4279851881
Gradients: w=>0.0456462197, b=>-0.7875204796
Iteration 4095: Achieved Loss=> 23.4273630814
Gradients: w=>0.0456219528, b=>-0.7871018096
Iteration 4096: Achieved Loss=> 23.4267416360
Gradients: w=>0.0455976988, b=>-0.7866833621
Iteration 4097: Achieved Loss=> 23.4261208512
Gradients: w=>0.0455734576, b=>-0.7862651371
Iteration 4098: Achieved Loss=> 23.4255007263
Gradients: w=>0.0455492294, b=>-0.7858471344
Iteration 4099: Achieved Loss=> 23.4248812605
Gradients: w=>0.0455250140, b=>-0.7854293539
Iteration 4100: Achieved Loss=> 23.4242624533
Gradients: w=>0.0455008115, b=>-0.7850117956
Iteration 4101: Achieved Loss=> 23.4236443038
Gradients: w=>0.0454766219, b=>-0.7845944592
Iteration 4102: Achieved Loss=> 23.4230268114
Gradients: w=>0.0454524451, b=>-0.7841773447
Iteration 4103: Achieved Loss=> 23.4224099754
Gradients: w=>0.0454282812, b=>-0.7837604520
Iteration 4104: Achieved Loss=> 23.4217937950
Gradients: w=>0.0454041301, b=>-0.7833437808
Iteration 4105: Achieved Loss=> 23.4211782697
Gradients: w=>0.0453799919, b=>-0.7829273312
Iteration 4106: Achieved Loss=> 23.4205633986
Gradients: w=>0.0453558665, b=>-0.7825111030
Iteration 4107: Achieved Loss=> 23.4199491811
Gradients: w=>0.0453317539, b=>-0.7820950961
Iteration 4108: Achieved Loss=> 23.4193356166
Gradients: w=>0.0453076542, b=>-0.7816793104
Iteration 4109: Achieved Loss=> 23.4187227042
Gradients: w=>0.0452835672, b=>-0.7812637457
Iteration 4110: Achieved Loss=> 23.4181104433
Gradients: w=>0.0452594931, b=>-0.7808484019
Iteration 4111: Achieved Loss=> 23.4174988333
Gradients: w=>0.0452354318, b=>-0.7804332789
Iteration 4112: Achieved Loss=> 23.4168878734
Gradients: w=>0.0452113832, b=>-0.7800183766
Iteration 4113: Achieved Loss=> 23.4162775630
Gradients: w=>0.0451873475, b=>-0.7796036949
Iteration 4114: Achieved Loss=> 23.4156679012
Gradients: w=>0.0451633245, b=>-0.7791892336
Iteration 4115: Achieved Loss=> 23.4150588876
Gradients: w=>0.0451393143, b=>-0.7787749927
Iteration 4116: Achieved Loss=> 23.4144505213
Gradients: w=>0.0451153168, b=>-0.7783609720
Iteration 4117: Achieved Loss=> 23.4138428017
Gradients: w=>0.0450913321, b=>-0.7779471714
Iteration 4118: Achieved Loss=> 23.4132357281
Gradients: w=>0.0450673602, b=>-0.7775335908
Iteration 4119: Achieved Loss=> 23.4126292997
Gradients: w=>0.0450434010, b=>-0.7771202301
Iteration 4120: Achieved Loss=> 23.4120235161
Gradients: w=>0.0450194546, b=>-0.7767070892
Iteration 4121: Achieved Loss=> 23.4114183763
Gradients: w=>0.0449955208, b=>-0.7762941678
Iteration 4122: Achieved Loss=> 23.4108138798
Gradients: w=>0.0449715998, b=>-0.7758814660
Iteration 4123: Achieved Loss=> 23.4102100259
Gradients: w=>0.0449476916, b=>-0.7754689836
Iteration 4124: Achieved Loss=> 23.4096068138
Gradients: w=>0.0449237960, b=>-0.7750567205
Iteration 4125: Achieved Loss=> 23.4090042430
Gradients: w=>0.0448999131, b=>-0.7746446766
Iteration 4126: Achieved Loss=> 23.4084023126
Gradients: w=>0.0448760429, b=>-0.7742328517
Iteration 4127: Achieved Loss=> 23.4078010221
Gradients: w=>0.0448521855, b=>-0.7738212457
Iteration 4128: Achieved Loss=> 23.4072003708
Gradients: w=>0.0448283407, b=>-0.7734098586
Iteration 4129: Achieved Loss=> 23.4066003579
Gradients: w=>0.0448045085, b=>-0.7729986902
Iteration 4130: Achieved Loss=> 23.4060009829
Gradients: w=>0.0447806891, b=>-0.7725877403
Iteration 4131: Achieved Loss=> 23.4054022450
Gradients: w=>0.0447568823, b=>-0.7721770090
Iteration 4132: Achieved Loss=> 23.4048041435
Gradients: w=>0.0447330882, b=>-0.7717664960
Iteration 4133: Achieved Loss=> 23.4042066778
Gradients: w=>0.0447093067, b=>-0.7713562012
Iteration 4134: Achieved Loss=> 23.4036098471
Gradients: w=>0.0446855378, b=>-0.7709461246
Iteration 4135: Achieved Loss=> 23.4030136509
Gradients: w=>0.0446617816, b=>-0.7705362660
Iteration 4136: Achieved Loss=> 23.4024180885
Gradients: w=>0.0446380381, b=>-0.7701266252
Iteration 4137: Achieved Loss=> 23.4018231591
Gradients: w=>0.0446143071, b=>-0.7697172023
Iteration 4138: Achieved Loss=> 23.4012288621
Gradients: w=>0.0445905888, b=>-0.7693079970
Iteration 4139: Achieved Loss=> 23.4006351968
Gradients: w=>0.0445668831, b=>-0.7688990093
Iteration 4140: Achieved Loss=> 23.4000421626
Gradients: w=>0.0445431899, b=>-0.7684902389
Iteration 4141: Achieved Loss=> 23.3994497588
Gradients: w=>0.0445195094, b=>-0.7680816859
Iteration 4142: Achieved Loss=> 23.3988579847
Gradients: w=>0.0444958415, b=>-0.7676733501
Iteration 4143: Achieved Loss=> 23.3982668396
Gradients: w=>0.0444721861, b=>-0.7672652314
Iteration 4144: Achieved Loss=> 23.3976763229
Gradients: w=>0.0444485433, b=>-0.7668573297
Iteration 4145: Achieved Loss=> 23.3970864339
Gradients: w=>0.0444249131, b=>-0.7664496448
Iteration 4146: Achieved Loss=> 23.3964971719
Gradients: w=>0.0444012955, b=>-0.7660421766
Iteration 4147: Achieved Loss=> 23.3959085363
Gradients: w=>0.0443776904, b=>-0.7656349251
Iteration 4148: Achieved Loss=> 23.3953205265
Gradients: w=>0.0443540978, b=>-0.7652278901
Iteration 4149: Achieved Loss=> 23.3947331416
Gradients: w=>0.0443305178, b=>-0.7648210714
Iteration 4150: Achieved Loss=> 23.3941463812
Gradients: w=>0.0443069504, b=>-0.7644144691
Iteration 4151: Achieved Loss=> 23.3935602444
Gradients: w=>0.0442833954, b=>-0.7640080829
Iteration 4152: Achieved Loss=> 23.3929747308
Gradients: w=>0.0442598530, b=>-0.7636019127
Iteration 4153: Achieved Loss=> 23.3923898395
Gradients: w=>0.0442363231, b=>-0.7631959585
Iteration 4154: Achieved Loss=> 23.3918055699
Gradients: w=>0.0442128057, b=>-0.7627902201
Iteration 4155: Achieved Loss=> 23.3912219214
Gradients: w=>0.0441893009, b=>-0.7623846974
Iteration 4156: Achieved Loss=> 23.3906388933
Gradients: w=>0.0441658085, b=>-0.7619793903
Iteration 4157: Achieved Loss=> 23.3900564849
Gradients: w=>0.0441423286, b=>-0.7615742987
Iteration 4158: Achieved Loss=> 23.3894746956
Gradients: w=>0.0441188612, b=>-0.7611694224
Iteration 4159: Achieved Loss=> 23.3888935248
Gradients: w=>0.0440954062, b=>-0.7607647614
Iteration 4160: Achieved Loss=> 23.3883129717
Gradients: w=>0.0440719637, b=>-0.7603603155
Iteration 4161: Achieved Loss=> 23.3877330358
Gradients: w=>0.0440485337, b=>-0.7599560846
Iteration 4162: Achieved Loss=> 23.3871537163
Gradients: w=>0.0440251162, b=>-0.7595520686
Iteration 4163: Achieved Loss=> 23.3865750126
Gradients: w=>0.0440017111, b=>-0.7591482674
Iteration 4164: Achieved Loss=> 23.3859969241
Gradients: w=>0.0439783184, b=>-0.7587446809
Iteration 4165: Achieved Loss=> 23.3854194500
Gradients: w=>0.0439549382, b=>-0.7583413089
Iteration 4166: Achieved Loss=> 23.3848425898
Gradients: w=>0.0439315704, b=>-0.7579381514
Iteration 4167: Achieved Loss=> 23.3842663428
Gradients: w=>0.0439082150, b=>-0.7575352082
Iteration 4168: Achieved Loss=> 23.3836907084
Gradients: w=>0.0438848721, b=>-0.7571324793
Iteration 4169: Achieved Loss=> 23.3831156858
Gradients: w=>0.0438615415, b=>-0.7567299644
Iteration 4170: Achieved Loss=> 23.3825412744
Gradients: w=>0.0438382234, b=>-0.7563276635
Iteration 4171: Achieved Loss=> 23.3819674737
Gradients: w=>0.0438149176, b=>-0.7559255765
Iteration 4172: Achieved Loss=> 23.3813942828
Gradients: w=>0.0437916243, b=>-0.7555237033
Iteration 4173: Achieved Loss=> 23.3808217013
Gradients: w=>0.0437683433, b=>-0.7551220437
Iteration 4174: Achieved Loss=> 23.3802497284
Gradients: w=>0.0437450747, b=>-0.7547205976
Iteration 4175: Achieved Loss=> 23.3796783635
Gradients: w=>0.0437218185, b=>-0.7543193650
Iteration 4176: Achieved Loss=> 23.3791076060
Gradients: w=>0.0436985746, b=>-0.7539183457
Iteration 4177: Achieved Loss=> 23.3785374551
Gradients: w=>0.0436753431, b=>-0.7535175395
Iteration 4178: Achieved Loss=> 23.3779679104
Gradients: w=>0.0436521240, b=>-0.7531169465
Iteration 4179: Achieved Loss=> 23.3773989710
Gradients: w=>0.0436289172, b=>-0.7527165664
Iteration 4180: Achieved Loss=> 23.3768306364
Gradients: w=>0.0436057227, b=>-0.7523163992
Iteration 4181: Achieved Loss=> 23.3762629059
Gradients: w=>0.0435825406, b=>-0.7519164447
Iteration 4182: Achieved Loss=> 23.3756957789
Gradients: w=>0.0435593707, b=>-0.7515167028
Iteration 4183: Achieved Loss=> 23.3751292548
Gradients: w=>0.0435362132, b=>-0.7511171735
Iteration 4184: Achieved Loss=> 23.3745633329
Gradients: w=>0.0435130681, b=>-0.7507178565
Iteration 4185: Achieved Loss=> 23.3739980125
Gradients: w=>0.0434899352, b=>-0.7503187519
Iteration 4186: Achieved Loss=> 23.3734332930
Gradients: w=>0.0434668146, b=>-0.7499198594
Iteration 4187: Achieved Loss=> 23.3728691739
Gradients: w=>0.0434437063, b=>-0.7495211790
Iteration 4188: Achieved Loss=> 23.3723056543
Gradients: w=>0.0434206103, b=>-0.7491227105
Iteration 4189: Achieved Loss=> 23.3717427338
Gradients: w=>0.0433975266, b=>-0.7487244539
Iteration 4190: Achieved Loss=> 23.3711804117
Gradients: w=>0.0433744551, b=>-0.7483264090
Iteration 4191: Achieved Loss=> 23.3706186873
Gradients: w=>0.0433513959, b=>-0.7479285757
Iteration 4192: Achieved Loss=> 23.3700575600
Gradients: w=>0.0433283490, b=>-0.7475309539
Iteration 4193: Achieved Loss=> 23.3694970291
Gradients: w=>0.0433053143, b=>-0.7471335435
Iteration 4194: Achieved Loss=> 23.3689370941
Gradients: w=>0.0432822919, b=>-0.7467363444
Iteration 4195: Achieved Loss=> 23.3683777543
Gradients: w=>0.0432592817, b=>-0.7463393564
Iteration 4196: Achieved Loss=> 23.3678190091
Gradients: w=>0.0432362837, b=>-0.7459425795
Iteration 4197: Achieved Loss=> 23.3672608578
Gradients: w=>0.0432132980, b=>-0.7455460135
Iteration 4198: Achieved Loss=> 23.3667032998
Gradients: w=>0.0431903245, b=>-0.7451496584
Iteration 4199: Achieved Loss=> 23.3661463345
Gradients: w=>0.0431673632, b=>-0.7447535140
Iteration 4200: Achieved Loss=> 23.3655899612
Gradients: w=>0.0431444141, b=>-0.7443575801
Iteration 4201: Achieved Loss=> 23.3650341793
Gradients: w=>0.0431214772, b=>-0.7439618568
Iteration 4202: Achieved Loss=> 23.3644789882
Gradients: w=>0.0430985525, b=>-0.7435663438
Iteration 4203: Achieved Loss=> 23.3639243873
Gradients: w=>0.0430756399, b=>-0.7431710411
Iteration 4204: Achieved Loss=> 23.3633703759
Gradients: w=>0.0430527396, b=>-0.7427759486
Iteration 4205: Achieved Loss=> 23.3628169534
Gradients: w=>0.0430298515, b=>-0.7423810661
Iteration 4206: Achieved Loss=> 23.3622641192
Gradients: w=>0.0430069755, b=>-0.7419863935
Iteration 4207: Achieved Loss=> 23.3617118726
Gradients: w=>0.0429841116, b=>-0.7415919308
Iteration 4208: Achieved Loss=> 23.3611602130
Gradients: w=>0.0429612600, b=>-0.7411976778
Iteration 4209: Achieved Loss=> 23.3606091399
Gradients: w=>0.0429384204, b=>-0.7408036343
Iteration 4210: Achieved Loss=> 23.3600586525
Gradients: w=>0.0429155931, b=>-0.7404098004
Iteration 4211: Achieved Loss=> 23.3595087503
Gradients: w=>0.0428927778, b=>-0.7400161758
Iteration 4212: Achieved Loss=> 23.3589594326
Gradients: w=>0.0428699747, b=>-0.7396227605
Iteration 4213: Achieved Loss=> 23.3584106989
Gradients: w=>0.0428471837, b=>-0.7392295543
Iteration 4214: Achieved Loss=> 23.3578625484
Gradients: w=>0.0428244048, b=>-0.7388365572
Iteration 4215: Achieved Loss=> 23.3573149806
Gradients: w=>0.0428016381, b=>-0.7384437690
Iteration 4216: Achieved Loss=> 23.3567679949
Gradients: w=>0.0427788834, b=>-0.7380511896
Iteration 4217: Achieved Loss=> 23.3562215906
Gradients: w=>0.0427561408, b=>-0.7376588190
Iteration 4218: Achieved Loss=> 23.3556757671
Gradients: w=>0.0427334103, b=>-0.7372666569
Iteration 4219: Achieved Loss=> 23.3551305238
Gradients: w=>0.0427106920, b=>-0.7368747033
Iteration 4220: Achieved Loss=> 23.3545858601
Gradients: w=>0.0426879856, b=>-0.7364829581
Iteration 4221: Achieved Loss=> 23.3540417753
Gradients: w=>0.0426652914, b=>-0.7360914212
Iteration 4222: Achieved Loss=> 23.3534982689
Gradients: w=>0.0426426092, b=>-0.7357000924
Iteration 4223: Achieved Loss=> 23.3529553403
Gradients: w=>0.0426199391, b=>-0.7353089716
Iteration 4224: Achieved Loss=> 23.3524129887
Gradients: w=>0.0425972810, b=>-0.7349180588
Iteration 4225: Achieved Loss=> 23.3518712137
Gradients: w=>0.0425746350, b=>-0.7345273538
Iteration 4226: Achieved Loss=> 23.3513300145
Gradients: w=>0.0425520010, b=>-0.7341368565
Iteration 4227: Achieved Loss=> 23.3507893907
Gradients: w=>0.0425293791, b=>-0.7337465668
Iteration 4228: Achieved Loss=> 23.3502493415
Gradients: w=>0.0425067692, b=>-0.7333564846
Iteration 4229: Achieved Loss=> 23.3497098664
Gradients: w=>0.0424841712, b=>-0.7329666098
Iteration 4230: Achieved Loss=> 23.3491709647
Gradients: w=>0.0424615854, b=>-0.7325769423
Iteration 4231: Achieved Loss=> 23.3486326359
Gradients: w=>0.0424390115, b=>-0.7321874819
Iteration 4232: Achieved Loss=> 23.3480948793
Gradients: w=>0.0424164496, b=>-0.7317982285
Iteration 4233: Achieved Loss=> 23.3475576943
Gradients: w=>0.0423938997, b=>-0.7314091821
Iteration 4234: Achieved Loss=> 23.3470210804
Gradients: w=>0.0423713618, b=>-0.7310203426
Iteration 4235: Achieved Loss=> 23.3464850368
Gradients: w=>0.0423488359, b=>-0.7306317097
Iteration 4236: Achieved Loss=> 23.3459495631
Gradients: w=>0.0423263220, b=>-0.7302432835
Iteration 4237: Achieved Loss=> 23.3454146585
Gradients: w=>0.0423038200, b=>-0.7298550637
Iteration 4238: Achieved Loss=> 23.3448803226
Gradients: w=>0.0422813300, b=>-0.7294670504
Iteration 4239: Achieved Loss=> 23.3443465546
Gradients: w=>0.0422588519, b=>-0.7290792433
Iteration 4240: Achieved Loss=> 23.3438133540
Gradients: w=>0.0422363858, b=>-0.7286916424
Iteration 4241: Achieved Loss=> 23.3432807202
Gradients: w=>0.0422139317, b=>-0.7283042475
Iteration 4242: Achieved Loss=> 23.3427486526
Gradients: w=>0.0421914894, b=>-0.7279170587
Iteration 4243: Achieved Loss=> 23.3422171506
Gradients: w=>0.0421690591, b=>-0.7275300756
Iteration 4244: Achieved Loss=> 23.3416862135
Gradients: w=>0.0421466408, b=>-0.7271432983
Iteration 4245: Achieved Loss=> 23.3411558408
Gradients: w=>0.0421242343, b=>-0.7267567266
Iteration 4246: Achieved Loss=> 23.3406260319
Gradients: w=>0.0421018398, b=>-0.7263703604
Iteration 4247: Achieved Loss=> 23.3400967861
Gradients: w=>0.0420794572, b=>-0.7259841996
Iteration 4248: Achieved Loss=> 23.3395681029
Gradients: w=>0.0420570864, b=>-0.7255982441
Iteration 4249: Achieved Loss=> 23.3390399818
Gradients: w=>0.0420347276, b=>-0.7252124938
Iteration 4250: Achieved Loss=> 23.3385124219
Gradients: w=>0.0420123806, b=>-0.7248269486
Iteration 4251: Achieved Loss=> 23.3379854229
Gradients: w=>0.0419900456, b=>-0.7244416084
Iteration 4252: Achieved Loss=> 23.3374589841
Gradients: w=>0.0419677224, b=>-0.7240564730
Iteration 4253: Achieved Loss=> 23.3369331048
Gradients: w=>0.0419454110, b=>-0.7236715423
Iteration 4254: Achieved Loss=> 23.3364077846
Gradients: w=>0.0419231116, b=>-0.7232868163
Iteration 4255: Achieved Loss=> 23.3358830228
Gradients: w=>0.0419008240, b=>-0.7229022948
Iteration 4256: Achieved Loss=> 23.3353588187
Gradients: w=>0.0418785482, b=>-0.7225179778
Iteration 4257: Achieved Loss=> 23.3348351719
Gradients: w=>0.0418562843, b=>-0.7221338651
Iteration 4258: Achieved Loss=> 23.3343120817
Gradients: w=>0.0418340322, b=>-0.7217499565
Iteration 4259: Achieved Loss=> 23.3337895476
Gradients: w=>0.0418117919, b=>-0.7213662521
Iteration 4260: Achieved Loss=> 23.3332675689
Gradients: w=>0.0417895635, b=>-0.7209827517
Iteration 4261: Achieved Loss=> 23.3327461450
Gradients: w=>0.0417673469, b=>-0.7205994551
Iteration 4262: Achieved Loss=> 23.3322252754
Gradients: w=>0.0417451421, b=>-0.7202163623
Iteration 4263: Achieved Loss=> 23.3317049595
Gradients: w=>0.0417229491, b=>-0.7198334732
Iteration 4264: Achieved Loss=> 23.3311851967
Gradients: w=>0.0417007679, b=>-0.7194507876
Iteration 4265: Achieved Loss=> 23.3306659863
Gradients: w=>0.0416785985, b=>-0.7190683055
Iteration 4266: Achieved Loss=> 23.3301473279
Gradients: w=>0.0416564409, b=>-0.7186860267
Iteration 4267: Achieved Loss=> 23.3296292208
Gradients: w=>0.0416342950, b=>-0.7183039512
Iteration 4268: Achieved Loss=> 23.3291116644
Gradients: w=>0.0416121609, b=>-0.7179220787
Iteration 4269: Achieved Loss=> 23.3285946582
Gradients: w=>0.0415900386, b=>-0.7175404093
Iteration 4270: Achieved Loss=> 23.3280782016
Gradients: w=>0.0415679281, b=>-0.7171589428
Iteration 4271: Achieved Loss=> 23.3275622939
Gradients: w=>0.0415458293, b=>-0.7167776791
Iteration 4272: Achieved Loss=> 23.3270469346
Gradients: w=>0.0415237423, b=>-0.7163966181
Iteration 4273: Achieved Loss=> 23.3265321232
Gradients: w=>0.0415016670, b=>-0.7160157597
Iteration 4274: Achieved Loss=> 23.3260178589
Gradients: w=>0.0414796034, b=>-0.7156351037
Iteration 4275: Achieved Loss=> 23.3255041414
Gradients: w=>0.0414575516, b=>-0.7152546501
Iteration 4276: Achieved Loss=> 23.3249909699
Gradients: w=>0.0414355115, b=>-0.7148743988
Iteration 4277: Achieved Loss=> 23.3244783439
Gradients: w=>0.0414134831, b=>-0.7144943496
Iteration 4278: Achieved Loss=> 23.3239662628
Gradients: w=>0.0413914664, b=>-0.7141145025
Iteration 4279: Achieved Loss=> 23.3234547260
Gradients: w=>0.0413694614, b=>-0.7137348573
Iteration 4280: Achieved Loss=> 23.3229437330
Gradients: w=>0.0413474682, b=>-0.7133554139
Iteration 4281: Achieved Loss=> 23.3224332832
Gradients: w=>0.0413254866, b=>-0.7129761723
Iteration 4282: Achieved Loss=> 23.3219233759
Gradients: w=>0.0413035167, b=>-0.7125971323
Iteration 4283: Achieved Loss=> 23.3214140107
Gradients: w=>0.0412815585, b=>-0.7122182938
Iteration 4284: Achieved Loss=> 23.3209051869
Gradients: w=>0.0412596119, b=>-0.7118396567
Iteration 4285: Achieved Loss=> 23.3203969040
Gradients: w=>0.0412376770, b=>-0.7114612209
Iteration 4286: Achieved Loss=> 23.3198891614
Gradients: w=>0.0412157538, b=>-0.7110829862
Iteration 4287: Achieved Loss=> 23.3193819585
Gradients: w=>0.0411938423, b=>-0.7107049527
Iteration 4288: Achieved Loss=> 23.3188752948
Gradients: w=>0.0411719424, b=>-0.7103271201
Iteration 4289: Achieved Loss=> 23.3183691696
Gradients: w=>0.0411500541, b=>-0.7099494884
Iteration 4290: Achieved Loss=> 23.3178635824
Gradients: w=>0.0411281775, b=>-0.7095720575
Iteration 4291: Achieved Loss=> 23.3173585327
Gradients: w=>0.0411063125, b=>-0.7091948272
Iteration 4292: Achieved Loss=> 23.3168540198
Gradients: w=>0.0410844591, b=>-0.7088177975
Iteration 4293: Achieved Loss=> 23.3163500432
Gradients: w=>0.0410626173, b=>-0.7084409682
Iteration 4294: Achieved Loss=> 23.3158466023
Gradients: w=>0.0410407872, b=>-0.7080643392
Iteration 4295: Achieved Loss=> 23.3153436965
Gradients: w=>0.0410189686, b=>-0.7076879105
Iteration 4296: Achieved Loss=> 23.3148413254
Gradients: w=>0.0409971617, b=>-0.7073116818
Iteration 4297: Achieved Loss=> 23.3143394882
Gradients: w=>0.0409753663, b=>-0.7069356532
Iteration 4298: Achieved Loss=> 23.3138381845
Gradients: w=>0.0409535826, b=>-0.7065598246
Iteration 4299: Achieved Loss=> 23.3133374137
Gradients: w=>0.0409318104, b=>-0.7061841957
Iteration 4300: Achieved Loss=> 23.3128371752
Gradients: w=>0.0409100498, b=>-0.7058087665
Iteration 4301: Achieved Loss=> 23.3123374684
Gradients: w=>0.0408883008, b=>-0.7054335369
Iteration 4302: Achieved Loss=> 23.3118382928
Gradients: w=>0.0408665633, b=>-0.7050585067
Iteration 4303: Achieved Loss=> 23.3113396478
Gradients: w=>0.0408448374, b=>-0.7046836760
Iteration 4304: Achieved Loss=> 23.3108415328
Gradients: w=>0.0408231230, b=>-0.7043090445
Iteration 4305: Achieved Loss=> 23.3103439474
Gradients: w=>0.0408014202, b=>-0.7039346122
Iteration 4306: Achieved Loss=> 23.3098468909
Gradients: w=>0.0407797289, b=>-0.7035603790
Iteration 4307: Achieved Loss=> 23.3093503627
Gradients: w=>0.0407580491, b=>-0.7031863447
Iteration 4308: Achieved Loss=> 23.3088543623
Gradients: w=>0.0407363809, b=>-0.7028125092
Iteration 4309: Achieved Loss=> 23.3083588892
Gradients: w=>0.0407147242, b=>-0.7024388725
Iteration 4310: Achieved Loss=> 23.3078639427
Gradients: w=>0.0406930790, b=>-0.7020654345
Iteration 4311: Achieved Loss=> 23.3073695224
Gradients: w=>0.0406714453, b=>-0.7016921949
Iteration 4312: Achieved Loss=> 23.3068756276
Gradients: w=>0.0406498231, b=>-0.7013191538
Iteration 4313: Achieved Loss=> 23.3063822578
Gradients: w=>0.0406282124, b=>-0.7009463110
Iteration 4314: Achieved Loss=> 23.3058894125
Gradients: w=>0.0406066132, b=>-0.7005736665
Iteration 4315: Achieved Loss=> 23.3053970910
Gradients: w=>0.0405850255, b=>-0.7002012200
Iteration 4316: Achieved Loss=> 23.3049052929
Gradients: w=>0.0405634492, b=>-0.6998289715
Iteration 4317: Achieved Loss=> 23.3044140175
Gradients: w=>0.0405418845, b=>-0.6994569210
Iteration 4318: Achieved Loss=> 23.3039232644
Gradients: w=>0.0405203312, b=>-0.6990850682
Iteration 4319: Achieved Loss=> 23.3034330329
Gradients: w=>0.0404987893, b=>-0.6987134131
Iteration 4320: Achieved Loss=> 23.3029433225
Gradients: w=>0.0404772589, b=>-0.6983419556
Iteration 4321: Achieved Loss=> 23.3024541327
Gradients: w=>0.0404557399, b=>-0.6979706956
Iteration 4322: Achieved Loss=> 23.3019654629
Gradients: w=>0.0404342324, b=>-0.6975996330
Iteration 4323: Achieved Loss=> 23.3014773125
Gradients: w=>0.0404127364, b=>-0.6972287676
Iteration 4324: Achieved Loss=> 23.3009896810
Gradients: w=>0.0403912517, b=>-0.6968580994
Iteration 4325: Achieved Loss=> 23.3005025678
Gradients: w=>0.0403697785, b=>-0.6964876282
Iteration 4326: Achieved Loss=> 23.3000159725
Gradients: w=>0.0403483167, b=>-0.6961173540
Iteration 4327: Achieved Loss=> 23.2995298944
Gradients: w=>0.0403268663, b=>-0.6957472767
Iteration 4328: Achieved Loss=> 23.2990443329
Gradients: w=>0.0403054273, b=>-0.6953773961
Iteration 4329: Achieved Loss=> 23.2985592877
Gradients: w=>0.0402839997, b=>-0.6950077121
Iteration 4330: Achieved Loss=> 23.2980747580
Gradients: w=>0.0402625834, b=>-0.6946382247
Iteration 4331: Achieved Loss=> 23.2975907433
Gradients: w=>0.0402411786, b=>-0.6942689336
Iteration 4332: Achieved Loss=> 23.2971072432
Gradients: w=>0.0402197852, b=>-0.6938998390
Iteration 4333: Achieved Loss=> 23.2966242570
Gradients: w=>0.0401984031, b=>-0.6935309405
Iteration 4334: Achieved Loss=> 23.2961417842
Gradients: w=>0.0401770324, b=>-0.6931622382
Iteration 4335: Achieved Loss=> 23.2956598242
Gradients: w=>0.0401556731, b=>-0.6927937319
Iteration 4336: Achieved Loss=> 23.2951783766
Gradients: w=>0.0401343251, b=>-0.6924254214
Iteration 4337: Achieved Loss=> 23.2946974408
Gradients: w=>0.0401129884, b=>-0.6920573068
Iteration 4338: Achieved Loss=> 23.2942170161
Gradients: w=>0.0400916631, b=>-0.6916893879
Iteration 4339: Achieved Loss=> 23.2937371022
Gradients: w=>0.0400703492, b=>-0.6913216646
Iteration 4340: Achieved Loss=> 23.2932576984
Gradients: w=>0.0400490465, b=>-0.6909541368
Iteration 4341: Achieved Loss=> 23.2927788042
Gradients: w=>0.0400277552, b=>-0.6905868044
Iteration 4342: Achieved Loss=> 23.2923004190
Gradients: w=>0.0400064753, b=>-0.6902196672
Iteration 4343: Achieved Loss=> 23.2918225424
Gradients: w=>0.0399852066, b=>-0.6898527253
Iteration 4344: Achieved Loss=> 23.2913451737
Gradients: w=>0.0399639492, b=>-0.6894859784
Iteration 4345: Achieved Loss=> 23.2908683124
Gradients: w=>0.0399427032, b=>-0.6891194265
Iteration 4346: Achieved Loss=> 23.2903919581
Gradients: w=>0.0399214684, b=>-0.6887530694
Iteration 4347: Achieved Loss=> 23.2899161101
Gradients: w=>0.0399002449, b=>-0.6883869072
Iteration 4348: Achieved Loss=> 23.2894407679
Gradients: w=>0.0398790327, b=>-0.6880209396
Iteration 4349: Achieved Loss=> 23.2889659310
Gradients: w=>0.0398578318, b=>-0.6876551665
Iteration 4350: Achieved Loss=> 23.2884915989
Gradients: w=>0.0398366422, b=>-0.6872895879
Iteration 4351: Achieved Loss=> 23.2880177709
Gradients: w=>0.0398154638, b=>-0.6869242037
Iteration 4352: Achieved Loss=> 23.2875444466
Gradients: w=>0.0397942967, b=>-0.6865590137
Iteration 4353: Achieved Loss=> 23.2870716255
Gradients: w=>0.0397731408, b=>-0.6861940178
Iteration 4354: Achieved Loss=> 23.2865993069
Gradients: w=>0.0397519962, b=>-0.6858292160
Iteration 4355: Achieved Loss=> 23.2861274904
Gradients: w=>0.0397308628, b=>-0.6854646082
Iteration 4356: Achieved Loss=> 23.2856561755
Gradients: w=>0.0397097406, b=>-0.6851001941
Iteration 4357: Achieved Loss=> 23.2851853615
Gradients: w=>0.0396886297, b=>-0.6847359739
Iteration 4358: Achieved Loss=> 23.2847150480
Gradients: w=>0.0396675300, b=>-0.6843719472
Iteration 4359: Achieved Loss=> 23.2842452345
Gradients: w=>0.0396464415, b=>-0.6840081141
Iteration 4360: Achieved Loss=> 23.2837759203
Gradients: w=>0.0396253643, b=>-0.6836444744
Iteration 4361: Achieved Loss=> 23.2833071050
Gradients: w=>0.0396042982, b=>-0.6832810280
Iteration 4362: Achieved Loss=> 23.2828387881
Gradients: w=>0.0395832434, b=>-0.6829177748
Iteration 4363: Achieved Loss=> 23.2823709690
Gradients: w=>0.0395621997, b=>-0.6825547147
Iteration 4364: Achieved Loss=> 23.2819036471
Gradients: w=>0.0395411672, b=>-0.6821918477
Iteration 4365: Achieved Loss=> 23.2814368220
Gradients: w=>0.0395201459, b=>-0.6818291736
Iteration 4366: Achieved Loss=> 23.2809704931
Gradients: w=>0.0394991358, b=>-0.6814666923
Iteration 4367: Achieved Loss=> 23.2805046600
Gradients: w=>0.0394781368, b=>-0.6811044037
Iteration 4368: Achieved Loss=> 23.2800393220
Gradients: w=>0.0394571491, b=>-0.6807423076
Iteration 4369: Achieved Loss=> 23.2795744786
Gradients: w=>0.0394361724, b=>-0.6803804041
Iteration 4370: Achieved Loss=> 23.2791101294
Gradients: w=>0.0394152069, b=>-0.6800186930
Iteration 4371: Achieved Loss=> 23.2786462737
Gradients: w=>0.0393942526, b=>-0.6796571742
Iteration 4372: Achieved Loss=> 23.2781829111
Gradients: w=>0.0393733094, b=>-0.6792958476
Iteration 4373: Achieved Loss=> 23.2777200411
Gradients: w=>0.0393523774, b=>-0.6789347131
Iteration 4374: Achieved Loss=> 23.2772576631
Gradients: w=>0.0393314564, b=>-0.6785737706
Iteration 4375: Achieved Loss=> 23.2767957766
Gradients: w=>0.0393105466, b=>-0.6782130199
Iteration 4376: Achieved Loss=> 23.2763343810
Gradients: w=>0.0392896479, b=>-0.6778524610
Iteration 4377: Achieved Loss=> 23.2758734759
Gradients: w=>0.0392687603, b=>-0.6774920939
Iteration 4378: Achieved Loss=> 23.2754130608
Gradients: w=>0.0392478839, b=>-0.6771319183
Iteration 4379: Achieved Loss=> 23.2749531350
Gradients: w=>0.0392270185, b=>-0.6767719342
Iteration 4380: Achieved Loss=> 23.2744936982
Gradients: w=>0.0392061642, b=>-0.6764121414
Iteration 4381: Achieved Loss=> 23.2740347497
Gradients: w=>0.0391853210, b=>-0.6760525400
Iteration 4382: Achieved Loss=> 23.2735762891
Gradients: w=>0.0391644889, b=>-0.6756931297
Iteration 4383: Achieved Loss=> 23.2731183158
Gradients: w=>0.0391436678, b=>-0.6753339105
Iteration 4384: Achieved Loss=> 23.2726608293
Gradients: w=>0.0391228579, b=>-0.6749748823
Iteration 4385: Achieved Loss=> 23.2722038291
Gradients: w=>0.0391020589, b=>-0.6746160449
Iteration 4386: Achieved Loss=> 23.2717473147
Gradients: w=>0.0390812711, b=>-0.6742573983
Iteration 4387: Achieved Loss=> 23.2712912856
Gradients: w=>0.0390604943, b=>-0.6738989424
Iteration 4388: Achieved Loss=> 23.2708357412
Gradients: w=>0.0390397285, b=>-0.6735406770
Iteration 4389: Achieved Loss=> 23.2703806811
Gradients: w=>0.0390189738, b=>-0.6731826021
Iteration 4390: Achieved Loss=> 23.2699261046
Gradients: w=>0.0389982301, b=>-0.6728247176
Iteration 4391: Achieved Loss=> 23.2694720114
Gradients: w=>0.0389774975, b=>-0.6724670233
Iteration 4392: Achieved Loss=> 23.2690184009
Gradients: w=>0.0389567758, b=>-0.6721095192
Iteration 4393: Achieved Loss=> 23.2685652725
Gradients: w=>0.0389360652, b=>-0.6717522051
Iteration 4394: Achieved Loss=> 23.2681126258
Gradients: w=>0.0389153656, b=>-0.6713950810
Iteration 4395: Achieved Loss=> 23.2676604603
Gradients: w=>0.0388946770, b=>-0.6710381468
Iteration 4396: Achieved Loss=> 23.2672087754
Gradients: w=>0.0388739994, b=>-0.6706814023
Iteration 4397: Achieved Loss=> 23.2667575706
Gradients: w=>0.0388533328, b=>-0.6703248475
Iteration 4398: Achieved Loss=> 23.2663068455
Gradients: w=>0.0388326772, b=>-0.6699684822
Iteration 4399: Achieved Loss=> 23.2658565995
Gradients: w=>0.0388120325, b=>-0.6696123064
Iteration 4400: Achieved Loss=> 23.2654068320
Gradients: w=>0.0387913989, b=>-0.6692563200
Iteration 4401: Achieved Loss=> 23.2649575427
Gradients: w=>0.0387707762, b=>-0.6689005228
Iteration 4402: Achieved Loss=> 23.2645087310
Gradients: w=>0.0387501644, b=>-0.6685449147
Iteration 4403: Achieved Loss=> 23.2640603963
Gradients: w=>0.0387295636, b=>-0.6681894957
Iteration 4404: Achieved Loss=> 23.2636125382
Gradients: w=>0.0387089738, b=>-0.6678342657
Iteration 4405: Achieved Loss=> 23.2631651561
Gradients: w=>0.0386883949, b=>-0.6674792245
Iteration 4406: Achieved Loss=> 23.2627182497
Gradients: w=>0.0386678270, b=>-0.6671243720
Iteration 4407: Achieved Loss=> 23.2622718182
Gradients: w=>0.0386472700, b=>-0.6667697083
Iteration 4408: Achieved Loss=> 23.2618258613
Gradients: w=>0.0386267239, b=>-0.6664152330
Iteration 4409: Achieved Loss=> 23.2613803785
Gradients: w=>0.0386061888, b=>-0.6660609462
Iteration 4410: Achieved Loss=> 23.2609353692
Gradients: w=>0.0385856645, b=>-0.6657068478
Iteration 4411: Achieved Loss=> 23.2604908329
Gradients: w=>0.0385651512, b=>-0.6653529376
Iteration 4412: Achieved Loss=> 23.2600467692
Gradients: w=>0.0385446488, b=>-0.6649992155
Iteration 4413: Achieved Loss=> 23.2596031775
Gradients: w=>0.0385241573, b=>-0.6646456815
Iteration 4414: Achieved Loss=> 23.2591600573
Gradients: w=>0.0385036767, b=>-0.6642923355
Iteration 4415: Achieved Loss=> 23.2587174082
Gradients: w=>0.0384832069, b=>-0.6639391773
Iteration 4416: Achieved Loss=> 23.2582752296
Gradients: w=>0.0384627481, b=>-0.6635862069
Iteration 4417: Achieved Loss=> 23.2578335210
Gradients: w=>0.0384423001, b=>-0.6632334241
Iteration 4418: Achieved Loss=> 23.2573922819
Gradients: w=>0.0384218630, b=>-0.6628808288
Iteration 4419: Achieved Loss=> 23.2569515119
Gradients: w=>0.0384014367, b=>-0.6625284210
Iteration 4420: Achieved Loss=> 23.2565112104
Gradients: w=>0.0383810214, b=>-0.6621762006
Iteration 4421: Achieved Loss=> 23.2560713769
Gradients: w=>0.0383606168, b=>-0.6618241674
Iteration 4422: Achieved Loss=> 23.2556320109
Gradients: w=>0.0383402231, b=>-0.6614723213
Iteration 4423: Achieved Loss=> 23.2551931120
Gradients: w=>0.0383198403, b=>-0.6611206623
Iteration 4424: Achieved Loss=> 23.2547546797
Gradients: w=>0.0382994683, b=>-0.6607691903
Iteration 4425: Achieved Loss=> 23.2543167133
Gradients: w=>0.0382791071, b=>-0.6604179051
Iteration 4426: Achieved Loss=> 23.2538792126
Gradients: w=>0.0382587568, b=>-0.6600668067
Iteration 4427: Achieved Loss=> 23.2534421769
Gradients: w=>0.0382384172, b=>-0.6597158949
Iteration 4428: Achieved Loss=> 23.2530056057
Gradients: w=>0.0382180885, b=>-0.6593651697
Iteration 4429: Achieved Loss=> 23.2525694986
Gradients: w=>0.0381977706, b=>-0.6590146309
Iteration 4430: Achieved Loss=> 23.2521338551
Gradients: w=>0.0381774635, b=>-0.6586642785
Iteration 4431: Achieved Loss=> 23.2516986746
Gradients: w=>0.0381571672, b=>-0.6583141124
Iteration 4432: Achieved Loss=> 23.2512639568
Gradients: w=>0.0381368817, b=>-0.6579641324
Iteration 4433: Achieved Loss=> 23.2508297010
Gradients: w=>0.0381166069, b=>-0.6576143385
Iteration 4434: Achieved Loss=> 23.2503959069
Gradients: w=>0.0380963430, b=>-0.6572647305
Iteration 4435: Achieved Loss=> 23.2499625738
Gradients: w=>0.0380760898, b=>-0.6569153084
Iteration 4436: Achieved Loss=> 23.2495297014
Gradients: w=>0.0380558474, b=>-0.6565660720
Iteration 4437: Achieved Loss=> 23.2490972891
Gradients: w=>0.0380356157, b=>-0.6562170214
Iteration 4438: Achieved Loss=> 23.2486653365
Gradients: w=>0.0380153948, b=>-0.6558681563
Iteration 4439: Achieved Loss=> 23.2482338430
Gradients: w=>0.0379951847, b=>-0.6555194766
Iteration 4440: Achieved Loss=> 23.2478028082
Gradients: w=>0.0379749853, b=>-0.6551709823
Iteration 4441: Achieved Loss=> 23.2473722316
Gradients: w=>0.0379547966, b=>-0.6548226733
Iteration 4442: Achieved Loss=> 23.2469421126
Gradients: w=>0.0379346187, b=>-0.6544745495
Iteration 4443: Achieved Loss=> 23.2465124509
Gradients: w=>0.0379144515, b=>-0.6541266107
Iteration 4444: Achieved Loss=> 23.2460832459
Gradients: w=>0.0378942950, b=>-0.6537788570
Iteration 4445: Achieved Loss=> 23.2456544971
Gradients: w=>0.0378741492, b=>-0.6534312881
Iteration 4446: Achieved Loss=> 23.2452262041
Gradients: w=>0.0378540141, b=>-0.6530839039
Iteration 4447: Achieved Loss=> 23.2447983663
Gradients: w=>0.0378338898, b=>-0.6527367045
Iteration 4448: Achieved Loss=> 23.2443709834
Gradients: w=>0.0378137761, b=>-0.6523896896
Iteration 4449: Achieved Loss=> 23.2439440547
Gradients: w=>0.0377936732, b=>-0.6520428592
Iteration 4450: Achieved Loss=> 23.2435175798
Gradients: w=>0.0377735809, b=>-0.6516962132
Iteration 4451: Achieved Loss=> 23.2430915583
Gradients: w=>0.0377534993, b=>-0.6513497515
Iteration 4452: Achieved Loss=> 23.2426659896
Gradients: w=>0.0377334284, b=>-0.6510034740
Iteration 4453: Achieved Loss=> 23.2422408733
Gradients: w=>0.0377133681, b=>-0.6506573806
Iteration 4454: Achieved Loss=> 23.2418162089
Gradients: w=>0.0376933185, b=>-0.6503114711
Iteration 4455: Achieved Loss=> 23.2413919959
Gradients: w=>0.0376732796, b=>-0.6499657456
Iteration 4456: Achieved Loss=> 23.2409682338
Gradients: w=>0.0376532513, b=>-0.6496202038
Iteration 4457: Achieved Loss=> 23.2405449222
Gradients: w=>0.0376332337, b=>-0.6492748458
Iteration 4458: Achieved Loss=> 23.2401220606
Gradients: w=>0.0376132267, b=>-0.6489296714
Iteration 4459: Achieved Loss=> 23.2396996484
Gradients: w=>0.0375932304, b=>-0.6485846804
Iteration 4460: Achieved Loss=> 23.2392776853
Gradients: w=>0.0375732447, b=>-0.6482398729
Iteration 4461: Achieved Loss=> 23.2388561707
Gradients: w=>0.0375532696, b=>-0.6478952487
Iteration 4462: Achieved Loss=> 23.2384351041
Gradients: w=>0.0375333051, b=>-0.6475508077
Iteration 4463: Achieved Loss=> 23.2380144851
Gradients: w=>0.0375133513, b=>-0.6472065498
Iteration 4464: Achieved Loss=> 23.2375943133
Gradients: w=>0.0374934080, b=>-0.6468624749
Iteration 4465: Achieved Loss=> 23.2371745881
Gradients: w=>0.0374734754, b=>-0.6465185830
Iteration 4466: Achieved Loss=> 23.2367553090
Gradients: w=>0.0374535533, b=>-0.6461748739
Iteration 4467: Achieved Loss=> 23.2363364756
Gradients: w=>0.0374336419, b=>-0.6458313475
Iteration 4468: Achieved Loss=> 23.2359180875
Gradients: w=>0.0374137410, b=>-0.6454880037
Iteration 4469: Achieved Loss=> 23.2355001441
Gradients: w=>0.0373938507, b=>-0.6451448425
Iteration 4470: Achieved Loss=> 23.2350826449
Gradients: w=>0.0373739710, b=>-0.6448018637
Iteration 4471: Achieved Loss=> 23.2346655895
Gradients: w=>0.0373541018, b=>-0.6444590672
Iteration 4472: Achieved Loss=> 23.2342489775
Gradients: w=>0.0373342432, b=>-0.6441164530
Iteration 4473: Achieved Loss=> 23.2338328083
Gradients: w=>0.0373143952, b=>-0.6437740209
Iteration 4474: Achieved Loss=> 23.2334170815
Gradients: w=>0.0372945577, b=>-0.6434317709
Iteration 4475: Achieved Loss=> 23.2330017966
Gradients: w=>0.0372747308, b=>-0.6430897028
Iteration 4476: Achieved Loss=> 23.2325869531
Gradients: w=>0.0372549144, b=>-0.6427478166
Iteration 4477: Achieved Loss=> 23.2321725506
Gradients: w=>0.0372351085, b=>-0.6424061121
Iteration 4478: Achieved Loss=> 23.2317585886
Gradients: w=>0.0372153132, b=>-0.6420645893
Iteration 4479: Achieved Loss=> 23.2313450666
Gradients: w=>0.0371955284, b=>-0.6417232481
Iteration 4480: Achieved Loss=> 23.2309319842
Gradients: w=>0.0371757541, b=>-0.6413820883
Iteration 4481: Achieved Loss=> 23.2305193409
Gradients: w=>0.0371559904, b=>-0.6410411099
Iteration 4482: Achieved Loss=> 23.2301071362
Gradients: w=>0.0371362371, b=>-0.6407003127
Iteration 4483: Achieved Loss=> 23.2296953697
Gradients: w=>0.0371164943, b=>-0.6403596968
Iteration 4484: Achieved Loss=> 23.2292840409
Gradients: w=>0.0370967621, b=>-0.6400192619
Iteration 4485: Achieved Loss=> 23.2288731493
Gradients: w=>0.0370770403, b=>-0.6396790080
Iteration 4486: Achieved Loss=> 23.2284626945
Gradients: w=>0.0370573290, b=>-0.6393389350
Iteration 4487: Achieved Loss=> 23.2280526760
Gradients: w=>0.0370376282, b=>-0.6389990428
Iteration 4488: Achieved Loss=> 23.2276430934
Gradients: w=>0.0370179378, b=>-0.6386593313
Iteration 4489: Achieved Loss=> 23.2272339461
Gradients: w=>0.0369982580, b=>-0.6383198004
Iteration 4490: Achieved Loss=> 23.2268252337
Gradients: w=>0.0369785886, b=>-0.6379804500
Iteration 4491: Achieved Loss=> 23.2264169558
Gradients: w=>0.0369589296, b=>-0.6376412800
Iteration 4492: Achieved Loss=> 23.2260091119
Gradients: w=>0.0369392811, b=>-0.6373022904
Iteration 4493: Achieved Loss=> 23.2256017015
Gradients: w=>0.0369196430, b=>-0.6369634809
Iteration 4494: Achieved Loss=> 23.2251947242
Gradients: w=>0.0369000154, b=>-0.6366248516
Iteration 4495: Achieved Loss=> 23.2247881794
Gradients: w=>0.0368803983, b=>-0.6362864023
Iteration 4496: Achieved Loss=> 23.2243820669
Gradients: w=>0.0368607915, b=>-0.6359481329
Iteration 4497: Achieved Loss=> 23.2239763860
Gradients: w=>0.0368411952, b=>-0.6356100433
Iteration 4498: Achieved Loss=> 23.2235711363
Gradients: w=>0.0368216093, b=>-0.6352721335
Iteration 4499: Achieved Loss=> 23.2231663175
Gradients: w=>0.0368020338, b=>-0.6349344034
Iteration 4500: Achieved Loss=> 23.2227619289
Gradients: w=>0.0367824687, b=>-0.6345968527
Iteration 4501: Achieved Loss=> 23.2223579702
Gradients: w=>0.0367629140, b=>-0.6342594816
Iteration 4502: Achieved Loss=> 23.2219544409
Gradients: w=>0.0367433697, b=>-0.6339222898
Iteration 4503: Achieved Loss=> 23.2215513405
Gradients: w=>0.0367238358, b=>-0.6335852772
Iteration 4504: Achieved Loss=> 23.2211486686
Gradients: w=>0.0367043123, b=>-0.6332484438
Iteration 4505: Achieved Loss=> 23.2207464248
Gradients: w=>0.0366847991, b=>-0.6329117895
Iteration 4506: Achieved Loss=> 23.2203446085
Gradients: w=>0.0366652964, b=>-0.6325753142
Iteration 4507: Achieved Loss=> 23.2199432194
Gradients: w=>0.0366458040, b=>-0.6322390177
Iteration 4508: Achieved Loss=> 23.2195422569
Gradients: w=>0.0366263219, b=>-0.6319029001
Iteration 4509: Achieved Loss=> 23.2191417206
Gradients: w=>0.0366068503, b=>-0.6315669611
Iteration 4510: Achieved Loss=> 23.2187416101
Gradients: w=>0.0365873889, b=>-0.6312312007
Iteration 4511: Achieved Loss=> 23.2183419249
Gradients: w=>0.0365679380, b=>-0.6308956188
Iteration 4512: Achieved Loss=> 23.2179426646
Gradients: w=>0.0365484973, b=>-0.6305602153
Iteration 4513: Achieved Loss=> 23.2175438287
Gradients: w=>0.0365290670, b=>-0.6302249902
Iteration 4514: Achieved Loss=> 23.2171454167
Gradients: w=>0.0365096470, b=>-0.6298899432
Iteration 4515: Achieved Loss=> 23.2167474282
Gradients: w=>0.0364902374, b=>-0.6295550744
Iteration 4516: Achieved Loss=> 23.2163498628
Gradients: w=>0.0364708381, b=>-0.6292203836
Iteration 4517: Achieved Loss=> 23.2159527200
Gradients: w=>0.0364514490, b=>-0.6288858707
Iteration 4518: Achieved Loss=> 23.2155559993
Gradients: w=>0.0364320703, b=>-0.6285515357
Iteration 4519: Achieved Loss=> 23.2151597003
Gradients: w=>0.0364127019, b=>-0.6282173784
Iteration 4520: Achieved Loss=> 23.2147638226
Gradients: w=>0.0363933438, b=>-0.6278833988
Iteration 4521: Achieved Loss=> 23.2143683658
Gradients: w=>0.0363739960, b=>-0.6275495967
Iteration 4522: Achieved Loss=> 23.2139733292
Gradients: w=>0.0363546585, b=>-0.6272159721
Iteration 4523: Achieved Loss=> 23.2135787126
Gradients: w=>0.0363353312, b=>-0.6268825248
Iteration 4524: Achieved Loss=> 23.2131845155
Gradients: w=>0.0363160142, b=>-0.6265492548
Iteration 4525: Achieved Loss=> 23.2127907373
Gradients: w=>0.0362967075, b=>-0.6262161620
Iteration 4526: Achieved Loss=> 23.2123973778
Gradients: w=>0.0362774111, b=>-0.6258832462
Iteration 4527: Achieved Loss=> 23.2120044364
Gradients: w=>0.0362581249, b=>-0.6255505075
Iteration 4528: Achieved Loss=> 23.2116119127
Gradients: w=>0.0362388490, b=>-0.6252179457
Iteration 4529: Achieved Loss=> 23.2112198062
Gradients: w=>0.0362195833, b=>-0.6248855606
Iteration 4530: Achieved Loss=> 23.2108281165
Gradients: w=>0.0362003278, b=>-0.6245533523
Iteration 4531: Achieved Loss=> 23.2104368432
Gradients: w=>0.0361810826, b=>-0.6242213205
Iteration 4532: Achieved Loss=> 23.2100459858
Gradients: w=>0.0361618477, b=>-0.6238894653
Iteration 4533: Achieved Loss=> 23.2096555439
Gradients: w=>0.0361426229, b=>-0.6235577865
Iteration 4534: Achieved Loss=> 23.2092655170
Gradients: w=>0.0361234084, b=>-0.6232262841
Iteration 4535: Achieved Loss=> 23.2088759046
Gradients: w=>0.0361042041, b=>-0.6228949578
Iteration 4536: Achieved Loss=> 23.2084867065
Gradients: w=>0.0360850100, b=>-0.6225638078
Iteration 4537: Achieved Loss=> 23.2080979220
Gradients: w=>0.0360658261, b=>-0.6222328337
Iteration 4538: Achieved Loss=> 23.2077095509
Gradients: w=>0.0360466524, b=>-0.6219020357
Iteration 4539: Achieved Loss=> 23.2073215925
Gradients: w=>0.0360274889, b=>-0.6215714134
Iteration 4540: Achieved Loss=> 23.2069340465
Gradients: w=>0.0360083355, b=>-0.6212409670
Iteration 4541: Achieved Loss=> 23.2065469125
Gradients: w=>0.0359891924, b=>-0.6209106962
Iteration 4542: Achieved Loss=> 23.2061601900
Gradients: w=>0.0359700594, b=>-0.6205806011
Iteration 4543: Achieved Loss=> 23.2057738786
Gradients: w=>0.0359509367, b=>-0.6202506814
Iteration 4544: Achieved Loss=> 23.2053879779
Gradients: w=>0.0359318240, b=>-0.6199209371
Iteration 4545: Achieved Loss=> 23.2050024873
Gradients: w=>0.0359127216, b=>-0.6195913681
Iteration 4546: Achieved Loss=> 23.2046174065
Gradients: w=>0.0358936293, b=>-0.6192619743
Iteration 4547: Achieved Loss=> 23.2042327350
Gradients: w=>0.0358745471, b=>-0.6189327556
Iteration 4548: Achieved Loss=> 23.2038484725
Gradients: w=>0.0358554751, b=>-0.6186037119
Iteration 4549: Achieved Loss=> 23.2034646184
Gradients: w=>0.0358364132, b=>-0.6182748432
Iteration 4550: Achieved Loss=> 23.2030811723
Gradients: w=>0.0358173615, b=>-0.6179461493
Iteration 4551: Achieved Loss=> 23.2026981338
Gradients: w=>0.0357983199, b=>-0.6176176302
Iteration 4552: Achieved Loss=> 23.2023155025
Gradients: w=>0.0357792884, b=>-0.6172892857
Iteration 4553: Achieved Loss=> 23.2019332779
Gradients: w=>0.0357602670, b=>-0.6169611158
Iteration 4554: Achieved Loss=> 23.2015514596
Gradients: w=>0.0357412558, b=>-0.6166331203
Iteration 4555: Achieved Loss=> 23.2011700472
Gradients: w=>0.0357222546, b=>-0.6163052992
Iteration 4556: Achieved Loss=> 23.2007890402
Gradients: w=>0.0357032636, b=>-0.6159776524
Iteration 4557: Achieved Loss=> 23.2004084382
Gradients: w=>0.0356842826, b=>-0.6156501798
Iteration 4558: Achieved Loss=> 23.2000282407
Gradients: w=>0.0356653118, b=>-0.6153228812
Iteration 4559: Achieved Loss=> 23.1996484475
Gradients: w=>0.0356463510, b=>-0.6149957567
Iteration 4560: Achieved Loss=> 23.1992690579
Gradients: w=>0.0356274003, b=>-0.6146688061
Iteration 4561: Achieved Loss=> 23.1988900716
Gradients: w=>0.0356084597, b=>-0.6143420293
Iteration 4562: Achieved Loss=> 23.1985114882
Gradients: w=>0.0355895291, b=>-0.6140154262
Iteration 4563: Achieved Loss=> 23.1981333072
Gradients: w=>0.0355706086, b=>-0.6136889968
Iteration 4564: Achieved Loss=> 23.1977555282
Gradients: w=>0.0355516982, b=>-0.6133627408
Iteration 4565: Achieved Loss=> 23.1973781507
Gradients: w=>0.0355327978, b=>-0.6130366584
Iteration 4566: Achieved Loss=> 23.1970011744
Gradients: w=>0.0355139075, b=>-0.6127107493
Iteration 4567: Achieved Loss=> 23.1966245989
Gradients: w=>0.0354950272, b=>-0.6123850134
Iteration 4568: Achieved Loss=> 23.1962484236
Gradients: w=>0.0354761570, b=>-0.6120594508
Iteration 4569: Achieved Loss=> 23.1958726482
Gradients: w=>0.0354572968, b=>-0.6117340612
Iteration 4570: Achieved Loss=> 23.1954972722
Gradients: w=>0.0354384466, b=>-0.6114088446
Iteration 4571: Achieved Loss=> 23.1951222952
Gradients: w=>0.0354196064, b=>-0.6110838009
Iteration 4572: Achieved Loss=> 23.1947477169
Gradients: w=>0.0354007763, b=>-0.6107589300
Iteration 4573: Achieved Loss=> 23.1943735367
Gradients: w=>0.0353819561, b=>-0.6104342318
Iteration 4574: Achieved Loss=> 23.1939997542
Gradients: w=>0.0353631460, b=>-0.6101097062
Iteration 4575: Achieved Loss=> 23.1936263691
Gradients: w=>0.0353443459, b=>-0.6097853531
Iteration 4576: Achieved Loss=> 23.1932533809
Gradients: w=>0.0353255557, b=>-0.6094611725
Iteration 4577: Achieved Loss=> 23.1928807891
Gradients: w=>0.0353067756, b=>-0.6091371643
Iteration 4578: Achieved Loss=> 23.1925085934
Gradients: w=>0.0352880054, b=>-0.6088133283
Iteration 4579: Achieved Loss=> 23.1921367934
Gradients: w=>0.0352692452, b=>-0.6084896644
Iteration 4580: Achieved Loss=> 23.1917653885
Gradients: w=>0.0352504950, b=>-0.6081661726
Iteration 4581: Achieved Loss=> 23.1913943785
Gradients: w=>0.0352317548, b=>-0.6078428528
Iteration 4582: Achieved Loss=> 23.1910237628
Gradients: w=>0.0352130245, b=>-0.6075197049
Iteration 4583: Achieved Loss=> 23.1906535411
Gradients: w=>0.0351943042, b=>-0.6071967288
Iteration 4584: Achieved Loss=> 23.1902837129
Gradients: w=>0.0351755938, b=>-0.6068739244
Iteration 4585: Achieved Loss=> 23.1899142779
Gradients: w=>0.0351568934, b=>-0.6065512915
Iteration 4586: Achieved Loss=> 23.1895452355
Gradients: w=>0.0351382029, b=>-0.6062288303
Iteration 4587: Achieved Loss=> 23.1891765855
Gradients: w=>0.0351195223, b=>-0.6059065404
Iteration 4588: Achieved Loss=> 23.1888083273
Gradients: w=>0.0351008517, b=>-0.6055844219
Iteration 4589: Achieved Loss=> 23.1884404605
Gradients: w=>0.0350821910, b=>-0.6052624746
Iteration 4590: Achieved Loss=> 23.1880729848
Gradients: w=>0.0350635403, b=>-0.6049406985
Iteration 4591: Achieved Loss=> 23.1877058997
Gradients: w=>0.0350448994, b=>-0.6046190935
Iteration 4592: Achieved Loss=> 23.1873392048
Gradients: w=>0.0350262685, b=>-0.6042976594
Iteration 4593: Achieved Loss=> 23.1869728997
Gradients: w=>0.0350076474, b=>-0.6039763962
Iteration 4594: Achieved Loss=> 23.1866069840
Gradients: w=>0.0349890363, b=>-0.6036553038
Iteration 4595: Achieved Loss=> 23.1862414572
Gradients: w=>0.0349704350, b=>-0.6033343821
Iteration 4596: Achieved Loss=> 23.1858763190
Gradients: w=>0.0349518437, b=>-0.6030136311
Iteration 4597: Achieved Loss=> 23.1855115689
Gradients: w=>0.0349332622, b=>-0.6026930505
Iteration 4598: Achieved Loss=> 23.1851472065
Gradients: w=>0.0349146906, b=>-0.6023726404
Iteration 4599: Achieved Loss=> 23.1847832315
Gradients: w=>0.0348961289, b=>-0.6020524006
Iteration 4600: Achieved Loss=> 23.1844196433
Gradients: w=>0.0348775770, b=>-0.6017323311
Iteration 4601: Achieved Loss=> 23.1840564416
Gradients: w=>0.0348590350, b=>-0.6014124317
Iteration 4602: Achieved Loss=> 23.1836936260
Gradients: w=>0.0348405029, b=>-0.6010927024
Iteration 4603: Achieved Loss=> 23.1833311961
Gradients: w=>0.0348219806, b=>-0.6007731430
Iteration 4604: Achieved Loss=> 23.1829691514
Gradients: w=>0.0348034682, b=>-0.6004537536
Iteration 4605: Achieved Loss=> 23.1826074916
Gradients: w=>0.0347849656, b=>-0.6001345340
Iteration 4606: Achieved Loss=> 23.1822462162
Gradients: w=>0.0347664728, b=>-0.5998154840
Iteration 4607: Achieved Loss=> 23.1818853248
Gradients: w=>0.0347479899, b=>-0.5994966037
Iteration 4608: Achieved Loss=> 23.1815248171
Gradients: w=>0.0347295168, b=>-0.5991778929
Iteration 4609: Achieved Loss=> 23.1811646925
Gradients: w=>0.0347110536, b=>-0.5988593515
Iteration 4610: Achieved Loss=> 23.1808049508
Gradients: w=>0.0346926001, b=>-0.5985409795
Iteration 4611: Achieved Loss=> 23.1804455915
Gradients: w=>0.0346741564, b=>-0.5982227768
Iteration 4612: Achieved Loss=> 23.1800866141
Gradients: w=>0.0346557226, b=>-0.5979047432
Iteration 4613: Achieved Loss=> 23.1797280184
Gradients: w=>0.0346372985, b=>-0.5975868787
Iteration 4614: Achieved Loss=> 23.1793698038
Gradients: w=>0.0346188843, b=>-0.5972691831
Iteration 4615: Achieved Loss=> 23.1790119700
Gradients: w=>0.0346004798, b=>-0.5969516565
Iteration 4616: Achieved Loss=> 23.1786545166
Gradients: w=>0.0345820852, b=>-0.5966342987
Iteration 4617: Achieved Loss=> 23.1782974431
Gradients: w=>0.0345637003, b=>-0.5963171096
Iteration 4618: Achieved Loss=> 23.1779407492
Gradients: w=>0.0345453251, b=>-0.5960000891
Iteration 4619: Achieved Loss=> 23.1775844345
Gradients: w=>0.0345269598, b=>-0.5956832372
Iteration 4620: Achieved Loss=> 23.1772284985
Gradients: w=>0.0345086042, b=>-0.5953665537
Iteration 4621: Achieved Loss=> 23.1768729408
Gradients: w=>0.0344902584, b=>-0.5950500385
Iteration 4622: Achieved Loss=> 23.1765177611
Gradients: w=>0.0344719223, b=>-0.5947336917
Iteration 4623: Achieved Loss=> 23.1761629590
Gradients: w=>0.0344535959, b=>-0.5944175130
Iteration 4624: Achieved Loss=> 23.1758085340
Gradients: w=>0.0344352794, b=>-0.5941015024
Iteration 4625: Achieved Loss=> 23.1754544858
Gradients: w=>0.0344169725, b=>-0.5937856598
Iteration 4626: Achieved Loss=> 23.1751008139
Gradients: w=>0.0343986754, b=>-0.5934699851
Iteration 4627: Achieved Loss=> 23.1747475179
Gradients: w=>0.0343803880, b=>-0.5931544782
Iteration 4628: Achieved Loss=> 23.1743945975
Gradients: w=>0.0343621103, b=>-0.5928391391
Iteration 4629: Achieved Loss=> 23.1740420522
Gradients: w=>0.0343438424, b=>-0.5925239676
Iteration 4630: Achieved Loss=> 23.1736898817
Gradients: w=>0.0343255841, b=>-0.5922089637
Iteration 4631: Achieved Loss=> 23.1733380855
Gradients: w=>0.0343073356, b=>-0.5918941272
Iteration 4632: Achieved Loss=> 23.1729866633
Gradients: w=>0.0342890968, b=>-0.5915794582
Iteration 4633: Achieved Loss=> 23.1726356147
Gradients: w=>0.0342708676, b=>-0.5912649564
Iteration 4634: Achieved Loss=> 23.1722849392
Gradients: w=>0.0342526482, b=>-0.5909506218
Iteration 4635: Achieved Loss=> 23.1719346364
Gradients: w=>0.0342344384, b=>-0.5906364543
Iteration 4636: Achieved Loss=> 23.1715847061
Gradients: w=>0.0342162383, b=>-0.5903224538
Iteration 4637: Achieved Loss=> 23.1712351476
Gradients: w=>0.0341980479, b=>-0.5900086203
Iteration 4638: Achieved Loss=> 23.1708859608
Gradients: w=>0.0341798672, b=>-0.5896949536
Iteration 4639: Achieved Loss=> 23.1705371452
Gradients: w=>0.0341616961, b=>-0.5893814536
Iteration 4640: Achieved Loss=> 23.1701887003
Gradients: w=>0.0341435347, b=>-0.5890681204
Iteration 4641: Achieved Loss=> 23.1698406258
Gradients: w=>0.0341253830, b=>-0.5887549537
Iteration 4642: Achieved Loss=> 23.1694929213
Gradients: w=>0.0341072409, b=>-0.5884419535
Iteration 4643: Achieved Loss=> 23.1691455864
Gradients: w=>0.0340891084, b=>-0.5881291197
Iteration 4644: Achieved Loss=> 23.1687986208
Gradients: w=>0.0340709856, b=>-0.5878164522
Iteration 4645: Achieved Loss=> 23.1684520239
Gradients: w=>0.0340528724, b=>-0.5875039509
Iteration 4646: Achieved Loss=> 23.1681057955
Gradients: w=>0.0340347689, b=>-0.5871916158
Iteration 4647: Achieved Loss=> 23.1677599351
Gradients: w=>0.0340166749, b=>-0.5868794467
Iteration 4648: Achieved Loss=> 23.1674144423
Gradients: w=>0.0339985906, b=>-0.5865674435
Iteration 4649: Achieved Loss=> 23.1670693168
Gradients: w=>0.0339805159, b=>-0.5862556063
Iteration 4650: Achieved Loss=> 23.1667245582
Gradients: w=>0.0339624509, b=>-0.5859439348
Iteration 4651: Achieved Loss=> 23.1663801660
Gradients: w=>0.0339443954, b=>-0.5856324290
Iteration 4652: Achieved Loss=> 23.1660361399
Gradients: w=>0.0339263495, b=>-0.5853210889
Iteration 4653: Achieved Loss=> 23.1656924795
Gradients: w=>0.0339083132, b=>-0.5850099142
Iteration 4654: Achieved Loss=> 23.1653491844
Gradients: w=>0.0338902865, b=>-0.5846989050
Iteration 4655: Achieved Loss=> 23.1650062542
Gradients: w=>0.0338722694, b=>-0.5843880611
Iteration 4656: Achieved Loss=> 23.1646636886
Gradients: w=>0.0338542619, b=>-0.5840773825
Iteration 4657: Achieved Loss=> 23.1643214870
Gradients: w=>0.0338362639, b=>-0.5837668690
Iteration 4658: Achieved Loss=> 23.1639796493
Gradients: w=>0.0338182755, b=>-0.5834565206
Iteration 4659: Achieved Loss=> 23.1636381749
Gradients: w=>0.0338002967, b=>-0.5831463372
Iteration 4660: Achieved Loss=> 23.1632970635
Gradients: w=>0.0337823274, b=>-0.5828363187
Iteration 4661: Achieved Loss=> 23.1629563147
Gradients: w=>0.0337643677, b=>-0.5825264650
Iteration 4662: Achieved Loss=> 23.1626159280
Gradients: w=>0.0337464175, b=>-0.5822167761
Iteration 4663: Achieved Loss=> 23.1622759033
Gradients: w=>0.0337284769, b=>-0.5819072518
Iteration 4664: Achieved Loss=> 23.1619362399
Gradients: w=>0.0337105458, b=>-0.5815978920
Iteration 4665: Achieved Loss=> 23.1615969376
Gradients: w=>0.0336926242, b=>-0.5812886967
Iteration 4666: Achieved Loss=> 23.1612579960
Gradients: w=>0.0336747122, b=>-0.5809796658
Iteration 4667: Achieved Loss=> 23.1609194147
Gradients: w=>0.0336568097, b=>-0.5806707992
Iteration 4668: Achieved Loss=> 23.1605811932
Gradients: w=>0.0336389167, b=>-0.5803620968
Iteration 4669: Achieved Loss=> 23.1602433313
Gradients: w=>0.0336210332, b=>-0.5800535585
Iteration 4670: Achieved Loss=> 23.1599058286
Gradients: w=>0.0336031592, b=>-0.5797451842
Iteration 4671: Achieved Loss=> 23.1595686845
Gradients: w=>0.0335852947, b=>-0.5794369739
Iteration 4672: Achieved Loss=> 23.1592318989
Gradients: w=>0.0335674398, b=>-0.5791289274
Iteration 4673: Achieved Loss=> 23.1588954713
Gradients: w=>0.0335495943, b=>-0.5788210447
Iteration 4674: Achieved Loss=> 23.1585594013
Gradients: w=>0.0335317583, b=>-0.5785133256
Iteration 4675: Achieved Loss=> 23.1582236885
Gradients: w=>0.0335139318, b=>-0.5782057702
Iteration 4676: Achieved Loss=> 23.1578883325
Gradients: w=>0.0334961147, b=>-0.5778983783
Iteration 4677: Achieved Loss=> 23.1575533331
Gradients: w=>0.0334783072, b=>-0.5775911497
Iteration 4678: Achieved Loss=> 23.1572186897
Gradients: w=>0.0334605091, b=>-0.5772840846
Iteration 4679: Achieved Loss=> 23.1568844021
Gradients: w=>0.0334427205, b=>-0.5769771826
Iteration 4680: Achieved Loss=> 23.1565504698
Gradients: w=>0.0334249413, b=>-0.5766704438
Iteration 4681: Achieved Loss=> 23.1562168925
Gradients: w=>0.0334071716, b=>-0.5763638681
Iteration 4682: Achieved Loss=> 23.1558836697
Gradients: w=>0.0333894113, b=>-0.5760574554
Iteration 4683: Achieved Loss=> 23.1555508012
Gradients: w=>0.0333716604, b=>-0.5757512056
Iteration 4684: Achieved Loss=> 23.1552182865
Gradients: w=>0.0333539190, b=>-0.5754451186
Iteration 4685: Achieved Loss=> 23.1548861252
Gradients: w=>0.0333361871, b=>-0.5751391943
Iteration 4686: Achieved Loss=> 23.1545543170
Gradients: w=>0.0333184645, b=>-0.5748334326
Iteration 4687: Achieved Loss=> 23.1542228616
Gradients: w=>0.0333007514, b=>-0.5745278335
Iteration 4688: Achieved Loss=> 23.1538917584
Gradients: w=>0.0332830477, b=>-0.5742223969
Iteration 4689: Achieved Loss=> 23.1535610073
Gradients: w=>0.0332653534, b=>-0.5739171226
Iteration 4690: Achieved Loss=> 23.1532306077
Gradients: w=>0.0332476685, b=>-0.5736120107
Iteration 4691: Achieved Loss=> 23.1529005593
Gradients: w=>0.0332299931, b=>-0.5733070609
Iteration 4692: Achieved Loss=> 23.1525708617
Gradients: w=>0.0332123270, b=>-0.5730022733
Iteration 4693: Achieved Loss=> 23.1522415146
Gradients: w=>0.0331946703, b=>-0.5726976477
Iteration 4694: Achieved Loss=> 23.1519125176
Gradients: w=>0.0331770230, b=>-0.5723931841
Iteration 4695: Achieved Loss=> 23.1515838703
Gradients: w=>0.0331593850, b=>-0.5720888823
Iteration 4696: Achieved Loss=> 23.1512555724
Gradients: w=>0.0331417565, b=>-0.5717847423
Iteration 4697: Achieved Loss=> 23.1509276234
Gradients: w=>0.0331241373, b=>-0.5714807639
Iteration 4698: Achieved Loss=> 23.1506000230
Gradients: w=>0.0331065275, b=>-0.5711769472
Iteration 4699: Achieved Loss=> 23.1502727709
Gradients: w=>0.0330889271, b=>-0.5708732920
Iteration 4700: Achieved Loss=> 23.1499458666
Gradients: w=>0.0330713360, b=>-0.5705697983
Iteration 4701: Achieved Loss=> 23.1496193099
Gradients: w=>0.0330537542, b=>-0.5702664658
Iteration 4702: Achieved Loss=> 23.1492931002
Gradients: w=>0.0330361819, b=>-0.5699632947
Iteration 4703: Achieved Loss=> 23.1489672373
Gradients: w=>0.0330186188, b=>-0.5696602847
Iteration 4704: Achieved Loss=> 23.1486417208
Gradients: w=>0.0330010651, b=>-0.5693574358
Iteration 4705: Achieved Loss=> 23.1483165503
Gradients: w=>0.0329835207, b=>-0.5690547479
Iteration 4706: Achieved Loss=> 23.1479917254
Gradients: w=>0.0329659857, b=>-0.5687522210
Iteration 4707: Achieved Loss=> 23.1476672459
Gradients: w=>0.0329484599, b=>-0.5684498548
Iteration 4708: Achieved Loss=> 23.1473431112
Gradients: w=>0.0329309435, b=>-0.5681476494
Iteration 4709: Achieved Loss=> 23.1470193211
Gradients: w=>0.0329134364, b=>-0.5678456047
Iteration 4710: Achieved Loss=> 23.1466958752
Gradients: w=>0.0328959386, b=>-0.5675437205
Iteration 4711: Achieved Loss=> 23.1463727731
Gradients: w=>0.0328784501, b=>-0.5672419969
Iteration 4712: Achieved Loss=> 23.1460500144
Gradients: w=>0.0328609709, b=>-0.5669404336
Iteration 4713: Achieved Loss=> 23.1457275988
Gradients: w=>0.0328435010, b=>-0.5666390307
Iteration 4714: Achieved Loss=> 23.1454055260
Gradients: w=>0.0328260404, b=>-0.5663377880
Iteration 4715: Achieved Loss=> 23.1450837955
Gradients: w=>0.0328085891, b=>-0.5660367054
Iteration 4716: Achieved Loss=> 23.1447624070
Gradients: w=>0.0327911470, b=>-0.5657357829
Iteration 4717: Achieved Loss=> 23.1444413601
Gradients: w=>0.0327737143, b=>-0.5654350204
Iteration 4718: Achieved Loss=> 23.1441206545
Gradients: w=>0.0327562908, b=>-0.5651344178
Iteration 4719: Achieved Loss=> 23.1438002898
Gradients: w=>0.0327388765, b=>-0.5648339750
Iteration 4720: Achieved Loss=> 23.1434802656
Gradients: w=>0.0327214715, b=>-0.5645336919
Iteration 4721: Achieved Loss=> 23.1431605816
Gradients: w=>0.0327040758, b=>-0.5642335685
Iteration 4722: Achieved Loss=> 23.1428412375
Gradients: w=>0.0326866893, b=>-0.5639336046
Iteration 4723: Achieved Loss=> 23.1425222328
Gradients: w=>0.0326693120, b=>-0.5636338002
Iteration 4724: Achieved Loss=> 23.1422035671
Gradients: w=>0.0326519440, b=>-0.5633341552
Iteration 4725: Achieved Loss=> 23.1418852403
Gradients: w=>0.0326345853, b=>-0.5630346694
Iteration 4726: Achieved Loss=> 23.1415672518
Gradients: w=>0.0326172357, b=>-0.5627353429
Iteration 4727: Achieved Loss=> 23.1412496013
Gradients: w=>0.0325998954, b=>-0.5624361756
Iteration 4728: Achieved Loss=> 23.1409322884
Gradients: w=>0.0325825643, b=>-0.5621371672
Iteration 4729: Achieved Loss=> 23.1406153129
Gradients: w=>0.0325652424, b=>-0.5618383178
Iteration 4730: Achieved Loss=> 23.1402986743
Gradients: w=>0.0325479297, b=>-0.5615396274
Iteration 4731: Achieved Loss=> 23.1399823723
Gradients: w=>0.0325306262, b=>-0.5612410957
Iteration 4732: Achieved Loss=> 23.1396664065
Gradients: w=>0.0325133319, b=>-0.5609427227
Iteration 4733: Achieved Loss=> 23.1393507765
Gradients: w=>0.0324960469, b=>-0.5606445083
Iteration 4734: Achieved Loss=> 23.1390354821
Gradients: w=>0.0324787710, b=>-0.5603464525
Iteration 4735: Achieved Loss=> 23.1387205228
Gradients: w=>0.0324615043, b=>-0.5600485551
Iteration 4736: Achieved Loss=> 23.1384058984
Gradients: w=>0.0324442467, b=>-0.5597508161
Iteration 4737: Achieved Loss=> 23.1380916083
Gradients: w=>0.0324269984, b=>-0.5594532354
Iteration 4738: Achieved Loss=> 23.1377776524
Gradients: w=>0.0324097592, b=>-0.5591558129
Iteration 4739: Achieved Loss=> 23.1374640301
Gradients: w=>0.0323925292, b=>-0.5588585485
Iteration 4740: Achieved Loss=> 23.1371507413
Gradients: w=>0.0323753083, b=>-0.5585614421
Iteration 4741: Achieved Loss=> 23.1368377854
Gradients: w=>0.0323580966, b=>-0.5582644937
Iteration 4742: Achieved Loss=> 23.1365251623
Gradients: w=>0.0323408940, b=>-0.5579677032
Iteration 4743: Achieved Loss=> 23.1362128714
Gradients: w=>0.0323237006, b=>-0.5576710704
Iteration 4744: Achieved Loss=> 23.1359009125
Gradients: w=>0.0323065163, b=>-0.5573745953
Iteration 4745: Achieved Loss=> 23.1355892852
Gradients: w=>0.0322893412, b=>-0.5570782779
Iteration 4746: Achieved Loss=> 23.1352779892
Gradients: w=>0.0322721752, b=>-0.5567821180
Iteration 4747: Achieved Loss=> 23.1349670241
Gradients: w=>0.0322550183, b=>-0.5564861155
Iteration 4748: Achieved Loss=> 23.1346563895
Gradients: w=>0.0322378706, b=>-0.5561902704
Iteration 4749: Achieved Loss=> 23.1343460851
Gradients: w=>0.0322207319, b=>-0.5558945826
Iteration 4750: Achieved Loss=> 23.1340361105
Gradients: w=>0.0322036024, b=>-0.5555990520
Iteration 4751: Achieved Loss=> 23.1337264655
Gradients: w=>0.0321864820, b=>-0.5553036785
Iteration 4752: Achieved Loss=> 23.1334171496
Gradients: w=>0.0321693706, b=>-0.5550084620
Iteration 4753: Achieved Loss=> 23.1331081625
Gradients: w=>0.0321522684, b=>-0.5547134025
Iteration 4754: Achieved Loss=> 23.1327995038
Gradients: w=>0.0321351753, b=>-0.5544184998
Iteration 4755: Achieved Loss=> 23.1324911732
Gradients: w=>0.0321180912, b=>-0.5541237539
Iteration 4756: Achieved Loss=> 23.1321831704
Gradients: w=>0.0321010163, b=>-0.5538291647
Iteration 4757: Achieved Loss=> 23.1318754950
Gradients: w=>0.0320839504, b=>-0.5535347321
Iteration 4758: Achieved Loss=> 23.1315681467
Gradients: w=>0.0320668936, b=>-0.5532404561
Iteration 4759: Achieved Loss=> 23.1312611250
Gradients: w=>0.0320498458, b=>-0.5529463364
Iteration 4760: Achieved Loss=> 23.1309544297
Gradients: w=>0.0320328072, b=>-0.5526523732
Iteration 4761: Achieved Loss=> 23.1306480604
Gradients: w=>0.0320157775, b=>-0.5523585662
Iteration 4762: Achieved Loss=> 23.1303420168
Gradients: w=>0.0319987570, b=>-0.5520649155
Iteration 4763: Achieved Loss=> 23.1300362985
Gradients: w=>0.0319817454, b=>-0.5517714208
Iteration 4764: Achieved Loss=> 23.1297309052
Gradients: w=>0.0319647430, b=>-0.5514780822
Iteration 4765: Achieved Loss=> 23.1294258365
Gradients: w=>0.0319477495, b=>-0.5511848995
Iteration 4766: Achieved Loss=> 23.1291210920
Gradients: w=>0.0319307651, b=>-0.5508918727
Iteration 4767: Achieved Loss=> 23.1288166715
Gradients: w=>0.0319137897, b=>-0.5505990017
Iteration 4768: Achieved Loss=> 23.1285125746
Gradients: w=>0.0318968234, b=>-0.5503062863
Iteration 4769: Achieved Loss=> 23.1282088010
Gradients: w=>0.0318798661, b=>-0.5500137266
Iteration 4770: Achieved Loss=> 23.1279053503
Gradients: w=>0.0318629177, b=>-0.5497213224
Iteration 4771: Achieved Loss=> 23.1276022221
Gradients: w=>0.0318459784, b=>-0.5494290737
Iteration 4772: Achieved Loss=> 23.1272994161
Gradients: w=>0.0318290481, b=>-0.5491369804
Iteration 4773: Achieved Loss=> 23.1269969320
Gradients: w=>0.0318121268, b=>-0.5488450423
Iteration 4774: Achieved Loss=> 23.1266947695
Gradients: w=>0.0317952145, b=>-0.5485532594
Iteration 4775: Achieved Loss=> 23.1263929281
Gradients: w=>0.0317783112, b=>-0.5482616317
Iteration 4776: Achieved Loss=> 23.1260914076
Gradients: w=>0.0317614169, b=>-0.5479701590
Iteration 4777: Achieved Loss=> 23.1257902076
Gradients: w=>0.0317445316, b=>-0.5476788412
Iteration 4778: Achieved Loss=> 23.1254893278
Gradients: w=>0.0317276552, b=>-0.5473876783
Iteration 4779: Achieved Loss=> 23.1251887678
Gradients: w=>0.0317107878, b=>-0.5470966702
Iteration 4780: Achieved Loss=> 23.1248885273
Gradients: w=>0.0316939294, b=>-0.5468058168
Iteration 4781: Achieved Loss=> 23.1245886059
Gradients: w=>0.0316770799, b=>-0.5465151181
Iteration 4782: Achieved Loss=> 23.1242890034
Gradients: w=>0.0316602394, b=>-0.5462245739
Iteration 4783: Achieved Loss=> 23.1239897193
Gradients: w=>0.0316434078, b=>-0.5459341841
Iteration 4784: Achieved Loss=> 23.1236907533
Gradients: w=>0.0316265852, b=>-0.5456439488
Iteration 4785: Achieved Loss=> 23.1233921052
Gradients: w=>0.0316097716, b=>-0.5453538677
Iteration 4786: Achieved Loss=> 23.1230937745
Gradients: w=>0.0315929668, b=>-0.5450639408
Iteration 4787: Achieved Loss=> 23.1227957609
Gradients: w=>0.0315761711, b=>-0.5447741681
Iteration 4788: Achieved Loss=> 23.1224980641
Gradients: w=>0.0315593842, b=>-0.5444845494
Iteration 4789: Achieved Loss=> 23.1222006838
Gradients: w=>0.0315426063, b=>-0.5441950847
Iteration 4790: Achieved Loss=> 23.1219036195
Gradients: w=>0.0315258372, b=>-0.5439057739
Iteration 4791: Achieved Loss=> 23.1216068711
Gradients: w=>0.0315090771, b=>-0.5436166169
Iteration 4792: Achieved Loss=> 23.1213104380
Gradients: w=>0.0314923259, b=>-0.5433276136
Iteration 4793: Achieved Loss=> 23.1210143201
Gradients: w=>0.0314755837, b=>-0.5430387640
Iteration 4794: Achieved Loss=> 23.1207185169
Gradients: w=>0.0314588503, b=>-0.5427500679
Iteration 4795: Achieved Loss=> 23.1204230282
Gradients: w=>0.0314421258, b=>-0.5424615253
Iteration 4796: Achieved Loss=> 23.1201278536
Gradients: w=>0.0314254102, b=>-0.5421731361
Iteration 4797: Achieved Loss=> 23.1198329927
Gradients: w=>0.0314087035, b=>-0.5418849003
Iteration 4798: Achieved Loss=> 23.1195384453
Gradients: w=>0.0313920056, b=>-0.5415968176
Iteration 4799: Achieved Loss=> 23.1192442109
Gradients: w=>0.0313753167, b=>-0.5413088881
Iteration 4800: Achieved Loss=> 23.1189502894
Gradients: w=>0.0313586366, b=>-0.5410211117
Iteration 4801: Achieved Loss=> 23.1186566802
Gradients: w=>0.0313419654, b=>-0.5407334883
Iteration 4802: Achieved Loss=> 23.1183633832
Gradients: w=>0.0313253030, b=>-0.5404460177
Iteration 4803: Achieved Loss=> 23.1180703979
Gradients: w=>0.0313086495, b=>-0.5401587001
Iteration 4804: Achieved Loss=> 23.1177777241
Gradients: w=>0.0312920049, b=>-0.5398715351
Iteration 4805: Achieved Loss=> 23.1174853613
Gradients: w=>0.0312753691, b=>-0.5395845228
Iteration 4806: Achieved Loss=> 23.1171933094
Gradients: w=>0.0312587422, b=>-0.5392976631
Iteration 4807: Achieved Loss=> 23.1169015679
Gradients: w=>0.0312421241, b=>-0.5390109559
Iteration 4808: Achieved Loss=> 23.1166101365
Gradients: w=>0.0312255148, b=>-0.5387244012
Iteration 4809: Achieved Loss=> 23.1163190149
Gradients: w=>0.0312089143, b=>-0.5384379987
Iteration 4810: Achieved Loss=> 23.1160282027
Gradients: w=>0.0311923227, b=>-0.5381517486
Iteration 4811: Achieved Loss=> 23.1157376997
Gradients: w=>0.0311757399, b=>-0.5378656506
Iteration 4812: Achieved Loss=> 23.1154475055
Gradients: w=>0.0311591660, b=>-0.5375797047
Iteration 4813: Achieved Loss=> 23.1151576197
Gradients: w=>0.0311426008, b=>-0.5372939108
Iteration 4814: Achieved Loss=> 23.1148680421
Gradients: w=>0.0311260444, b=>-0.5370082689
Iteration 4815: Achieved Loss=> 23.1145787723
Gradients: w=>0.0311094969, b=>-0.5367227788
Iteration 4816: Achieved Loss=> 23.1142898100
Gradients: w=>0.0310929581, b=>-0.5364374405
Iteration 4817: Achieved Loss=> 23.1140011548
Gradients: w=>0.0310764281, b=>-0.5361522539
Iteration 4818: Achieved Loss=> 23.1137128065
Gradients: w=>0.0310599069, b=>-0.5358672189
Iteration 4819: Achieved Loss=> 23.1134247647
Gradients: w=>0.0310433945, b=>-0.5355823354
Iteration 4820: Achieved Loss=> 23.1131370291
Gradients: w=>0.0310268909, b=>-0.5352976034
Iteration 4821: Achieved Loss=> 23.1128495993
Gradients: w=>0.0310103961, b=>-0.5350130228
Iteration 4822: Achieved Loss=> 23.1125624751
Gradients: w=>0.0309939100, b=>-0.5347285935
Iteration 4823: Achieved Loss=> 23.1122756561
Gradients: w=>0.0309774327, b=>-0.5344443153
Iteration 4824: Achieved Loss=> 23.1119891419
Gradients: w=>0.0309609641, b=>-0.5341601883
Iteration 4825: Achieved Loss=> 23.1117029323
Gradients: w=>0.0309445043, b=>-0.5338762124
Iteration 4826: Achieved Loss=> 23.1114170270
Gradients: w=>0.0309280533, b=>-0.5335923874
Iteration 4827: Achieved Loss=> 23.1111314255
Gradients: w=>0.0309116110, b=>-0.5333087133
Iteration 4828: Achieved Loss=> 23.1108461277
Gradients: w=>0.0308951774, b=>-0.5330251900
Iteration 4829: Achieved Loss=> 23.1105611331
Gradients: w=>0.0308787526, b=>-0.5327418175
Iteration 4830: Achieved Loss=> 23.1102764414
Gradients: w=>0.0308623365, b=>-0.5324585956
Iteration 4831: Achieved Loss=> 23.1099920524
Gradients: w=>0.0308459291, b=>-0.5321755242
Iteration 4832: Achieved Loss=> 23.1097079657
Gradients: w=>0.0308295305, b=>-0.5318926034
Iteration 4833: Achieved Loss=> 23.1094241809
Gradients: w=>0.0308131406, b=>-0.5316098329
Iteration 4834: Achieved Loss=> 23.1091406978
Gradients: w=>0.0307967594, b=>-0.5313272128
Iteration 4835: Achieved Loss=> 23.1088575161
Gradients: w=>0.0307803869, b=>-0.5310447430
Iteration 4836: Achieved Loss=> 23.1085746354
Gradients: w=>0.0307640231, b=>-0.5307624233
Iteration 4837: Achieved Loss=> 23.1082920553
Gradients: w=>0.0307476680, b=>-0.5304802537
Iteration 4838: Achieved Loss=> 23.1080097757
Gradients: w=>0.0307313216, b=>-0.5301982341
Iteration 4839: Achieved Loss=> 23.1077277960
Gradients: w=>0.0307149838, b=>-0.5299163645
Iteration 4840: Achieved Loss=> 23.1074461162
Gradients: w=>0.0306986548, b=>-0.5296346447
Iteration 4841: Achieved Loss=> 23.1071647357
Gradients: w=>0.0306823345, b=>-0.5293530746
Iteration 4842: Achieved Loss=> 23.1068836544
Gradients: w=>0.0306660228, b=>-0.5290716543
Iteration 4843: Achieved Loss=> 23.1066028718
Gradients: w=>0.0306497198, b=>-0.5287903835
Iteration 4844: Achieved Loss=> 23.1063223877
Gradients: w=>0.0306334255, b=>-0.5285092623
Iteration 4845: Achieved Loss=> 23.1060422018
Gradients: w=>0.0306171398, b=>-0.5282282906
Iteration 4846: Achieved Loss=> 23.1057623137
Gradients: w=>0.0306008628, b=>-0.5279474682
Iteration 4847: Achieved Loss=> 23.1054827230
Gradients: w=>0.0305845944, b=>-0.5276667951
Iteration 4848: Achieved Loss=> 23.1052034296
Gradients: w=>0.0305683347, b=>-0.5273862713
Iteration 4849: Achieved Loss=> 23.1049244331
Gradients: w=>0.0305520836, b=>-0.5271058966
Iteration 4850: Achieved Loss=> 23.1046457332
Gradients: w=>0.0305358412, b=>-0.5268256709
Iteration 4851: Achieved Loss=> 23.1043673295
Gradients: w=>0.0305196074, b=>-0.5265455942
Iteration 4852: Achieved Loss=> 23.1040892217
Gradients: w=>0.0305033823, b=>-0.5262656664
Iteration 4853: Achieved Loss=> 23.1038114096
Gradients: w=>0.0304871657, b=>-0.5259858874
Iteration 4854: Achieved Loss=> 23.1035338927
Gradients: w=>0.0304709578, b=>-0.5257062572
Iteration 4855: Achieved Loss=> 23.1032566709
Gradients: w=>0.0304547585, b=>-0.5254267756
Iteration 4856: Achieved Loss=> 23.1029797437
Gradients: w=>0.0304385678, b=>-0.5251474426
Iteration 4857: Achieved Loss=> 23.1027031109
Gradients: w=>0.0304223858, b=>-0.5248682581
Iteration 4858: Achieved Loss=> 23.1024267722
Gradients: w=>0.0304062123, b=>-0.5245892220
Iteration 4859: Achieved Loss=> 23.1021507272
Gradients: w=>0.0303900474, b=>-0.5243103343
Iteration 4860: Achieved Loss=> 23.1018749757
Gradients: w=>0.0303738911, b=>-0.5240315948
Iteration 4861: Achieved Loss=> 23.1015995172
Gradients: w=>0.0303577434, b=>-0.5237530035
Iteration 4862: Achieved Loss=> 23.1013243516
Gradients: w=>0.0303416043, b=>-0.5234745603
Iteration 4863: Achieved Loss=> 23.1010494784
Gradients: w=>0.0303254738, b=>-0.5231962652
Iteration 4864: Achieved Loss=> 23.1007748975
Gradients: w=>0.0303093519, b=>-0.5229181180
Iteration 4865: Achieved Loss=> 23.1005006084
Gradients: w=>0.0302932385, b=>-0.5226401187
Iteration 4866: Achieved Loss=> 23.1002266109
Gradients: w=>0.0302771337, b=>-0.5223622672
Iteration 4867: Achieved Loss=> 23.0999529046
Gradients: w=>0.0302610374, b=>-0.5220845633
Iteration 4868: Achieved Loss=> 23.0996794893
Gradients: w=>0.0302449497, b=>-0.5218070072
Iteration 4869: Achieved Loss=> 23.0994063646
Gradients: w=>0.0302288706, b=>-0.5215295985
Iteration 4870: Achieved Loss=> 23.0991335302
Gradients: w=>0.0302128000, b=>-0.5212523374
Iteration 4871: Achieved Loss=> 23.0988609859
Gradients: w=>0.0301967379, b=>-0.5209752237
Iteration 4872: Achieved Loss=> 23.0985887312
Gradients: w=>0.0301806844, b=>-0.5206982572
Iteration 4873: Achieved Loss=> 23.0983167660
Gradients: w=>0.0301646394, b=>-0.5204214381
Iteration 4874: Achieved Loss=> 23.0980450898
Gradients: w=>0.0301486030, b=>-0.5201447660
Iteration 4875: Achieved Loss=> 23.0977737025
Gradients: w=>0.0301325751, b=>-0.5198682411
Iteration 4876: Achieved Loss=> 23.0975026036
Gradients: w=>0.0301165557, b=>-0.5195918632
Iteration 4877: Achieved Loss=> 23.0972317929
Gradients: w=>0.0301005448, b=>-0.5193156322
Iteration 4878: Achieved Loss=> 23.0969612701
Gradients: w=>0.0300845424, b=>-0.5190395481
Iteration 4879: Achieved Loss=> 23.0966910348
Gradients: w=>0.0300685485, b=>-0.5187636107
Iteration 4880: Achieved Loss=> 23.0964210868
Gradients: w=>0.0300525632, b=>-0.5184878201
Iteration 4881: Achieved Loss=> 23.0961514257
Gradients: w=>0.0300365863, b=>-0.5182121760
Iteration 4882: Achieved Loss=> 23.0958820512
Gradients: w=>0.0300206179, b=>-0.5179366785
Iteration 4883: Achieved Loss=> 23.0956129632
Gradients: w=>0.0300046580, b=>-0.5176613275
Iteration 4884: Achieved Loss=> 23.0953441611
Gradients: w=>0.0299887066, b=>-0.5173861229
Iteration 4885: Achieved Loss=> 23.0950756448
Gradients: w=>0.0299727637, b=>-0.5171110645
Iteration 4886: Achieved Loss=> 23.0948074139
Gradients: w=>0.0299568293, b=>-0.5168361524
Iteration 4887: Achieved Loss=> 23.0945394681
Gradients: w=>0.0299409033, b=>-0.5165613864
Iteration 4888: Achieved Loss=> 23.0942718071
Gradients: w=>0.0299249858, b=>-0.5162867665
Iteration 4889: Achieved Loss=> 23.0940044307
Gradients: w=>0.0299090768, b=>-0.5160122927
Iteration 4890: Achieved Loss=> 23.0937373385
Gradients: w=>0.0298931762, b=>-0.5157379647
Iteration 4891: Achieved Loss=> 23.0934705302
Gradients: w=>0.0298772841, b=>-0.5154637825
Iteration 4892: Achieved Loss=> 23.0932040055
Gradients: w=>0.0298614004, b=>-0.5151897462
Iteration 4893: Achieved Loss=> 23.0929377641
Gradients: w=>0.0298455251, b=>-0.5149158555
Iteration 4894: Achieved Loss=> 23.0926718057
Gradients: w=>0.0298296583, b=>-0.5146421104
Iteration 4895: Achieved Loss=> 23.0924061300
Gradients: w=>0.0298138000, b=>-0.5143685109
Iteration 4896: Achieved Loss=> 23.0921407368
Gradients: w=>0.0297979501, b=>-0.5140950568
Iteration 4897: Achieved Loss=> 23.0918756256
Gradients: w=>0.0297821085, b=>-0.5138217481
Iteration 4898: Achieved Loss=> 23.0916107963
Gradients: w=>0.0297662755, b=>-0.5135485847
Iteration 4899: Achieved Loss=> 23.0913462484
Gradients: w=>0.0297504508, b=>-0.5132755665
Iteration 4900: Achieved Loss=> 23.0910819818
Gradients: w=>0.0297346345, b=>-0.5130026934
Iteration 4901: Achieved Loss=> 23.0908179961
Gradients: w=>0.0297188267, b=>-0.5127299654
Iteration 4902: Achieved Loss=> 23.0905542910
Gradients: w=>0.0297030273, b=>-0.5124573825
Iteration 4903: Achieved Loss=> 23.0902908662
Gradients: w=>0.0296872362, b=>-0.5121849444
Iteration 4904: Achieved Loss=> 23.0900277214
Gradients: w=>0.0296714536, b=>-0.5119126511
Iteration 4905: Achieved Loss=> 23.0897648563
Gradients: w=>0.0296556793, b=>-0.5116405027
Iteration 4906: Achieved Loss=> 23.0895022707
Gradients: w=>0.0296399135, b=>-0.5113684989
Iteration 4907: Achieved Loss=> 23.0892399641
Gradients: w=>0.0296241560, b=>-0.5110966397
Iteration 4908: Achieved Loss=> 23.0889779364
Gradients: w=>0.0296084069, b=>-0.5108249250
Iteration 4909: Achieved Loss=> 23.0887161873
Gradients: w=>0.0295926661, b=>-0.5105533548
Iteration 4910: Achieved Loss=> 23.0884547163
Gradients: w=>0.0295769337, b=>-0.5102819290
Iteration 4911: Achieved Loss=> 23.0881935233
Gradients: w=>0.0295612097, b=>-0.5100106474
Iteration 4912: Achieved Loss=> 23.0879326080
Gradients: w=>0.0295454941, b=>-0.5097395101
Iteration 4913: Achieved Loss=> 23.0876719700
Gradients: w=>0.0295297868, b=>-0.5094685170
Iteration 4914: Achieved Loss=> 23.0874116090
Gradients: w=>0.0295140879, b=>-0.5091976679
Iteration 4915: Achieved Loss=> 23.0871515248
Gradients: w=>0.0294983973, b=>-0.5089269627
Iteration 4916: Achieved Loss=> 23.0868917171
Gradients: w=>0.0294827150, b=>-0.5086564015
Iteration 4917: Achieved Loss=> 23.0866321856
Gradients: w=>0.0294670411, b=>-0.5083859842
Iteration 4918: Achieved Loss=> 23.0863729299
Gradients: w=>0.0294513755, b=>-0.5081157106
Iteration 4919: Achieved Loss=> 23.0861139498
Gradients: w=>0.0294357183, b=>-0.5078455807
Iteration 4920: Achieved Loss=> 23.0858552450
Gradients: w=>0.0294200693, b=>-0.5075755944
Iteration 4921: Achieved Loss=> 23.0855968152
Gradients: w=>0.0294044287, b=>-0.5073057516
Iteration 4922: Achieved Loss=> 23.0853386600
Gradients: w=>0.0293887964, b=>-0.5070360523
Iteration 4923: Achieved Loss=> 23.0850807794
Gradients: w=>0.0293731724, b=>-0.5067664964
Iteration 4924: Achieved Loss=> 23.0848231728
Gradients: w=>0.0293575567, b=>-0.5064970838
Iteration 4925: Achieved Loss=> 23.0845658400
Gradients: w=>0.0293419494, b=>-0.5062278144
Iteration 4926: Achieved Loss=> 23.0843087808
Gradients: w=>0.0293263503, b=>-0.5059586881
Iteration 4927: Achieved Loss=> 23.0840519949
Gradients: w=>0.0293107595, b=>-0.5056897050
Iteration 4928: Achieved Loss=> 23.0837954819
Gradients: w=>0.0292951770, b=>-0.5054208648
Iteration 4929: Achieved Loss=> 23.0835392416
Gradients: w=>0.0292796028, b=>-0.5051521675
Iteration 4930: Achieved Loss=> 23.0832832736
Gradients: w=>0.0292640369, b=>-0.5048836131
Iteration 4931: Achieved Loss=> 23.0830275778
Gradients: w=>0.0292484792, b=>-0.5046152015
Iteration 4932: Achieved Loss=> 23.0827721537
Gradients: w=>0.0292329298, b=>-0.5043469326
Iteration 4933: Achieved Loss=> 23.0825170011
Gradients: w=>0.0292173887, b=>-0.5040788063
Iteration 4934: Achieved Loss=> 23.0822621198
Gradients: w=>0.0292018558, b=>-0.5038108225
Iteration 4935: Achieved Loss=> 23.0820075094
Gradients: w=>0.0291863312, b=>-0.5035429812
Iteration 4936: Achieved Loss=> 23.0817531697
Gradients: w=>0.0291708149, b=>-0.5032752823
Iteration 4937: Achieved Loss=> 23.0814991003
Gradients: w=>0.0291553068, b=>-0.5030077257
Iteration 4938: Achieved Loss=> 23.0812453010
Gradients: w=>0.0291398069, b=>-0.5027403114
Iteration 4939: Achieved Loss=> 23.0809917714
Gradients: w=>0.0291243153, b=>-0.5024730392
Iteration 4940: Achieved Loss=> 23.0807385114
Gradients: w=>0.0291088319, b=>-0.5022059091
Iteration 4941: Achieved Loss=> 23.0804855205
Gradients: w=>0.0290933568, b=>-0.5019389210
Iteration 4942: Achieved Loss=> 23.0802327986
Gradients: w=>0.0290778898, b=>-0.5016720749
Iteration 4943: Achieved Loss=> 23.0799803454
Gradients: w=>0.0290624311, b=>-0.5014053706
Iteration 4944: Achieved Loss=> 23.0797281604
Gradients: w=>0.0290469807, b=>-0.5011388081
Iteration 4945: Achieved Loss=> 23.0794762436
Gradients: w=>0.0290315384, b=>-0.5008723874
Iteration 4946: Achieved Loss=> 23.0792245945
Gradients: w=>0.0290161043, b=>-0.5006061082
Iteration 4947: Achieved Loss=> 23.0789732130
Gradients: w=>0.0290006785, b=>-0.5003399706
Iteration 4948: Achieved Loss=> 23.0787220986
Gradients: w=>0.0289852608, b=>-0.5000739746
Iteration 4949: Achieved Loss=> 23.0784712512
Gradients: w=>0.0289698514, b=>-0.4998081199
Iteration 4950: Achieved Loss=> 23.0782206704
Gradients: w=>0.0289544501, b=>-0.4995424065
Iteration 4951: Achieved Loss=> 23.0779703560
Gradients: w=>0.0289390570, b=>-0.4992768345
Iteration 4952: Achieved Loss=> 23.0777203076
Gradients: w=>0.0289236722, b=>-0.4990114036
Iteration 4953: Achieved Loss=> 23.0774705251
Gradients: w=>0.0289082954, b=>-0.4987461138
Iteration 4954: Achieved Loss=> 23.0772210080
Gradients: w=>0.0288929269, b=>-0.4984809651
Iteration 4955: Achieved Loss=> 23.0769717562
Gradients: w=>0.0288775665, b=>-0.4982159573
Iteration 4956: Achieved Loss=> 23.0767227694
Gradients: w=>0.0288622143, b=>-0.4979510904
Iteration 4957: Achieved Loss=> 23.0764740472
Gradients: w=>0.0288468703, b=>-0.4976863643
Iteration 4958: Achieved Loss=> 23.0762255894
Gradients: w=>0.0288315344, b=>-0.4974217789
Iteration 4959: Achieved Loss=> 23.0759773957
Gradients: w=>0.0288162067, b=>-0.4971573343
Iteration 4960: Achieved Loss=> 23.0757294658
Gradients: w=>0.0288008871, b=>-0.4968930302
Iteration 4961: Achieved Loss=> 23.0754817995
Gradients: w=>0.0287855757, b=>-0.4966288666
Iteration 4962: Achieved Loss=> 23.0752343965
Gradients: w=>0.0287702724, b=>-0.4963648434
Iteration 4963: Achieved Loss=> 23.0749872564
Gradients: w=>0.0287549772, b=>-0.4961009606
Iteration 4964: Achieved Loss=> 23.0747403790
Gradients: w=>0.0287396902, b=>-0.4958372181
Iteration 4965: Achieved Loss=> 23.0744937641
Gradients: w=>0.0287244113, b=>-0.4955736159
Iteration 4966: Achieved Loss=> 23.0742474113
Gradients: w=>0.0287091405, b=>-0.4953101537
Iteration 4967: Achieved Loss=> 23.0740013203
Gradients: w=>0.0286938779, b=>-0.4950468316
Iteration 4968: Achieved Loss=> 23.0737554910
Gradients: w=>0.0286786233, b=>-0.4947836495
Iteration 4969: Achieved Loss=> 23.0735099230
Gradients: w=>0.0286633769, b=>-0.4945206074
Iteration 4970: Achieved Loss=> 23.0732646160
Gradients: w=>0.0286481385, b=>-0.4942577050
Iteration 4971: Achieved Loss=> 23.0730195697
Gradients: w=>0.0286329083, b=>-0.4939949425
Iteration 4972: Achieved Loss=> 23.0727747840
Gradients: w=>0.0286176862, b=>-0.4937323196
Iteration 4973: Achieved Loss=> 23.0725302584
Gradients: w=>0.0286024721, b=>-0.4934698363
Iteration 4974: Achieved Loss=> 23.0722859928
Gradients: w=>0.0285872662, b=>-0.4932074926
Iteration 4975: Achieved Loss=> 23.0720419868
Gradients: w=>0.0285720683, b=>-0.4929452884
Iteration 4976: Achieved Loss=> 23.0717982402
Gradients: w=>0.0285568785, b=>-0.4926832235
Iteration 4977: Achieved Loss=> 23.0715547527
Gradients: w=>0.0285416968, b=>-0.4924212980
Iteration 4978: Achieved Loss=> 23.0713115240
Gradients: w=>0.0285265232, b=>-0.4921595117
Iteration 4979: Achieved Loss=> 23.0710685538
Gradients: w=>0.0285113576, b=>-0.4918978646
Iteration 4980: Achieved Loss=> 23.0708258420
Gradients: w=>0.0284962001, b=>-0.4916363566
Iteration 4981: Achieved Loss=> 23.0705833881
Gradients: w=>0.0284810506, b=>-0.4913749876
Iteration 4982: Achieved Loss=> 23.0703411919
Gradients: w=>0.0284659092, b=>-0.4911137576
Iteration 4983: Achieved Loss=> 23.0700992532
Gradients: w=>0.0284507759, b=>-0.4908526665
Iteration 4984: Achieved Loss=> 23.0698575717
Gradients: w=>0.0284356506, b=>-0.4905917141
Iteration 4985: Achieved Loss=> 23.0696161471
Gradients: w=>0.0284205333, b=>-0.4903309005
Iteration 4986: Achieved Loss=> 23.0693749791
Gradients: w=>0.0284054241, b=>-0.4900702256
Iteration 4987: Achieved Loss=> 23.0691340674
Gradients: w=>0.0283903229, b=>-0.4898096892
Iteration 4988: Achieved Loss=> 23.0688934119
Gradients: w=>0.0283752297, b=>-0.4895492913
Iteration 4989: Achieved Loss=> 23.0686530121
Gradients: w=>0.0283601446, b=>-0.4892890319
Iteration 4990: Achieved Loss=> 23.0684128679
Gradients: w=>0.0283450674, b=>-0.4890289108
Iteration 4991: Achieved Loss=> 23.0681729790
Gradients: w=>0.0283299983, b=>-0.4887689280
Iteration 4992: Achieved Loss=> 23.0679333451
Gradients: w=>0.0283149372, b=>-0.4885090835
Iteration 4993: Achieved Loss=> 23.0676939659
Gradients: w=>0.0282998842, b=>-0.4882493771
Iteration 4994: Achieved Loss=> 23.0674548411
Gradients: w=>0.0282848391, b=>-0.4879898087
Iteration 4995: Achieved Loss=> 23.0672159705
Gradients: w=>0.0282698020, b=>-0.4877303783
Iteration 4996: Achieved Loss=> 23.0669773539
Gradients: w=>0.0282547729, b=>-0.4874710859
Iteration 4997: Achieved Loss=> 23.0667389908
Gradients: w=>0.0282397518, b=>-0.4872119313
Iteration 4998: Achieved Loss=> 23.0665008812
Gradients: w=>0.0282247387, b=>-0.4869529145
Iteration 4999: Achieved Loss=> 23.0662630247
Gradients: w=>0.0282097335, b=>-0.4866940354
Iteration 5000: Achieved Loss=> 23.0660254210
Gradients: w=>0.0281947364, b=>-0.4864352939
Iteration 5001: Achieved Loss=> 23.0657880698
Gradients: w=>0.0281797472, b=>-0.4861766899
Iteration 5002: Achieved Loss=> 23.0655509710
Gradients: w=>0.0281647660, b=>-0.4859182235
Iteration 5003: Achieved Loss=> 23.0653141242
Gradients: w=>0.0281497927, b=>-0.4856598945
Iteration 5004: Achieved Loss=> 23.0650775292
Gradients: w=>0.0281348274, b=>-0.4854017027
Iteration 5005: Achieved Loss=> 23.0648411856
Gradients: w=>0.0281198701, b=>-0.4851436483
Iteration 5006: Achieved Loss=> 23.0646050933
Gradients: w=>0.0281049207, b=>-0.4848857310
Iteration 5007: Achieved Loss=> 23.0643692520
Gradients: w=>0.0280899792, b=>-0.4846279509
Iteration 5008: Achieved Loss=> 23.0641336613
Gradients: w=>0.0280750458, b=>-0.4843703078
Iteration 5009: Achieved Loss=> 23.0638983211
Gradients: w=>0.0280601202, b=>-0.4841128017
Iteration 5010: Achieved Loss=> 23.0636632310
Gradients: w=>0.0280452026, b=>-0.4838554325
Iteration 5011: Achieved Loss=> 23.0634283908
Gradients: w=>0.0280302929, b=>-0.4835982001
Iteration 5012: Achieved Loss=> 23.0631938003
Gradients: w=>0.0280153911, b=>-0.4833411044
Iteration 5013: Achieved Loss=> 23.0629594591
Gradients: w=>0.0280004973, b=>-0.4830841455
Iteration 5014: Achieved Loss=> 23.0627253671
Gradients: w=>0.0279856114, b=>-0.4828273231
Iteration 5015: Achieved Loss=> 23.0624915238
Gradients: w=>0.0279707333, b=>-0.4825706373
Iteration 5016: Achieved Loss=> 23.0622579291
Gradients: w=>0.0279558632, b=>-0.4823140879
Iteration 5017: Achieved Loss=> 23.0620245828
Gradients: w=>0.0279410010, b=>-0.4820576749
Iteration 5018: Achieved Loss=> 23.0617914844
Gradients: w=>0.0279261468, b=>-0.4818013983
Iteration 5019: Achieved Loss=> 23.0615586339
Gradients: w=>0.0279113004, b=>-0.4815452579
Iteration 5020: Achieved Loss=> 23.0613260309
Gradients: w=>0.0278964619, b=>-0.4812892536
Iteration 5021: Achieved Loss=> 23.0610936751
Gradients: w=>0.0278816312, b=>-0.4810333855
Iteration 5022: Achieved Loss=> 23.0608615663
Gradients: w=>0.0278668085, b=>-0.4807776534
Iteration 5023: Achieved Loss=> 23.0606297042
Gradients: w=>0.0278519937, b=>-0.4805220572
Iteration 5024: Achieved Loss=> 23.0603980886
Gradients: w=>0.0278371867, b=>-0.4802665969
Iteration 5025: Achieved Loss=> 23.0601667192
Gradients: w=>0.0278223876, b=>-0.4800112725
Iteration 5026: Achieved Loss=> 23.0599355958
Gradients: w=>0.0278075963, b=>-0.4797560837
Iteration 5027: Achieved Loss=> 23.0597047180
Gradients: w=>0.0277928130, b=>-0.4795010307
Iteration 5028: Achieved Loss=> 23.0594740857
Gradients: w=>0.0277780375, b=>-0.4792461132
Iteration 5029: Achieved Loss=> 23.0592436985
Gradients: w=>0.0277632698, b=>-0.4789913313
Iteration 5030: Achieved Loss=> 23.0590135562
Gradients: w=>0.0277485100, b=>-0.4787366848
Iteration 5031: Achieved Loss=> 23.0587836585
Gradients: w=>0.0277337580, b=>-0.4784821737
Iteration 5032: Achieved Loss=> 23.0585540052
Gradients: w=>0.0277190139, b=>-0.4782277979
Iteration 5033: Achieved Loss=> 23.0583245960
Gradients: w=>0.0277042776, b=>-0.4779735573
Iteration 5034: Achieved Loss=> 23.0580954307
Gradients: w=>0.0276895492, b=>-0.4777194519
Iteration 5035: Achieved Loss=> 23.0578665090
Gradients: w=>0.0276748286, b=>-0.4774654815
Iteration 5036: Achieved Loss=> 23.0576378306
Gradients: w=>0.0276601158, b=>-0.4772116462
Iteration 5037: Achieved Loss=> 23.0574093953
Gradients: w=>0.0276454108, b=>-0.4769579459
Iteration 5038: Achieved Loss=> 23.0571812028
Gradients: w=>0.0276307137, b=>-0.4767043804
Iteration 5039: Achieved Loss=> 23.0569532529
Gradients: w=>0.0276160243, b=>-0.4764509497
Iteration 5040: Achieved Loss=> 23.0567255453
Gradients: w=>0.0276013428, b=>-0.4761976538
Iteration 5041: Achieved Loss=> 23.0564980798
Gradients: w=>0.0275866691, b=>-0.4759444925
Iteration 5042: Achieved Loss=> 23.0562708560
Gradients: w=>0.0275720032, b=>-0.4756914658
Iteration 5043: Achieved Loss=> 23.0560438738
Gradients: w=>0.0275573451, b=>-0.4754385736
Iteration 5044: Achieved Loss=> 23.0558171328
Gradients: w=>0.0275426947, b=>-0.4751858158
Iteration 5045: Achieved Loss=> 23.0555906329
Gradients: w=>0.0275280522, b=>-0.4749331925
Iteration 5046: Achieved Loss=> 23.0553643737
Gradients: w=>0.0275134174, b=>-0.4746807034
Iteration 5047: Achieved Loss=> 23.0551383550
Gradients: w=>0.0274987904, b=>-0.4744283486
Iteration 5048: Achieved Loss=> 23.0549125766
Gradients: w=>0.0274841712, b=>-0.4741761279
Iteration 5049: Achieved Loss=> 23.0546870382
Gradients: w=>0.0274695598, b=>-0.4739240413
Iteration 5050: Achieved Loss=> 23.0544617395
Gradients: w=>0.0274549562, b=>-0.4736720888
Iteration 5051: Achieved Loss=> 23.0542366803
Gradients: w=>0.0274403603, b=>-0.4734202701
Iteration 5052: Achieved Loss=> 23.0540118604
Gradients: w=>0.0274257721, b=>-0.4731685854
Iteration 5053: Achieved Loss=> 23.0537872794
Gradients: w=>0.0274111917, b=>-0.4729170344
Iteration 5054: Achieved Loss=> 23.0535629372
Gradients: w=>0.0273966191, b=>-0.4726656172
Iteration 5055: Achieved Loss=> 23.0533388334
Gradients: w=>0.0273820542, b=>-0.4724143337
Iteration 5056: Achieved Loss=> 23.0531149679
Gradients: w=>0.0273674971, b=>-0.4721631837
Iteration 5057: Achieved Loss=> 23.0528913403
Gradients: w=>0.0273529477, b=>-0.4719121673
Iteration 5058: Achieved Loss=> 23.0526679504
Gradients: w=>0.0273384060, b=>-0.4716612843
Iteration 5059: Achieved Loss=> 23.0524447980
Gradients: w=>0.0273238721, b=>-0.4714105347
Iteration 5060: Achieved Loss=> 23.0522218827
Gradients: w=>0.0273093459, b=>-0.4711599184
Iteration 5061: Achieved Loss=> 23.0519992045
Gradients: w=>0.0272948274, b=>-0.4709094353
Iteration 5062: Achieved Loss=> 23.0517767629
Gradients: w=>0.0272803166, b=>-0.4706590854
Iteration 5063: Achieved Loss=> 23.0515545578
Gradients: w=>0.0272658136, b=>-0.4704088686
Iteration 5064: Achieved Loss=> 23.0513325889
Gradients: w=>0.0272513182, b=>-0.4701587848
Iteration 5065: Achieved Loss=> 23.0511108559
Gradients: w=>0.0272368306, b=>-0.4699088339
Iteration 5066: Achieved Loss=> 23.0508893587
Gradients: w=>0.0272223506, b=>-0.4696590160
Iteration 5067: Achieved Loss=> 23.0506680969
Gradients: w=>0.0272078784, b=>-0.4694093308
Iteration 5068: Achieved Loss=> 23.0504470702
Gradients: w=>0.0271934139, b=>-0.4691597784
Iteration 5069: Achieved Loss=> 23.0502262786
Gradients: w=>0.0271789570, b=>-0.4689103587
Iteration 5070: Achieved Loss=> 23.0500057216
Gradients: w=>0.0271645078, b=>-0.4686610715
Iteration 5071: Achieved Loss=> 23.0497853991
Gradients: w=>0.0271500664, b=>-0.4684119169
Iteration 5072: Achieved Loss=> 23.0495653107
Gradients: w=>0.0271356325, b=>-0.4681628948
Iteration 5073: Achieved Loss=> 23.0493454563
Gradients: w=>0.0271212064, b=>-0.4679140050
Iteration 5074: Achieved Loss=> 23.0491258357
Gradients: w=>0.0271067879, b=>-0.4676652476
Iteration 5075: Achieved Loss=> 23.0489064484
Gradients: w=>0.0270923771, b=>-0.4674166224
Iteration 5076: Achieved Loss=> 23.0486872944
Gradients: w=>0.0270779740, b=>-0.4671681293
Iteration 5077: Achieved Loss=> 23.0484683733
Gradients: w=>0.0270635785, b=>-0.4669197684
Iteration 5078: Achieved Loss=> 23.0482496850
Gradients: w=>0.0270491907, b=>-0.4666715395
Iteration 5079: Achieved Loss=> 23.0480312291
Gradients: w=>0.0270348105, b=>-0.4664234426
Iteration 5080: Achieved Loss=> 23.0478130054
Gradients: w=>0.0270204380, b=>-0.4661754776
Iteration 5081: Achieved Loss=> 23.0475950137
Gradients: w=>0.0270060731, b=>-0.4659276444
Iteration 5082: Achieved Loss=> 23.0473772537
Gradients: w=>0.0269917158, b=>-0.4656799430
Iteration 5083: Achieved Loss=> 23.0471597251
Gradients: w=>0.0269773662, b=>-0.4654323732
Iteration 5084: Achieved Loss=> 23.0469424278
Gradients: w=>0.0269630242, b=>-0.4651849351
Iteration 5085: Achieved Loss=> 23.0467253615
Gradients: w=>0.0269486898, b=>-0.4649376285
Iteration 5086: Achieved Loss=> 23.0465085260
Gradients: w=>0.0269343631, b=>-0.4646904534
Iteration 5087: Achieved Loss=> 23.0462919209
Gradients: w=>0.0269200440, b=>-0.4644434097
Iteration 5088: Achieved Loss=> 23.0460755460
Gradients: w=>0.0269057324, b=>-0.4641964973
Iteration 5089: Achieved Loss=> 23.0458594012
Gradients: w=>0.0268914285, b=>-0.4639497162
Iteration 5090: Achieved Loss=> 23.0456434861
Gradients: w=>0.0268771322, b=>-0.4637030663
Iteration 5091: Achieved Loss=> 23.0454278006
Gradients: w=>0.0268628435, b=>-0.4634565475
Iteration 5092: Achieved Loss=> 23.0452123443
Gradients: w=>0.0268485624, b=>-0.4632101598
Iteration 5093: Achieved Loss=> 23.0449971170
Gradients: w=>0.0268342889, b=>-0.4629639030
Iteration 5094: Achieved Loss=> 23.0447821185
Gradients: w=>0.0268200229, b=>-0.4627177772
Iteration 5095: Achieved Loss=> 23.0445673485
Gradients: w=>0.0268057646, b=>-0.4624717822
Iteration 5096: Achieved Loss=> 23.0443528069
Gradients: w=>0.0267915138, b=>-0.4622259180
Iteration 5097: Achieved Loss=> 23.0441384933
Gradients: w=>0.0267772706, b=>-0.4619801846
Iteration 5098: Achieved Loss=> 23.0439244075
Gradients: w=>0.0267630350, b=>-0.4617345817
Iteration 5099: Achieved Loss=> 23.0437105493
Gradients: w=>0.0267488070, b=>-0.4614891094
Iteration 5100: Achieved Loss=> 23.0434969184
Gradients: w=>0.0267345865, b=>-0.4612437677
Iteration 5101: Achieved Loss=> 23.0432835146
Gradients: w=>0.0267203735, b=>-0.4609985563
Iteration 5102: Achieved Loss=> 23.0430703376
Gradients: w=>0.0267061682, b=>-0.4607534753
Iteration 5103: Achieved Loss=> 23.0428573872
Gradients: w=>0.0266919704, b=>-0.4605085246
Iteration 5104: Achieved Loss=> 23.0426446632
Gradients: w=>0.0266777801, b=>-0.4602637042
Iteration 5105: Achieved Loss=> 23.0424321654
Gradients: w=>0.0266635974, b=>-0.4600190139
Iteration 5106: Achieved Loss=> 23.0422198934
Gradients: w=>0.0266494222, b=>-0.4597744536
Iteration 5107: Achieved Loss=> 23.0420078470
Gradients: w=>0.0266352545, b=>-0.4595300234
Iteration 5108: Achieved Loss=> 23.0417960260
Gradients: w=>0.0266210944, b=>-0.4592857232
Iteration 5109: Achieved Loss=> 23.0415844302
Gradients: w=>0.0266069418, b=>-0.4590415528
Iteration 5110: Achieved Loss=> 23.0413730594
Gradients: w=>0.0265927967, b=>-0.4587975122
Iteration 5111: Achieved Loss=> 23.0411619132
Gradients: w=>0.0265786592, b=>-0.4585536013
Iteration 5112: Achieved Loss=> 23.0409509914
Gradients: w=>0.0265645292, b=>-0.4583098202
Iteration 5113: Achieved Loss=> 23.0407402939
Gradients: w=>0.0265504066, b=>-0.4580661686
Iteration 5114: Achieved Loss=> 23.0405298203
Gradients: w=>0.0265362916, b=>-0.4578226466
Iteration 5115: Achieved Loss=> 23.0403195705
Gradients: w=>0.0265221841, b=>-0.4575792540
Iteration 5116: Achieved Loss=> 23.0401095441
Gradients: w=>0.0265080841, b=>-0.4573359908
Iteration 5117: Achieved Loss=> 23.0398997410
Gradients: w=>0.0264939916, b=>-0.4570928570
Iteration 5118: Achieved Loss=> 23.0396901609
Gradients: w=>0.0264799066, b=>-0.4568498524
Iteration 5119: Achieved Loss=> 23.0394808036
Gradients: w=>0.0264658291, b=>-0.4566069770
Iteration 5120: Achieved Loss=> 23.0392716688
Gradients: w=>0.0264517590, b=>-0.4563642307
Iteration 5121: Achieved Loss=> 23.0390627564
Gradients: w=>0.0264376964, b=>-0.4561216135
Iteration 5122: Achieved Loss=> 23.0388540660
Gradients: w=>0.0264236414, b=>-0.4558791252
Iteration 5123: Achieved Loss=> 23.0386455974
Gradients: w=>0.0264095937, b=>-0.4556367659
Iteration 5124: Achieved Loss=> 23.0384373505
Gradients: w=>0.0263955536, b=>-0.4553945354
Iteration 5125: Achieved Loss=> 23.0382293249
Gradients: w=>0.0263815209, b=>-0.4551524337
Iteration 5126: Achieved Loss=> 23.0380215204
Gradients: w=>0.0263674957, b=>-0.4549104607
Iteration 5127: Achieved Loss=> 23.0378139368
Gradients: w=>0.0263534779, b=>-0.4546686163
Iteration 5128: Achieved Loss=> 23.0376065739
Gradients: w=>0.0263394676, b=>-0.4544269005
Iteration 5129: Achieved Loss=> 23.0373994314
Gradients: w=>0.0263254647, b=>-0.4541853132
Iteration 5130: Achieved Loss=> 23.0371925091
Gradients: w=>0.0263114693, b=>-0.4539438544
Iteration 5131: Achieved Loss=> 23.0369858067
Gradients: w=>0.0262974813, b=>-0.4537025239
Iteration 5132: Achieved Loss=> 23.0367793241
Gradients: w=>0.0262835008, b=>-0.4534613217
Iteration 5133: Achieved Loss=> 23.0365730609
Gradients: w=>0.0262695276, b=>-0.4532202477
Iteration 5134: Achieved Loss=> 23.0363670170
Gradients: w=>0.0262555620, b=>-0.4529793019
Iteration 5135: Achieved Loss=> 23.0361611922
Gradients: w=>0.0262416037, b=>-0.4527384842
Iteration 5136: Achieved Loss=> 23.0359555861
Gradients: w=>0.0262276529, b=>-0.4524977946
Iteration 5137: Achieved Loss=> 23.0357501985
Gradients: w=>0.0262137094, b=>-0.4522572328
Iteration 5138: Achieved Loss=> 23.0355450293
Gradients: w=>0.0261997734, b=>-0.4520167990
Iteration 5139: Achieved Loss=> 23.0353400782
Gradients: w=>0.0261858448, b=>-0.4517764930
Iteration 5140: Achieved Loss=> 23.0351353450
Gradients: w=>0.0261719236, b=>-0.4515363148
Iteration 5141: Achieved Loss=> 23.0349308293
Gradients: w=>0.0261580098, b=>-0.4512962642
Iteration 5142: Achieved Loss=> 23.0347265311
Gradients: w=>0.0261441034, b=>-0.4510563413
Iteration 5143: Achieved Loss=> 23.0345224500
Gradients: w=>0.0261302044, b=>-0.4508165459
Iteration 5144: Achieved Loss=> 23.0343185859
Gradients: w=>0.0261163128, b=>-0.4505768779
Iteration 5145: Achieved Loss=> 23.0341149384
Gradients: w=>0.0261024286, b=>-0.4503373374
Iteration 5146: Achieved Loss=> 23.0339115075
Gradients: w=>0.0260885517, b=>-0.4500979243
Iteration 5147: Achieved Loss=> 23.0337082927
Gradients: w=>0.0260746822, b=>-0.4498586384
Iteration 5148: Achieved Loss=> 23.0335052940
Gradients: w=>0.0260608201, b=>-0.4496194798
Iteration 5149: Achieved Loss=> 23.0333025111
Gradients: w=>0.0260469654, b=>-0.4493804482
Iteration 5150: Achieved Loss=> 23.0330999437
Gradients: w=>0.0260331180, b=>-0.4491415438
Iteration 5151: Achieved Loss=> 23.0328975917
Gradients: w=>0.0260192780, b=>-0.4489027664
Iteration 5152: Achieved Loss=> 23.0326954547
Gradients: w=>0.0260054454, b=>-0.4486641159
Iteration 5153: Achieved Loss=> 23.0324935326
Gradients: w=>0.0259916201, b=>-0.4484255923
Iteration 5154: Achieved Loss=> 23.0322918252
Gradients: w=>0.0259778021, b=>-0.4481871955
Iteration 5155: Achieved Loss=> 23.0320903322
Gradients: w=>0.0259639915, b=>-0.4479489254
Iteration 5156: Achieved Loss=> 23.0318890533
Gradients: w=>0.0259501883, b=>-0.4477107820
Iteration 5157: Achieved Loss=> 23.0316879884
Gradients: w=>0.0259363924, b=>-0.4474727652
Iteration 5158: Achieved Loss=> 23.0314871373
Gradients: w=>0.0259226038, b=>-0.4472348749
Iteration 5159: Achieved Loss=> 23.0312864996
Gradients: w=>0.0259088225, b=>-0.4469971111
Iteration 5160: Achieved Loss=> 23.0310860752
Gradients: w=>0.0258950486, b=>-0.4467594738
Iteration 5161: Achieved Loss=> 23.0308858638
Gradients: w=>0.0258812820, b=>-0.4465219627
Iteration 5162: Achieved Loss=> 23.0306858653
Gradients: w=>0.0258675227, b=>-0.4462845779
Iteration 5163: Achieved Loss=> 23.0304860794
Gradients: w=>0.0258537708, b=>-0.4460473194
Iteration 5164: Achieved Loss=> 23.0302865058
Gradients: w=>0.0258400261, b=>-0.4458101869
Iteration 5165: Achieved Loss=> 23.0300871444
Gradients: w=>0.0258262887, b=>-0.4455731805
Iteration 5166: Achieved Loss=> 23.0298879949
Gradients: w=>0.0258125587, b=>-0.4453363001
Iteration 5167: Achieved Loss=> 23.0296890571
Gradients: w=>0.0257988359, b=>-0.4450995457
Iteration 5168: Achieved Loss=> 23.0294903307
Gradients: w=>0.0257851205, b=>-0.4448629171
Iteration 5169: Achieved Loss=> 23.0292918156
Gradients: w=>0.0257714123, b=>-0.4446264143
Iteration 5170: Achieved Loss=> 23.0290935115
Gradients: w=>0.0257577115, b=>-0.4443900373
Iteration 5171: Achieved Loss=> 23.0288954182
Gradients: w=>0.0257440179, b=>-0.4441537859
Iteration 5172: Achieved Loss=> 23.0286975355
Gradients: w=>0.0257303316, b=>-0.4439176601
Iteration 5173: Achieved Loss=> 23.0284998631
Gradients: w=>0.0257166525, b=>-0.4436816599
Iteration 5174: Achieved Loss=> 23.0283024009
Gradients: w=>0.0257029808, b=>-0.4434457851
Iteration 5175: Achieved Loss=> 23.0281051485
Gradients: w=>0.0256893163, b=>-0.4432100357
Iteration 5176: Achieved Loss=> 23.0279081058
Gradients: w=>0.0256756590, b=>-0.4429744116
Iteration 5177: Achieved Loss=> 23.0277112726
Gradients: w=>0.0256620091, b=>-0.4427389128
Iteration 5178: Achieved Loss=> 23.0275146486
Gradients: w=>0.0256483664, b=>-0.4425035392
Iteration 5179: Achieved Loss=> 23.0273182336
Gradients: w=>0.0256347309, b=>-0.4422682908
Iteration 5180: Achieved Loss=> 23.0271220274
Gradients: w=>0.0256211027, b=>-0.4420331674
Iteration 5181: Achieved Loss=> 23.0269260297
Gradients: w=>0.0256074817, b=>-0.4417981690
Iteration 5182: Achieved Loss=> 23.0267302404
Gradients: w=>0.0255938680, b=>-0.4415632955
Iteration 5183: Achieved Loss=> 23.0265346592
Gradients: w=>0.0255802615, b=>-0.4413285469
Iteration 5184: Achieved Loss=> 23.0263392859
Gradients: w=>0.0255666623, b=>-0.4410939231
Iteration 5185: Achieved Loss=> 23.0261441203
Gradients: w=>0.0255530702, b=>-0.4408594240
Iteration 5186: Achieved Loss=> 23.0259491622
Gradients: w=>0.0255394854, b=>-0.4406250496
Iteration 5187: Achieved Loss=> 23.0257544113
Gradients: w=>0.0255259079, b=>-0.4403907998
Iteration 5188: Achieved Loss=> 23.0255598674
Gradients: w=>0.0255123375, b=>-0.4401566746
Iteration 5189: Achieved Loss=> 23.0253655303
Gradients: w=>0.0254987744, b=>-0.4399226738
Iteration 5190: Achieved Loss=> 23.0251713997
Gradients: w=>0.0254852184, b=>-0.4396887974
Iteration 5191: Achieved Loss=> 23.0249774756
Gradients: w=>0.0254716697, b=>-0.4394550453
Iteration 5192: Achieved Loss=> 23.0247837575
Gradients: w=>0.0254581282, b=>-0.4392214175
Iteration 5193: Achieved Loss=> 23.0245902454
Gradients: w=>0.0254445939, b=>-0.4389879139
Iteration 5194: Achieved Loss=> 23.0243969390
Gradients: w=>0.0254310667, b=>-0.4387545345
Iteration 5195: Achieved Loss=> 23.0242038380
Gradients: w=>0.0254175468, b=>-0.4385212791
Iteration 5196: Achieved Loss=> 23.0240109424
Gradients: w=>0.0254040341, b=>-0.4382881477
Iteration 5197: Achieved Loss=> 23.0238182517
Gradients: w=>0.0253905285, b=>-0.4380551403
Iteration 5198: Achieved Loss=> 23.0236257659
Gradients: w=>0.0253770301, b=>-0.4378222567
Iteration 5199: Achieved Loss=> 23.0234334847
Gradients: w=>0.0253635389, b=>-0.4375894970
Iteration 5200: Achieved Loss=> 23.0232414079
Gradients: w=>0.0253500549, b=>-0.4373568610
Iteration 5201: Achieved Loss=> 23.0230495353
Gradients: w=>0.0253365780, b=>-0.4371243486
Iteration 5202: Achieved Loss=> 23.0228578666
Gradients: w=>0.0253231083, b=>-0.4368919599
Iteration 5203: Achieved Loss=> 23.0226664017
Gradients: w=>0.0253096458, b=>-0.4366596948
Iteration 5204: Achieved Loss=> 23.0224751403
Gradients: w=>0.0252961904, b=>-0.4364275531
Iteration 5205: Achieved Loss=> 23.0222840822
Gradients: w=>0.0252827421, b=>-0.4361955348
Iteration 5206: Achieved Loss=> 23.0220932271
Gradients: w=>0.0252693011, b=>-0.4359636398
Iteration 5207: Achieved Loss=> 23.0219025750
Gradients: w=>0.0252558671, b=>-0.4357318682
Iteration 5208: Achieved Loss=> 23.0217121255
Gradients: w=>0.0252424403, b=>-0.4355002197
Iteration 5209: Achieved Loss=> 23.0215218785
Gradients: w=>0.0252290207, b=>-0.4352686945
Iteration 5210: Achieved Loss=> 23.0213318337
Gradients: w=>0.0252156082, b=>-0.4350372923
Iteration 5211: Achieved Loss=> 23.0211419909
Gradients: w=>0.0252022028, b=>-0.4348060131
Iteration 5212: Achieved Loss=> 23.0209523499
Gradients: w=>0.0251888045, b=>-0.4345748569
Iteration 5213: Achieved Loss=> 23.0207629105
Gradients: w=>0.0251754134, b=>-0.4343438235
Iteration 5214: Achieved Loss=> 23.0205736724
Gradients: w=>0.0251620293, b=>-0.4341129130
Iteration 5215: Achieved Loss=> 23.0203846355
Gradients: w=>0.0251486524, b=>-0.4338821253
Iteration 5216: Achieved Loss=> 23.0201957996
Gradients: w=>0.0251352826, b=>-0.4336514602
Iteration 5217: Achieved Loss=> 23.0200071644
Gradients: w=>0.0251219199, b=>-0.4334209178
Iteration 5218: Achieved Loss=> 23.0198187297
Gradients: w=>0.0251085644, b=>-0.4331904979
Iteration 5219: Achieved Loss=> 23.0196304953
Gradients: w=>0.0250952159, b=>-0.4329602006
Iteration 5220: Achieved Loss=> 23.0194424610
Gradients: w=>0.0250818745, b=>-0.4327300256
Iteration 5221: Achieved Loss=> 23.0192546265
Gradients: w=>0.0250685402, b=>-0.4324999731
Iteration 5222: Achieved Loss=> 23.0190669918
Gradients: w=>0.0250552130, b=>-0.4322700428
Iteration 5223: Achieved Loss=> 23.0188795565
Gradients: w=>0.0250418929, b=>-0.4320402348
Iteration 5224: Achieved Loss=> 23.0186923204
Gradients: w=>0.0250285798, b=>-0.4318105489
Iteration 5225: Achieved Loss=> 23.0185052833
Gradients: w=>0.0250152739, b=>-0.4315809852
Iteration 5226: Achieved Loss=> 23.0183184451
Gradients: w=>0.0250019750, b=>-0.4313515435
Iteration 5227: Achieved Loss=> 23.0181318055
Gradients: w=>0.0249886832, b=>-0.4311222238
Iteration 5228: Achieved Loss=> 23.0179453642
Gradients: w=>0.0249753984, b=>-0.4308930260
Iteration 5229: Achieved Loss=> 23.0177591212
Gradients: w=>0.0249621207, b=>-0.4306639500
Iteration 5230: Achieved Loss=> 23.0175730761
Gradients: w=>0.0249488501, b=>-0.4304349958
Iteration 5231: Achieved Loss=> 23.0173872288
Gradients: w=>0.0249355865, b=>-0.4302061634
Iteration 5232: Achieved Loss=> 23.0172015790
Gradients: w=>0.0249223300, b=>-0.4299774526
Iteration 5233: Achieved Loss=> 23.0170161266
Gradients: w=>0.0249090805, b=>-0.4297488634
Iteration 5234: Achieved Loss=> 23.0168308713
Gradients: w=>0.0248958381, b=>-0.4295203957
Iteration 5235: Achieved Loss=> 23.0166458129
Gradients: w=>0.0248826027, b=>-0.4292920494
Iteration 5236: Achieved Loss=> 23.0164609513
Gradients: w=>0.0248693743, b=>-0.4290638246
Iteration 5237: Achieved Loss=> 23.0162762861
Gradients: w=>0.0248561530, b=>-0.4288357211
Iteration 5238: Achieved Loss=> 23.0160918173
Gradients: w=>0.0248429387, b=>-0.4286077389
Iteration 5239: Achieved Loss=> 23.0159075445
Gradients: w=>0.0248297315, b=>-0.4283798779
Iteration 5240: Achieved Loss=> 23.0157234676
Gradients: w=>0.0248165312, b=>-0.4281521380
Iteration 5241: Achieved Loss=> 23.0155395864
Gradients: w=>0.0248033380, b=>-0.4279245191
Iteration 5242: Achieved Loss=> 23.0153559006
Gradients: w=>0.0247901518, b=>-0.4276970213
Iteration 5243: Achieved Loss=> 23.0151724101
Gradients: w=>0.0247769726, b=>-0.4274696445
Iteration 5244: Achieved Loss=> 23.0149891147
Gradients: w=>0.0247638004, b=>-0.4272423885
Iteration 5245: Achieved Loss=> 23.0148060140
Gradients: w=>0.0247506352, b=>-0.4270152533
Iteration 5246: Achieved Loss=> 23.0146231081
Gradients: w=>0.0247374770, b=>-0.4267882389
Iteration 5247: Achieved Loss=> 23.0144403965
Gradients: w=>0.0247243258, b=>-0.4265613452
Iteration 5248: Achieved Loss=> 23.0142578791
Gradients: w=>0.0247111816, b=>-0.4263345721
Iteration 5249: Achieved Loss=> 23.0140755558
Gradients: w=>0.0246980443, b=>-0.4261079195
Iteration 5250: Achieved Loss=> 23.0138934263
Gradients: w=>0.0246849141, b=>-0.4258813875
Iteration 5251: Achieved Loss=> 23.0137114904
Gradients: w=>0.0246717908, b=>-0.4256549759
Iteration 5252: Achieved Loss=> 23.0135297478
Gradients: w=>0.0246586746, b=>-0.4254286846
Iteration 5253: Achieved Loss=> 23.0133481985
Gradients: w=>0.0246455652, b=>-0.4252025137
Iteration 5254: Achieved Loss=> 23.0131668421
Gradients: w=>0.0246324629, b=>-0.4249764629
Iteration 5255: Achieved Loss=> 23.0129856785
Gradients: w=>0.0246193675, b=>-0.4247505324
Iteration 5256: Achieved Loss=> 23.0128047075
Gradients: w=>0.0246062791, b=>-0.4245247220
Iteration 5257: Achieved Loss=> 23.0126239289
Gradients: w=>0.0245931977, b=>-0.4242990316
Iteration 5258: Achieved Loss=> 23.0124433424
Gradients: w=>0.0245801232, b=>-0.4240734612
Iteration 5259: Achieved Loss=> 23.0122629479
Gradients: w=>0.0245670556, b=>-0.4238480108
Iteration 5260: Achieved Loss=> 23.0120827451
Gradients: w=>0.0245539950, b=>-0.4236226802
Iteration 5261: Achieved Loss=> 23.0119027339
Gradients: w=>0.0245409414, b=>-0.4233974693
Iteration 5262: Achieved Loss=> 23.0117229141
Gradients: w=>0.0245278946, b=>-0.4231723782
Iteration 5263: Achieved Loss=> 23.0115432853
Gradients: w=>0.0245148549, b=>-0.4229474068
Iteration 5264: Achieved Loss=> 23.0113638476
Gradients: w=>0.0245018220, b=>-0.4227225550
Iteration 5265: Achieved Loss=> 23.0111846005
Gradients: w=>0.0244887961, b=>-0.4224978227
Iteration 5266: Achieved Loss=> 23.0110055440
Gradients: w=>0.0244757771, b=>-0.4222732099
Iteration 5267: Achieved Loss=> 23.0108266778
Gradients: w=>0.0244627650, b=>-0.4220487165
Iteration 5268: Achieved Loss=> 23.0106480018
Gradients: w=>0.0244497599, b=>-0.4218243424
Iteration 5269: Achieved Loss=> 23.0104695157
Gradients: w=>0.0244367616, b=>-0.4216000877
Iteration 5270: Achieved Loss=> 23.0102912193
Gradients: w=>0.0244237703, b=>-0.4213759521
Iteration 5271: Achieved Loss=> 23.0101131125
Gradients: w=>0.0244107859, b=>-0.4211519357
Iteration 5272: Achieved Loss=> 23.0099351949
Gradients: w=>0.0243978083, b=>-0.4209280384
Iteration 5273: Achieved Loss=> 23.0097574665
Gradients: w=>0.0243848377, b=>-0.4207042602
Iteration 5274: Achieved Loss=> 23.0095799270
Gradients: w=>0.0243718740, b=>-0.4204806009
Iteration 5275: Achieved Loss=> 23.0094025763
Gradients: w=>0.0243589172, b=>-0.4202570605
Iteration 5276: Achieved Loss=> 23.0092254140
Gradients: w=>0.0243459672, b=>-0.4200336389
Iteration 5277: Achieved Loss=> 23.0090484401
Gradients: w=>0.0243330241, b=>-0.4198103362
Iteration 5278: Achieved Loss=> 23.0088716543
Gradients: w=>0.0243200880, b=>-0.4195871521
Iteration 5279: Achieved Loss=> 23.0086950564
Gradients: w=>0.0243071587, b=>-0.4193640867
Iteration 5280: Achieved Loss=> 23.0085186462
Gradients: w=>0.0242942362, b=>-0.4191411399
Iteration 5281: Achieved Loss=> 23.0083424236
Gradients: w=>0.0242813207, b=>-0.4189183116
Iteration 5282: Achieved Loss=> 23.0081663882
Gradients: w=>0.0242684120, b=>-0.4186956017
Iteration 5283: Achieved Loss=> 23.0079905400
Gradients: w=>0.0242555101, b=>-0.4184730103
Iteration 5284: Achieved Loss=> 23.0078148787
Gradients: w=>0.0242426152, b=>-0.4182505372
Iteration 5285: Achieved Loss=> 23.0076394042
Gradients: w=>0.0242297270, b=>-0.4180281824
Iteration 5286: Achieved Loss=> 23.0074641161
Gradients: w=>0.0242168458, b=>-0.4178059458
Iteration 5287: Achieved Loss=> 23.0072890144
Gradients: w=>0.0242039714, b=>-0.4175838273
Iteration 5288: Achieved Loss=> 23.0071140989
Gradients: w=>0.0241911038, b=>-0.4173618269
Iteration 5289: Achieved Loss=> 23.0069393692
Gradients: w=>0.0241782430, b=>-0.4171399446
Iteration 5290: Achieved Loss=> 23.0067648253
Gradients: w=>0.0241653892, b=>-0.4169181802
Iteration 5291: Achieved Loss=> 23.0065904669
Gradients: w=>0.0241525421, b=>-0.4166965337
Iteration 5292: Achieved Loss=> 23.0064162939
Gradients: w=>0.0241397019, b=>-0.4164750050
Iteration 5293: Achieved Loss=> 23.0062423060
Gradients: w=>0.0241268685, b=>-0.4162535941
Iteration 5294: Achieved Loss=> 23.0060685030
Gradients: w=>0.0241140419, b=>-0.4160323009
Iteration 5295: Achieved Loss=> 23.0058948848
Gradients: w=>0.0241012221, b=>-0.4158111254
Iteration 5296: Achieved Loss=> 23.0057214512
Gradients: w=>0.0240884092, b=>-0.4155900674
Iteration 5297: Achieved Loss=> 23.0055482019
Gradients: w=>0.0240756030, b=>-0.4153691270
Iteration 5298: Achieved Loss=> 23.0053751368
Gradients: w=>0.0240628037, b=>-0.4151483040
Iteration 5299: Achieved Loss=> 23.0052022556
Gradients: w=>0.0240500112, b=>-0.4149275984
Iteration 5300: Achieved Loss=> 23.0050295582
Gradients: w=>0.0240372254, b=>-0.4147070102
Iteration 5301: Achieved Loss=> 23.0048570444
Gradients: w=>0.0240244465, b=>-0.4144865392
Iteration 5302: Achieved Loss=> 23.0046847139
Gradients: w=>0.0240116744, b=>-0.4142661854
Iteration 5303: Achieved Loss=> 23.0045125667
Gradients: w=>0.0239989090, b=>-0.4140459488
Iteration 5304: Achieved Loss=> 23.0043406024
Gradients: w=>0.0239861505, b=>-0.4138258293
Iteration 5305: Achieved Loss=> 23.0041688209
Gradients: w=>0.0239733987, b=>-0.4136058268
Iteration 5306: Achieved Loss=> 23.0039972221
Gradients: w=>0.0239606537, b=>-0.4133859412
Iteration 5307: Achieved Loss=> 23.0038258056
Gradients: w=>0.0239479155, b=>-0.4131661726
Iteration 5308: Achieved Loss=> 23.0036545713
Gradients: w=>0.0239351840, b=>-0.4129465207
Iteration 5309: Achieved Loss=> 23.0034835191
Gradients: w=>0.0239224594, b=>-0.4127269857
Iteration 5310: Achieved Loss=> 23.0033126487
Gradients: w=>0.0239097415, b=>-0.4125075674
Iteration 5311: Achieved Loss=> 23.0031419599
Gradients: w=>0.0238970303, b=>-0.4122882657
Iteration 5312: Achieved Loss=> 23.0029714525
Gradients: w=>0.0238843259, b=>-0.4120690806
Iteration 5313: Achieved Loss=> 23.0028011264
Gradients: w=>0.0238716283, b=>-0.4118500120
Iteration 5314: Achieved Loss=> 23.0026309814
Gradients: w=>0.0238589374, b=>-0.4116310599
Iteration 5315: Achieved Loss=> 23.0024610172
Gradients: w=>0.0238462532, b=>-0.4114122242
Iteration 5316: Achieved Loss=> 23.0022912337
Gradients: w=>0.0238335758, b=>-0.4111935048
Iteration 5317: Achieved Loss=> 23.0021216306
Gradients: w=>0.0238209052, b=>-0.4109749017
Iteration 5318: Achieved Loss=> 23.0019522079
Gradients: w=>0.0238082412, b=>-0.4107564149
Iteration 5319: Achieved Loss=> 23.0017829652
Gradients: w=>0.0237955841, b=>-0.4105380441
Iteration 5320: Achieved Loss=> 23.0016139024
Gradients: w=>0.0237829336, b=>-0.4103197895
Iteration 5321: Achieved Loss=> 23.0014450194
Gradients: w=>0.0237702899, b=>-0.4101016509
Iteration 5322: Achieved Loss=> 23.0012763159
Gradients: w=>0.0237576528, b=>-0.4098836283
Iteration 5323: Achieved Loss=> 23.0011077917
Gradients: w=>0.0237450225, b=>-0.4096657216
Iteration 5324: Achieved Loss=> 23.0009394466
Gradients: w=>0.0237323990, b=>-0.4094479307
Iteration 5325: Achieved Loss=> 23.0007712805
Gradients: w=>0.0237197821, b=>-0.4092302556
Iteration 5326: Achieved Loss=> 23.0006032931
Gradients: w=>0.0237071719, b=>-0.4090126963
Iteration 5327: Achieved Loss=> 23.0004354843
Gradients: w=>0.0236945685, b=>-0.4087952525
Iteration 5328: Achieved Loss=> 23.0002678539
Gradients: w=>0.0236819717, b=>-0.4085779244
Iteration 5329: Achieved Loss=> 23.0001004016
Gradients: w=>0.0236693816, b=>-0.4083607119
Iteration 5330: Achieved Loss=> 22.9999331274
Gradients: w=>0.0236567983, b=>-0.4081436148
Iteration 5331: Achieved Loss=> 22.9997660310
Gradients: w=>0.0236442216, b=>-0.4079266331
Iteration 5332: Achieved Loss=> 22.9995991122
Gradients: w=>0.0236316516, b=>-0.4077097668
Iteration 5333: Achieved Loss=> 22.9994323708
Gradients: w=>0.0236190883, b=>-0.4074930158
Iteration 5334: Achieved Loss=> 22.9992658067
Gradients: w=>0.0236065316, b=>-0.4072763800
Iteration 5335: Achieved Loss=> 22.9990994196
Gradients: w=>0.0235939817, b=>-0.4070598593
Iteration 5336: Achieved Loss=> 22.9989332094
Gradients: w=>0.0235814384, b=>-0.4068434538
Iteration 5337: Achieved Loss=> 22.9987671758
Gradients: w=>0.0235689018, b=>-0.4066271634
Iteration 5338: Achieved Loss=> 22.9986013188
Gradients: w=>0.0235563718, b=>-0.4064109879
Iteration 5339: Achieved Loss=> 22.9984356381
Gradients: w=>0.0235438485, b=>-0.4061949273
Iteration 5340: Achieved Loss=> 22.9982701334
Gradients: w=>0.0235313319, b=>-0.4059789816
Iteration 5341: Achieved Loss=> 22.9981048047
Gradients: w=>0.0235188219, b=>-0.4057631507
Iteration 5342: Achieved Loss=> 22.9979396518
Gradients: w=>0.0235063186, b=>-0.4055474346
Iteration 5343: Achieved Loss=> 22.9977746744
Gradients: w=>0.0234938219, b=>-0.4053318331
Iteration 5344: Achieved Loss=> 22.9976098723
Gradients: w=>0.0234813319, b=>-0.4051163463
Iteration 5345: Achieved Loss=> 22.9974452455
Gradients: w=>0.0234688485, b=>-0.4049009740
Iteration 5346: Achieved Loss=> 22.9972807936
Gradients: w=>0.0234563717, b=>-0.4046857162
Iteration 5347: Achieved Loss=> 22.9971165166
Gradients: w=>0.0234439016, b=>-0.4044705729
Iteration 5348: Achieved Loss=> 22.9969524141
Gradients: w=>0.0234314381, b=>-0.4042555439
Iteration 5349: Achieved Loss=> 22.9967884862
Gradients: w=>0.0234189812, b=>-0.4040406292
Iteration 5350: Achieved Loss=> 22.9966247324
Gradients: w=>0.0234065310, b=>-0.4038258288
Iteration 5351: Achieved Loss=> 22.9964611527
Gradients: w=>0.0233940874, b=>-0.4036111426
Iteration 5352: Achieved Loss=> 22.9962977470
Gradients: w=>0.0233816503, b=>-0.4033965706
Iteration 5353: Achieved Loss=> 22.9961345149
Gradients: w=>0.0233692199, b=>-0.4031821126
Iteration 5354: Achieved Loss=> 22.9959714563
Gradients: w=>0.0233567961, b=>-0.4029677686
Iteration 5355: Achieved Loss=> 22.9958085710
Gradients: w=>0.0233443790, b=>-0.4027535385
Iteration 5356: Achieved Loss=> 22.9956458589
Gradients: w=>0.0233319684, b=>-0.4025394224
Iteration 5357: Achieved Loss=> 22.9954833198
Gradients: w=>0.0233195644, b=>-0.4023254201
Iteration 5358: Achieved Loss=> 22.9953209534
Gradients: w=>0.0233071670, b=>-0.4021115316
Iteration 5359: Achieved Loss=> 22.9951587596
Gradients: w=>0.0232947762, b=>-0.4018977567
Iteration 5360: Achieved Loss=> 22.9949967382
Gradients: w=>0.0232823920, b=>-0.4016840955
Iteration 5361: Achieved Loss=> 22.9948348891
Gradients: w=>0.0232700143, b=>-0.4014705479
Iteration 5362: Achieved Loss=> 22.9946732120
Gradients: w=>0.0232576433, b=>-0.4012571139
Iteration 5363: Achieved Loss=> 22.9945117067
Gradients: w=>0.0232452788, b=>-0.4010437933
Iteration 5364: Achieved Loss=> 22.9943503732
Gradients: w=>0.0232329209, b=>-0.4008305861
Iteration 5365: Achieved Loss=> 22.9941892111
Gradients: w=>0.0232205696, b=>-0.4006174922
Iteration 5366: Achieved Loss=> 22.9940282203
Gradients: w=>0.0232082248, b=>-0.4004045117
Iteration 5367: Achieved Loss=> 22.9938674007
Gradients: w=>0.0231958866, b=>-0.4001916444
Iteration 5368: Achieved Loss=> 22.9937067520
Gradients: w=>0.0231835549, b=>-0.3999788902
Iteration 5369: Achieved Loss=> 22.9935462741
Gradients: w=>0.0231712298, b=>-0.3997662491
Iteration 5370: Achieved Loss=> 22.9933859667
Gradients: w=>0.0231589113, b=>-0.3995537211
Iteration 5371: Achieved Loss=> 22.9932258298
Gradients: w=>0.0231465993, b=>-0.3993413061
Iteration 5372: Achieved Loss=> 22.9930658631
Gradients: w=>0.0231342939, b=>-0.3991290040
Iteration 5373: Achieved Loss=> 22.9929060664
Gradients: w=>0.0231219950, b=>-0.3989168148
Iteration 5374: Achieved Loss=> 22.9927464396
Gradients: w=>0.0231097026, b=>-0.3987047384
Iteration 5375: Achieved Loss=> 22.9925869825
Gradients: w=>0.0230974168, b=>-0.3984927747
Iteration 5376: Achieved Loss=> 22.9924276949
Gradients: w=>0.0230851375, b=>-0.3982809237
Iteration 5377: Achieved Loss=> 22.9922685765
Gradients: w=>0.0230728647, b=>-0.3980691853
Iteration 5378: Achieved Loss=> 22.9921096274
Gradients: w=>0.0230605985, b=>-0.3978575595
Iteration 5379: Achieved Loss=> 22.9919508472
Gradients: w=>0.0230483388, b=>-0.3976460462
Iteration 5380: Achieved Loss=> 22.9917922357
Gradients: w=>0.0230360856, b=>-0.3974346454
Iteration 5381: Achieved Loss=> 22.9916337929
Gradients: w=>0.0230238389, b=>-0.3972233569
Iteration 5382: Achieved Loss=> 22.9914755185
Gradients: w=>0.0230115987, b=>-0.3970121808
Iteration 5383: Achieved Loss=> 22.9913174123
Gradients: w=>0.0229993650, b=>-0.3968011169
Iteration 5384: Achieved Loss=> 22.9911594742
Gradients: w=>0.0229871378, b=>-0.3965901653
Iteration 5385: Achieved Loss=> 22.9910017040
Gradients: w=>0.0229749172, b=>-0.3963793258
Iteration 5386: Achieved Loss=> 22.9908441015
Gradients: w=>0.0229627030, b=>-0.3961685984
Iteration 5387: Achieved Loss=> 22.9906866665
Gradients: w=>0.0229504953, b=>-0.3959579830
Iteration 5388: Achieved Loss=> 22.9905293989
Gradients: w=>0.0229382941, b=>-0.3957474796
Iteration 5389: Achieved Loss=> 22.9903722984
Gradients: w=>0.0229260994, b=>-0.3955370881
Iteration 5390: Achieved Loss=> 22.9902153649
Gradients: w=>0.0229139112, b=>-0.3953268084
Iteration 5391: Achieved Loss=> 22.9900585983
Gradients: w=>0.0229017295, b=>-0.3951166405
Iteration 5392: Achieved Loss=> 22.9899019983
Gradients: w=>0.0228895542, b=>-0.3949065844
Iteration 5393: Achieved Loss=> 22.9897455647
Gradients: w=>0.0228773854, b=>-0.3946966399
Iteration 5394: Achieved Loss=> 22.9895892975
Gradients: w=>0.0228652231, b=>-0.3944868071
Iteration 5395: Achieved Loss=> 22.9894331963
Gradients: w=>0.0228530673, b=>-0.3942770858
Iteration 5396: Achieved Loss=> 22.9892772611
Gradients: w=>0.0228409179, b=>-0.3940674760
Iteration 5397: Achieved Loss=> 22.9891214916
Gradients: w=>0.0228287749, b=>-0.3938579776
Iteration 5398: Achieved Loss=> 22.9889658877
Gradients: w=>0.0228166385, b=>-0.3936485906
Iteration 5399: Achieved Loss=> 22.9888104493
Gradients: w=>0.0228045084, b=>-0.3934393150
Iteration 5400: Achieved Loss=> 22.9886551760
Gradients: w=>0.0227923849, b=>-0.3932301506
Iteration 5401: Achieved Loss=> 22.9885000678
Gradients: w=>0.0227802677, b=>-0.3930210973
Iteration 5402: Achieved Loss=> 22.9883451245
Gradients: w=>0.0227681570, b=>-0.3928121553
Iteration 5403: Achieved Loss=> 22.9881903458
Gradients: w=>0.0227560528, b=>-0.3926033243
Iteration 5404: Achieved Loss=> 22.9880357317
Gradients: w=>0.0227439550, b=>-0.3923946043
Iteration 5405: Achieved Loss=> 22.9878812820
Gradients: w=>0.0227318636, b=>-0.3921859953
Iteration 5406: Achieved Loss=> 22.9877269964
Gradients: w=>0.0227197786, b=>-0.3919774971
Iteration 5407: Achieved Loss=> 22.9875728749
Gradients: w=>0.0227077001, b=>-0.3917691099
Iteration 5408: Achieved Loss=> 22.9874189171
Gradients: w=>0.0226956280, b=>-0.3915608334
Iteration 5409: Achieved Loss=> 22.9872651230
Gradients: w=>0.0226835623, b=>-0.3913526676
Iteration 5410: Achieved Loss=> 22.9871114924
Gradients: w=>0.0226715030, b=>-0.3911446125
Iteration 5411: Achieved Loss=> 22.9869580251
Gradients: w=>0.0226594501, b=>-0.3909366681
Iteration 5412: Achieved Loss=> 22.9868047210
Gradients: w=>0.0226474037, b=>-0.3907288341
Iteration 5413: Achieved Loss=> 22.9866515798
Gradients: w=>0.0226353636, b=>-0.3905211107
Iteration 5414: Achieved Loss=> 22.9864986013
Gradients: w=>0.0226233300, b=>-0.3903134977
Iteration 5415: Achieved Loss=> 22.9863457855
Gradients: w=>0.0226113027, b=>-0.3901059950
Iteration 5416: Achieved Loss=> 22.9861931322
Gradients: w=>0.0225992819, b=>-0.3898986027
Iteration 5417: Achieved Loss=> 22.9860406410
Gradients: w=>0.0225872674, b=>-0.3896913207
Iteration 5418: Achieved Loss=> 22.9858883120
Gradients: w=>0.0225752593, b=>-0.3894841488
Iteration 5419: Achieved Loss=> 22.9857361450
Gradients: w=>0.0225632576, b=>-0.3892770871
Iteration 5420: Achieved Loss=> 22.9855841396
Gradients: w=>0.0225512623, b=>-0.3890701354
Iteration 5421: Achieved Loss=> 22.9854322959
Gradients: w=>0.0225392733, b=>-0.3888632938
Iteration 5422: Achieved Loss=> 22.9852806135
Gradients: w=>0.0225272908, b=>-0.3886565621
Iteration 5423: Achieved Loss=> 22.9851290924
Gradients: w=>0.0225153146, b=>-0.3884499404
Iteration 5424: Achieved Loss=> 22.9849777323
Gradients: w=>0.0225033447, b=>-0.3882434285
Iteration 5425: Achieved Loss=> 22.9848265332
Gradients: w=>0.0224913813, b=>-0.3880370263
Iteration 5426: Achieved Loss=> 22.9846754948
Gradients: w=>0.0224794242, b=>-0.3878307339
Iteration 5427: Achieved Loss=> 22.9845246169
Gradients: w=>0.0224674734, b=>-0.3876245512
Iteration 5428: Achieved Loss=> 22.9843738994
Gradients: w=>0.0224555290, b=>-0.3874184781
Iteration 5429: Achieved Loss=> 22.9842233421
Gradients: w=>0.0224435910, b=>-0.3872125145
Iteration 5430: Achieved Loss=> 22.9840729448
Gradients: w=>0.0224316593, b=>-0.3870066605
Iteration 5431: Achieved Loss=> 22.9839227074
Gradients: w=>0.0224197339, b=>-0.3868009159
Iteration 5432: Achieved Loss=> 22.9837726297
Gradients: w=>0.0224078149, b=>-0.3865952806
Iteration 5433: Achieved Loss=> 22.9836227116
Gradients: w=>0.0223959022, b=>-0.3863897547
Iteration 5434: Achieved Loss=> 22.9834729528
Gradients: w=>0.0223839958, b=>-0.3861843381
Iteration 5435: Achieved Loss=> 22.9833233532
Gradients: w=>0.0223720958, b=>-0.3859790306
Iteration 5436: Achieved Loss=> 22.9831739126
Gradients: w=>0.0223602021, b=>-0.3857738323
Iteration 5437: Achieved Loss=> 22.9830246308
Gradients: w=>0.0223483148, b=>-0.3855687431
Iteration 5438: Achieved Loss=> 22.9828755078
Gradients: w=>0.0223364337, b=>-0.3853637629
Iteration 5439: Achieved Loss=> 22.9827265432
Gradients: w=>0.0223245590, b=>-0.3851588917
Iteration 5440: Achieved Loss=> 22.9825777370
Gradients: w=>0.0223126906, b=>-0.3849541294
Iteration 5441: Achieved Loss=> 22.9824290890
Gradients: w=>0.0223008285, b=>-0.3847494760
Iteration 5442: Achieved Loss=> 22.9822805990
Gradients: w=>0.0222889726, b=>-0.3845449314
Iteration 5443: Achieved Loss=> 22.9821322669
Gradients: w=>0.0222771231, b=>-0.3843404955
Iteration 5444: Achieved Loss=> 22.9819840924
Gradients: w=>0.0222652799, b=>-0.3841361683
Iteration 5445: Achieved Loss=> 22.9818360754
Gradients: w=>0.0222534430, b=>-0.3839319497
Iteration 5446: Achieved Loss=> 22.9816882157
Gradients: w=>0.0222416124, b=>-0.3837278397
Iteration 5447: Achieved Loss=> 22.9815405133
Gradients: w=>0.0222297881, b=>-0.3835238382
Iteration 5448: Achieved Loss=> 22.9813929678
Gradients: w=>0.0222179701, b=>-0.3833199451
Iteration 5449: Achieved Loss=> 22.9812455792
Gradients: w=>0.0222061583, b=>-0.3831161605
Iteration 5450: Achieved Loss=> 22.9810983472
Gradients: w=>0.0221943528, b=>-0.3829124842
Iteration 5451: Achieved Loss=> 22.9809512718
Gradients: w=>0.0221825536, b=>-0.3827089162
Iteration 5452: Achieved Loss=> 22.9808043526
Gradients: w=>0.0221707607, b=>-0.3825054563
Iteration 5453: Achieved Loss=> 22.9806575897
Gradients: w=>0.0221589740, b=>-0.3823021047
Iteration 5454: Achieved Loss=> 22.9805109828
Gradients: w=>0.0221471937, b=>-0.3820988612
Iteration 5455: Achieved Loss=> 22.9803645317
Gradients: w=>0.0221354195, b=>-0.3818957257
Iteration 5456: Achieved Loss=> 22.9802182362
Gradients: w=>0.0221236517, b=>-0.3816926982
Iteration 5457: Achieved Loss=> 22.9800720963
Gradients: w=>0.0221118900, b=>-0.3814897786
Iteration 5458: Achieved Loss=> 22.9799261118
Gradients: w=>0.0221001347, b=>-0.3812869670
Iteration 5459: Achieved Loss=> 22.9797802824
Gradients: w=>0.0220883856, b=>-0.3810842631
Iteration 5460: Achieved Loss=> 22.9796346080
Gradients: w=>0.0220766427, b=>-0.3808816670
Iteration 5461: Achieved Loss=> 22.9794890885
Gradients: w=>0.0220649061, b=>-0.3806791786
Iteration 5462: Achieved Loss=> 22.9793437236
Gradients: w=>0.0220531757, b=>-0.3804767979
Iteration 5463: Achieved Loss=> 22.9791985133
Gradients: w=>0.0220414516, b=>-0.3802745248
Iteration 5464: Achieved Loss=> 22.9790534573
Gradients: w=>0.0220297337, b=>-0.3800723591
Iteration 5465: Achieved Loss=> 22.9789085555
Gradients: w=>0.0220180220, b=>-0.3798703010
Iteration 5466: Achieved Loss=> 22.9787638078
Gradients: w=>0.0220063165, b=>-0.3796683503
Iteration 5467: Achieved Loss=> 22.9786192139
Gradients: w=>0.0219946173, b=>-0.3794665070
Iteration 5468: Achieved Loss=> 22.9784747737
Gradients: w=>0.0219829243, b=>-0.3792647709
Iteration 5469: Achieved Loss=> 22.9783304870
Gradients: w=>0.0219712375, b=>-0.3790631421
Iteration 5470: Achieved Loss=> 22.9781863538
Gradients: w=>0.0219595569, b=>-0.3788616205
Iteration 5471: Achieved Loss=> 22.9780423737
Gradients: w=>0.0219478825, b=>-0.3786602060
Iteration 5472: Achieved Loss=> 22.9778985467
Gradients: w=>0.0219362143, b=>-0.3784588987
Iteration 5473: Achieved Loss=> 22.9777548725
Gradients: w=>0.0219245524, b=>-0.3782576983
Iteration 5474: Achieved Loss=> 22.9776113511
Gradients: w=>0.0219128966, b=>-0.3780566049
Iteration 5475: Achieved Loss=> 22.9774679823
Gradients: w=>0.0219012470, b=>-0.3778556184
Iteration 5476: Achieved Loss=> 22.9773247658
Gradients: w=>0.0218896037, b=>-0.3776547387
Iteration 5477: Achieved Loss=> 22.9771817016
Gradients: w=>0.0218779665, b=>-0.3774539659
Iteration 5478: Achieved Loss=> 22.9770387895
Gradients: w=>0.0218663355, b=>-0.3772532998
Iteration 5479: Achieved Loss=> 22.9768960292
Gradients: w=>0.0218547107, b=>-0.3770527403
Iteration 5480: Achieved Loss=> 22.9767534208
Gradients: w=>0.0218430920, b=>-0.3768522875
Iteration 5481: Achieved Loss=> 22.9766109639
Gradients: w=>0.0218314796, b=>-0.3766519413
Iteration 5482: Achieved Loss=> 22.9764686584
Gradients: w=>0.0218198733, b=>-0.3764517016
Iteration 5483: Achieved Loss=> 22.9763265042
Gradients: w=>0.0218082732, b=>-0.3762515683
Iteration 5484: Achieved Loss=> 22.9761845011
Gradients: w=>0.0217966792, b=>-0.3760515414
Iteration 5485: Achieved Loss=> 22.9760426490
Gradients: w=>0.0217850914, b=>-0.3758516209
Iteration 5486: Achieved Loss=> 22.9759009476
Gradients: w=>0.0217735098, b=>-0.3756518066
Iteration 5487: Achieved Loss=> 22.9757593969
Gradients: w=>0.0217619344, b=>-0.3754520986
Iteration 5488: Achieved Loss=> 22.9756179966
Gradients: w=>0.0217503650, b=>-0.3752524967
Iteration 5489: Achieved Loss=> 22.9754767467
Gradients: w=>0.0217388019, b=>-0.3750530010
Iteration 5490: Achieved Loss=> 22.9753356469
Gradients: w=>0.0217272449, b=>-0.3748536113
Iteration 5491: Achieved Loss=> 22.9751946970
Gradients: w=>0.0217156940, b=>-0.3746543276
Iteration 5492: Achieved Loss=> 22.9750538970
Gradients: w=>0.0217041493, b=>-0.3744551498
Iteration 5493: Achieved Loss=> 22.9749132467
Gradients: w=>0.0216926107, b=>-0.3742560780
Iteration 5494: Achieved Loss=> 22.9747727459
Gradients: w=>0.0216810782, b=>-0.3740571120
Iteration 5495: Achieved Loss=> 22.9746323944
Gradients: w=>0.0216695519, b=>-0.3738582517
Iteration 5496: Achieved Loss=> 22.9744921921
Gradients: w=>0.0216580317, b=>-0.3736594972
Iteration 5497: Achieved Loss=> 22.9743521389
Gradients: w=>0.0216465176, b=>-0.3734608484
Iteration 5498: Achieved Loss=> 22.9742122345
Gradients: w=>0.0216350097, b=>-0.3732623051
Iteration 5499: Achieved Loss=> 22.9740724788
Gradients: w=>0.0216235078, b=>-0.3730638674
Iteration 5500: Achieved Loss=> 22.9739328717
Gradients: w=>0.0216120121, b=>-0.3728655352
Iteration 5501: Achieved Loss=> 22.9737934130
Gradients: w=>0.0216005225, b=>-0.3726673084
Iteration 5502: Achieved Loss=> 22.9736541026
Gradients: w=>0.0215890390, b=>-0.3724691871
Iteration 5503: Achieved Loss=> 22.9735149402
Gradients: w=>0.0215775616, b=>-0.3722711710
Iteration 5504: Achieved Loss=> 22.9733759257
Gradients: w=>0.0215660903, b=>-0.3720732602
Iteration 5505: Achieved Loss=> 22.9732370590
Gradients: w=>0.0215546251, b=>-0.3718754547
Iteration 5506: Achieved Loss=> 22.9730983400
Gradients: w=>0.0215431660, b=>-0.3716777543
Iteration 5507: Achieved Loss=> 22.9729597683
Gradients: w=>0.0215317130, b=>-0.3714801590
Iteration 5508: Achieved Loss=> 22.9728213440
Gradients: w=>0.0215202661, b=>-0.3712826687
Iteration 5509: Achieved Loss=> 22.9726830669
Gradients: w=>0.0215088253, b=>-0.3710852834
Iteration 5510: Achieved Loss=> 22.9725449367
Gradients: w=>0.0214973905, b=>-0.3708880031
Iteration 5511: Achieved Loss=> 22.9724069533
Gradients: w=>0.0214859618, b=>-0.3706908277
Iteration 5512: Achieved Loss=> 22.9722691166
Gradients: w=>0.0214745392, b=>-0.3704937570
Iteration 5513: Achieved Loss=> 22.9721314264
Gradients: w=>0.0214631227, b=>-0.3702967912
Iteration 5514: Achieved Loss=> 22.9719938826
Gradients: w=>0.0214517123, b=>-0.3700999300
Iteration 5515: Achieved Loss=> 22.9718564850
Gradients: w=>0.0214403079, b=>-0.3699031736
Iteration 5516: Achieved Loss=> 22.9717192335
Gradients: w=>0.0214289095, b=>-0.3697065217
Iteration 5517: Achieved Loss=> 22.9715821278
Gradients: w=>0.0214175173, b=>-0.3695099743
Iteration 5518: Achieved Loss=> 22.9714451679
Gradients: w=>0.0214061311, b=>-0.3693135315
Iteration 5519: Achieved Loss=> 22.9713083536
Gradients: w=>0.0213947509, b=>-0.3691171931
Iteration 5520: Achieved Loss=> 22.9711716847
Gradients: w=>0.0213833768, b=>-0.3689209591
Iteration 5521: Achieved Loss=> 22.9710351611
Gradients: w=>0.0213720087, b=>-0.3687248294
Iteration 5522: Achieved Loss=> 22.9708987826
Gradients: w=>0.0213606467, b=>-0.3685288039
Iteration 5523: Achieved Loss=> 22.9707625490
Gradients: w=>0.0213492908, b=>-0.3683328827
Iteration 5524: Achieved Loss=> 22.9706264603
Gradients: w=>0.0213379408, b=>-0.3681370656
Iteration 5525: Achieved Loss=> 22.9704905162
Gradients: w=>0.0213265969, b=>-0.3679413527
Iteration 5526: Achieved Loss=> 22.9703547167
Gradients: w=>0.0213152590, b=>-0.3677457437
Iteration 5527: Achieved Loss=> 22.9702190615
Gradients: w=>0.0213039272, b=>-0.3675502388
Iteration 5528: Achieved Loss=> 22.9700835505
Gradients: w=>0.0212926014, b=>-0.3673548378
Iteration 5529: Achieved Loss=> 22.9699481835
Gradients: w=>0.0212812816, b=>-0.3671595407
Iteration 5530: Achieved Loss=> 22.9698129605
Gradients: w=>0.0212699678, b=>-0.3669643474
Iteration 5531: Achieved Loss=> 22.9696778811
Gradients: w=>0.0212586600, b=>-0.3667692579
Iteration 5532: Achieved Loss=> 22.9695429454
Gradients: w=>0.0212473583, b=>-0.3665742721
Iteration 5533: Achieved Loss=> 22.9694081531
Gradients: w=>0.0212360625, b=>-0.3663793900
Iteration 5534: Achieved Loss=> 22.9692735040
Gradients: w=>0.0212247728, b=>-0.3661846115
Iteration 5535: Achieved Loss=> 22.9691389982
Gradients: w=>0.0212134890, b=>-0.3659899365
Iteration 5536: Achieved Loss=> 22.9690046352
Gradients: w=>0.0212022113, b=>-0.3657953650
Iteration 5537: Achieved Loss=> 22.9688704151
Gradients: w=>0.0211909395, b=>-0.3656008970
Iteration 5538: Achieved Loss=> 22.9687363377
Gradients: w=>0.0211796738, b=>-0.3654065323
Iteration 5539: Achieved Loss=> 22.9686024028
Gradients: w=>0.0211684140, b=>-0.3652122710
Iteration 5540: Achieved Loss=> 22.9684686103
Gradients: w=>0.0211571602, b=>-0.3650181129
Iteration 5541: Achieved Loss=> 22.9683349600
Gradients: w=>0.0211459124, b=>-0.3648240581
Iteration 5542: Achieved Loss=> 22.9682014517
Gradients: w=>0.0211346706, b=>-0.3646301064
Iteration 5543: Achieved Loss=> 22.9680680854
Gradients: w=>0.0211234348, b=>-0.3644362579
Iteration 5544: Achieved Loss=> 22.9679348608
Gradients: w=>0.0211122049, b=>-0.3642425124
Iteration 5545: Achieved Loss=> 22.9678017779
Gradients: w=>0.0211009810, b=>-0.3640488699
Iteration 5546: Achieved Loss=> 22.9676688364
Gradients: w=>0.0210897631, b=>-0.3638553303
Iteration 5547: Achieved Loss=> 22.9675360362
Gradients: w=>0.0210785511, b=>-0.3636618937
Iteration 5548: Achieved Loss=> 22.9674033772
Gradients: w=>0.0210673451, b=>-0.3634685598
Iteration 5549: Achieved Loss=> 22.9672708592
Gradients: w=>0.0210561451, b=>-0.3632753288
Iteration 5550: Achieved Loss=> 22.9671384821
Gradients: w=>0.0210449510, b=>-0.3630822005
Iteration 5551: Achieved Loss=> 22.9670062457
Gradients: w=>0.0210337628, b=>-0.3628891748
Iteration 5552: Achieved Loss=> 22.9668741498
Gradients: w=>0.0210225807, b=>-0.3626962518
Iteration 5553: Achieved Loss=> 22.9667421944
Gradients: w=>0.0210114044, b=>-0.3625034313
Iteration 5554: Achieved Loss=> 22.9666103792
Gradients: w=>0.0210002341, b=>-0.3623107134
Iteration 5555: Achieved Loss=> 22.9664787042
Gradients: w=>0.0209890697, b=>-0.3621180979
Iteration 5556: Achieved Loss=> 22.9663471691
Gradients: w=>0.0209779113, b=>-0.3619255848
Iteration 5557: Achieved Loss=> 22.9662157738
Gradients: w=>0.0209667588, b=>-0.3617331740
Iteration 5558: Achieved Loss=> 22.9660845182
Gradients: w=>0.0209556122, b=>-0.3615408656
Iteration 5559: Achieved Loss=> 22.9659534022
Gradients: w=>0.0209444716, b=>-0.3613486594
Iteration 5560: Achieved Loss=> 22.9658224255
Gradients: w=>0.0209333369, b=>-0.3611565553
Iteration 5561: Achieved Loss=> 22.9656915880
Gradients: w=>0.0209222080, b=>-0.3609645534
Iteration 5562: Achieved Loss=> 22.9655608896
Gradients: w=>0.0209110852, b=>-0.3607726536
Iteration 5563: Achieved Loss=> 22.9654303301
Gradients: w=>0.0208999682, b=>-0.3605808558
Iteration 5564: Achieved Loss=> 22.9652999094
Gradients: w=>0.0208888571, b=>-0.3603891599
Iteration 5565: Achieved Loss=> 22.9651696274
Gradients: w=>0.0208777520, b=>-0.3601975660
Iteration 5566: Achieved Loss=> 22.9650394838
Gradients: w=>0.0208666527, b=>-0.3600060739
Iteration 5567: Achieved Loss=> 22.9649094786
Gradients: w=>0.0208555594, b=>-0.3598146836
Iteration 5568: Achieved Loss=> 22.9647796116
Gradients: w=>0.0208444719, b=>-0.3596233951
Iteration 5569: Achieved Loss=> 22.9646498826
Gradients: w=>0.0208333903, b=>-0.3594322082
Iteration 5570: Achieved Loss=> 22.9645202915
Gradients: w=>0.0208223147, b=>-0.3592411231
Iteration 5571: Achieved Loss=> 22.9643908382
Gradients: w=>0.0208112449, b=>-0.3590501394
Iteration 5572: Achieved Loss=> 22.9642615224
Gradients: w=>0.0208001810, b=>-0.3588592574
Iteration 5573: Achieved Loss=> 22.9641323442
Gradients: w=>0.0207891230, b=>-0.3586684768
Iteration 5574: Achieved Loss=> 22.9640033032
Gradients: w=>0.0207780708, b=>-0.3584777976
Iteration 5575: Achieved Loss=> 22.9638743994
Gradients: w=>0.0207670246, b=>-0.3582872198
Iteration 5576: Achieved Loss=> 22.9637456327
Gradients: w=>0.0207559842, b=>-0.3580967433
Iteration 5577: Achieved Loss=> 22.9636170028
Gradients: w=>0.0207449497, b=>-0.3579063681
Iteration 5578: Achieved Loss=> 22.9634885096
Gradients: w=>0.0207339210, b=>-0.3577160941
Iteration 5579: Achieved Loss=> 22.9633601531
Gradients: w=>0.0207228982, b=>-0.3575259212
Iteration 5580: Achieved Loss=> 22.9632319329
Gradients: w=>0.0207118813, b=>-0.3573358495
Iteration 5581: Achieved Loss=> 22.9631038491
Gradients: w=>0.0207008702, b=>-0.3571458788
Iteration 5582: Achieved Loss=> 22.9629759014
Gradients: w=>0.0206898650, b=>-0.3569560091
Iteration 5583: Achieved Loss=> 22.9628480897
Gradients: w=>0.0206788656, b=>-0.3567662403
Iteration 5584: Achieved Loss=> 22.9627204139
Gradients: w=>0.0206678721, b=>-0.3565765724
Iteration 5585: Achieved Loss=> 22.9625928738
Gradients: w=>0.0206568845, b=>-0.3563870054
Iteration 5586: Achieved Loss=> 22.9624654693
Gradients: w=>0.0206459026, b=>-0.3561975391
Iteration 5587: Achieved Loss=> 22.9623382002
Gradients: w=>0.0206349266, b=>-0.3560081735
Iteration 5588: Achieved Loss=> 22.9622110664
Gradients: w=>0.0206239565, b=>-0.3558189087
Iteration 5589: Achieved Loss=> 22.9620840677
Gradients: w=>0.0206129921, b=>-0.3556297444
Iteration 5590: Achieved Loss=> 22.9619572040
Gradients: w=>0.0206020336, b=>-0.3554406807
Iteration 5591: Achieved Loss=> 22.9618304752
Gradients: w=>0.0205910810, b=>-0.3552517175
Iteration 5592: Achieved Loss=> 22.9617038811
Gradients: w=>0.0205801341, b=>-0.3550628548
Iteration 5593: Achieved Loss=> 22.9615774215
Gradients: w=>0.0205691931, b=>-0.3548740925
Iteration 5594: Achieved Loss=> 22.9614510964
Gradients: w=>0.0205582579, b=>-0.3546854306
Iteration 5595: Achieved Loss=> 22.9613249055
Gradients: w=>0.0205473285, b=>-0.3544968689
Iteration 5596: Achieved Loss=> 22.9611988488
Gradients: w=>0.0205364049, b=>-0.3543084075
Iteration 5597: Achieved Loss=> 22.9610729261
Gradients: w=>0.0205254871, b=>-0.3541200463
Iteration 5598: Achieved Loss=> 22.9609471373
Gradients: w=>0.0205145751, b=>-0.3539317852
Iteration 5599: Achieved Loss=> 22.9608214821
Gradients: w=>0.0205036689, b=>-0.3537436242
Iteration 5600: Achieved Loss=> 22.9606959605
Gradients: w=>0.0204927686, b=>-0.3535555632
Iteration 5601: Achieved Loss=> 22.9605705724
Gradients: w=>0.0204818740, b=>-0.3533676022
Iteration 5602: Achieved Loss=> 22.9604453175
Gradients: w=>0.0204709852, b=>-0.3531797411
Iteration 5603: Achieved Loss=> 22.9603201958
Gradients: w=>0.0204601022, b=>-0.3529919800
Iteration 5604: Achieved Loss=> 22.9601952071
Gradients: w=>0.0204492250, b=>-0.3528043186
Iteration 5605: Achieved Loss=> 22.9600703512
Gradients: w=>0.0204383535, b=>-0.3526167570
Iteration 5606: Achieved Loss=> 22.9599456281
Gradients: w=>0.0204274879, b=>-0.3524292951
Iteration 5607: Achieved Loss=> 22.9598210375
Gradients: w=>0.0204166280, b=>-0.3522419329
Iteration 5608: Achieved Loss=> 22.9596965794
Gradients: w=>0.0204057739, b=>-0.3520546702
Iteration 5609: Achieved Loss=> 22.9595722536
Gradients: w=>0.0203949255, b=>-0.3518675072
Iteration 5610: Achieved Loss=> 22.9594480599
Gradients: w=>0.0203840830, b=>-0.3516804436
Iteration 5611: Achieved Loss=> 22.9593239982
Gradients: w=>0.0203732462, b=>-0.3514934795
Iteration 5612: Achieved Loss=> 22.9592000684
Gradients: w=>0.0203624151, b=>-0.3513066148
Iteration 5613: Achieved Loss=> 22.9590762704
Gradients: w=>0.0203515898, b=>-0.3511198494
Iteration 5614: Achieved Loss=> 22.9589526039
Gradients: w=>0.0203407703, b=>-0.3509331833
Iteration 5615: Achieved Loss=> 22.9588290689
Gradients: w=>0.0203299565, b=>-0.3507466164
Iteration 5616: Achieved Loss=> 22.9587056653
Gradients: w=>0.0203191485, b=>-0.3505601488
Iteration 5617: Achieved Loss=> 22.9585823927
Gradients: w=>0.0203083462, b=>-0.3503737802
Iteration 5618: Achieved Loss=> 22.9584592513
Gradients: w=>0.0202975497, b=>-0.3501875108
Iteration 5619: Achieved Loss=> 22.9583362407
Gradients: w=>0.0202867589, b=>-0.3500013403
Iteration 5620: Achieved Loss=> 22.9582133609
Gradients: w=>0.0202759738, b=>-0.3498152689
Iteration 5621: Achieved Loss=> 22.9580906117
Gradients: w=>0.0202651945, b=>-0.3496292964
Iteration 5622: Achieved Loss=> 22.9579679930
Gradients: w=>0.0202544209, b=>-0.3494434227
Iteration 5623: Achieved Loss=> 22.9578455046
Gradients: w=>0.0202436530, b=>-0.3492576478
Iteration 5624: Achieved Loss=> 22.9577231464
Gradients: w=>0.0202328909, b=>-0.3490719718
Iteration 5625: Achieved Loss=> 22.9576009183
Gradients: w=>0.0202221344, b=>-0.3488863944
Iteration 5626: Achieved Loss=> 22.9574788201
Gradients: w=>0.0202113837, b=>-0.3487009157
Iteration 5627: Achieved Loss=> 22.9573568517
Gradients: w=>0.0202006387, b=>-0.3485155355
Iteration 5628: Achieved Loss=> 22.9572350130
Gradients: w=>0.0201898995, b=>-0.3483302540
Iteration 5629: Achieved Loss=> 22.9571133037
Gradients: w=>0.0201791659, b=>-0.3481450709
Iteration 5630: Achieved Loss=> 22.9569917239
Gradients: w=>0.0201684380, b=>-0.3479599863
Iteration 5631: Achieved Loss=> 22.9568702732
Gradients: w=>0.0201577159, b=>-0.3477750001
Iteration 5632: Achieved Loss=> 22.9567489517
Gradients: w=>0.0201469994, b=>-0.3475901123
Iteration 5633: Achieved Loss=> 22.9566277591
Gradients: w=>0.0201362886, b=>-0.3474053227
Iteration 5634: Achieved Loss=> 22.9565066954
Gradients: w=>0.0201255836, b=>-0.3472206314
Iteration 5635: Achieved Loss=> 22.9563857603
Gradients: w=>0.0201148842, b=>-0.3470360382
Iteration 5636: Achieved Loss=> 22.9562649538
Gradients: w=>0.0201041905, b=>-0.3468515432
Iteration 5637: Achieved Loss=> 22.9561442758
Gradients: w=>0.0200935025, b=>-0.3466671463
Iteration 5638: Achieved Loss=> 22.9560237260
Gradients: w=>0.0200828202, b=>-0.3464828474
Iteration 5639: Achieved Loss=> 22.9559033043
Gradients: w=>0.0200721435, b=>-0.3462986465
Iteration 5640: Achieved Loss=> 22.9557830106
Gradients: w=>0.0200614726, b=>-0.3461145435
Iteration 5641: Achieved Loss=> 22.9556628448
Gradients: w=>0.0200508073, b=>-0.3459305384
Iteration 5642: Achieved Loss=> 22.9555428068
Gradients: w=>0.0200401476, b=>-0.3457466311
Iteration 5643: Achieved Loss=> 22.9554228963
Gradients: w=>0.0200294937, b=>-0.3455628215
Iteration 5644: Achieved Loss=> 22.9553031133
Gradients: w=>0.0200188454, b=>-0.3453791097
Iteration 5645: Achieved Loss=> 22.9551834577
Gradients: w=>0.0200082028, b=>-0.3451954956
Iteration 5646: Achieved Loss=> 22.9550639292
Gradients: w=>0.0199975658, b=>-0.3450119791
Iteration 5647: Achieved Loss=> 22.9549445278
Gradients: w=>0.0199869345, b=>-0.3448285601
Iteration 5648: Achieved Loss=> 22.9548252533
Gradients: w=>0.0199763088, b=>-0.3446452387
Iteration 5649: Achieved Loss=> 22.9547061055
Gradients: w=>0.0199656888, b=>-0.3444620147
Iteration 5650: Achieved Loss=> 22.9545870845
Gradients: w=>0.0199550744, b=>-0.3442788881
Iteration 5651: Achieved Loss=> 22.9544681899
Gradients: w=>0.0199444657, b=>-0.3440958589
Iteration 5652: Achieved Loss=> 22.9543494218
Gradients: w=>0.0199338626, b=>-0.3439129270
Iteration 5653: Achieved Loss=> 22.9542307799
Gradients: w=>0.0199232651, b=>-0.3437300923
Iteration 5654: Achieved Loss=> 22.9541122641
Gradients: w=>0.0199126733, b=>-0.3435473548
Iteration 5655: Achieved Loss=> 22.9539938742
Gradients: w=>0.0199020871, b=>-0.3433647145
Iteration 5656: Achieved Loss=> 22.9538756103
Gradients: w=>0.0198915066, b=>-0.3431821713
Iteration 5657: Achieved Loss=> 22.9537574720
Gradients: w=>0.0198809316, b=>-0.3429997251
Iteration 5658: Achieved Loss=> 22.9536394593
Gradients: w=>0.0198703623, b=>-0.3428173759
Iteration 5659: Achieved Loss=> 22.9535215721
Gradients: w=>0.0198597986, b=>-0.3426351237
Iteration 5660: Achieved Loss=> 22.9534038101
Gradients: w=>0.0198492405, b=>-0.3424529683
Iteration 5661: Achieved Loss=> 22.9532861734
Gradients: w=>0.0198386881, b=>-0.3422709098
Iteration 5662: Achieved Loss=> 22.9531686617
Gradients: w=>0.0198281412, b=>-0.3420889481
Iteration 5663: Achieved Loss=> 22.9530512749
Gradients: w=>0.0198176000, b=>-0.3419070831
Iteration 5664: Achieved Loss=> 22.9529340129
Gradients: w=>0.0198070643, b=>-0.3417253148
Iteration 5665: Achieved Loss=> 22.9528168755
Gradients: w=>0.0197965343, b=>-0.3415436432
Iteration 5666: Achieved Loss=> 22.9526998627
Gradients: w=>0.0197860098, b=>-0.3413620681
Iteration 5667: Achieved Loss=> 22.9525829742
Gradients: w=>0.0197754910, b=>-0.3411805895
Iteration 5668: Achieved Loss=> 22.9524662100
Gradients: w=>0.0197649777, b=>-0.3409992074
Iteration 5669: Achieved Loss=> 22.9523495699
Gradients: w=>0.0197544701, b=>-0.3408179218
Iteration 5670: Achieved Loss=> 22.9522330538
Gradients: w=>0.0197439680, b=>-0.3406367325
Iteration 5671: Achieved Loss=> 22.9521166615
Gradients: w=>0.0197334715, b=>-0.3404556396
Iteration 5672: Achieved Loss=> 22.9520003930
Gradients: w=>0.0197229806, b=>-0.3402746429
Iteration 5673: Achieved Loss=> 22.9518842480
Gradients: w=>0.0197124952, b=>-0.3400937425
Iteration 5674: Achieved Loss=> 22.9517682265
Gradients: w=>0.0197020155, b=>-0.3399129382
Iteration 5675: Achieved Loss=> 22.9516523284
Gradients: w=>0.0196915413, b=>-0.3397322300
Iteration 5676: Achieved Loss=> 22.9515365534
Gradients: w=>0.0196810726, b=>-0.3395516180
Iteration 5677: Achieved Loss=> 22.9514209015
Gradients: w=>0.0196706096, b=>-0.3393711019
Iteration 5678: Achieved Loss=> 22.9513053726
Gradients: w=>0.0196601521, b=>-0.3391906818
Iteration 5679: Achieved Loss=> 22.9511899664
Gradients: w=>0.0196497001, b=>-0.3390103576
Iteration 5680: Achieved Loss=> 22.9510746829
Gradients: w=>0.0196392537, b=>-0.3388301293
Iteration 5681: Achieved Loss=> 22.9509595220
Gradients: w=>0.0196288129, b=>-0.3386499968
Iteration 5682: Achieved Loss=> 22.9508444835
Gradients: w=>0.0196183776, b=>-0.3384699601
Iteration 5683: Achieved Loss=> 22.9507295672
Gradients: w=>0.0196079479, b=>-0.3382900191
Iteration 5684: Achieved Loss=> 22.9506147731
Gradients: w=>0.0195975237, b=>-0.3381101737
Iteration 5685: Achieved Loss=> 22.9505001011
Gradients: w=>0.0195871051, b=>-0.3379304240
Iteration 5686: Achieved Loss=> 22.9503855509
Gradients: w=>0.0195766920, b=>-0.3377507698
Iteration 5687: Achieved Loss=> 22.9502711225
Gradients: w=>0.0195662844, b=>-0.3375712111
Iteration 5688: Achieved Loss=> 22.9501568157
Gradients: w=>0.0195558824, b=>-0.3373917479
Iteration 5689: Achieved Loss=> 22.9500426305
Gradients: w=>0.0195454859, b=>-0.3372123801
Iteration 5690: Achieved Loss=> 22.9499285666
Gradients: w=>0.0195350949, b=>-0.3370331076
Iteration 5691: Achieved Loss=> 22.9498146239
Gradients: w=>0.0195247094, b=>-0.3368539305
Iteration 5692: Achieved Loss=> 22.9497008024
Gradients: w=>0.0195143295, b=>-0.3366748486
Iteration 5693: Achieved Loss=> 22.9495871019
Gradients: w=>0.0195039551, b=>-0.3364958619
Iteration 5694: Achieved Loss=> 22.9494735222
Gradients: w=>0.0194935862, b=>-0.3363169704
Iteration 5695: Achieved Loss=> 22.9493600633
Gradients: w=>0.0194832228, b=>-0.3361381740
Iteration 5696: Achieved Loss=> 22.9492467249
Gradients: w=>0.0194728649, b=>-0.3359594726
Iteration 5697: Achieved Loss=> 22.9491335071
Gradients: w=>0.0194625125, b=>-0.3357808662
Iteration 5698: Achieved Loss=> 22.9490204096
Gradients: w=>0.0194521657, b=>-0.3356023548
Iteration 5699: Achieved Loss=> 22.9489074323
Gradients: w=>0.0194418243, b=>-0.3354239383
Iteration 5700: Achieved Loss=> 22.9487945751
Gradients: w=>0.0194314884, b=>-0.3352456166
Iteration 5701: Achieved Loss=> 22.9486818379
Gradients: w=>0.0194211580, b=>-0.3350673898
Iteration 5702: Achieved Loss=> 22.9485692205
Gradients: w=>0.0194108332, b=>-0.3348892577
Iteration 5703: Achieved Loss=> 22.9484567228
Gradients: w=>0.0194005138, b=>-0.3347112202
Iteration 5704: Achieved Loss=> 22.9483443447
Gradients: w=>0.0193901999, b=>-0.3345332775
Iteration 5705: Achieved Loss=> 22.9482320860
Gradients: w=>0.0193798914, b=>-0.3343554293
Iteration 5706: Achieved Loss=> 22.9481199467
Gradients: w=>0.0193695885, b=>-0.3341776757
Iteration 5707: Achieved Loss=> 22.9480079266
Gradients: w=>0.0193592910, b=>-0.3340000166
Iteration 5708: Achieved Loss=> 22.9478960256
Gradients: w=>0.0193489990, b=>-0.3338224519
Iteration 5709: Achieved Loss=> 22.9477842435
Gradients: w=>0.0193387125, b=>-0.3336449817
Iteration 5710: Achieved Loss=> 22.9476725802
Gradients: w=>0.0193284314, b=>-0.3334676058
Iteration 5711: Achieved Loss=> 22.9475610356
Gradients: w=>0.0193181559, b=>-0.3332903242
Iteration 5712: Achieved Loss=> 22.9474496096
Gradients: w=>0.0193078857, b=>-0.3331131368
Iteration 5713: Achieved Loss=> 22.9473383021
Gradients: w=>0.0192976211, b=>-0.3329360436
Iteration 5714: Achieved Loss=> 22.9472271128
Gradients: w=>0.0192873619, b=>-0.3327590446
Iteration 5715: Achieved Loss=> 22.9471160418
Gradients: w=>0.0192771081, b=>-0.3325821397
Iteration 5716: Achieved Loss=> 22.9470050888
Gradients: w=>0.0192668598, b=>-0.3324053288
Iteration 5717: Achieved Loss=> 22.9468942537
Gradients: w=>0.0192566170, b=>-0.3322286119
Iteration 5718: Achieved Loss=> 22.9467835365
Gradients: w=>0.0192463795, b=>-0.3320519890
Iteration 5719: Achieved Loss=> 22.9466729370
Gradients: w=>0.0192361476, b=>-0.3318754600
Iteration 5720: Achieved Loss=> 22.9465624550
Gradients: w=>0.0192259210, b=>-0.3316990248
Iteration 5721: Achieved Loss=> 22.9464520905
Gradients: w=>0.0192157000, b=>-0.3315226834
Iteration 5722: Achieved Loss=> 22.9463418432
Gradients: w=>0.0192054843, b=>-0.3313464358
Iteration 5723: Achieved Loss=> 22.9462317132
Gradients: w=>0.0191952741, b=>-0.3311702818
Iteration 5724: Achieved Loss=> 22.9461217003
Gradients: w=>0.0191850693, b=>-0.3309942216
Iteration 5725: Achieved Loss=> 22.9460118042
Gradients: w=>0.0191748699, b=>-0.3308182549
Iteration 5726: Achieved Loss=> 22.9459020250
Gradients: w=>0.0191646760, b=>-0.3306423817
Iteration 5727: Achieved Loss=> 22.9457923625
Gradients: w=>0.0191544874, b=>-0.3304666021
Iteration 5728: Achieved Loss=> 22.9456828166
Gradients: w=>0.0191443043, b=>-0.3302909159
Iteration 5729: Achieved Loss=> 22.9455733871
Gradients: w=>0.0191341266, b=>-0.3301153231
Iteration 5730: Achieved Loss=> 22.9454640739
Gradients: w=>0.0191239543, b=>-0.3299398237
Iteration 5731: Achieved Loss=> 22.9453548769
Gradients: w=>0.0191137874, b=>-0.3297644175
Iteration 5732: Achieved Loss=> 22.9452457960
Gradients: w=>0.0191036260, b=>-0.3295891046
Iteration 5733: Achieved Loss=> 22.9451368311
Gradients: w=>0.0190934699, b=>-0.3294138850
Iteration 5734: Achieved Loss=> 22.9450279819
Gradients: w=>0.0190833192, b=>-0.3292387584
Iteration 5735: Achieved Loss=> 22.9449192485
Gradients: w=>0.0190731739, b=>-0.3290637250
Iteration 5736: Achieved Loss=> 22.9448106307
Gradients: w=>0.0190630341, b=>-0.3288887846
Iteration 5737: Achieved Loss=> 22.9447021283
Gradients: w=>0.0190528996, b=>-0.3287139372
Iteration 5738: Achieved Loss=> 22.9445937412
Gradients: w=>0.0190427705, b=>-0.3285391828
Iteration 5739: Achieved Loss=> 22.9444854694
Gradients: w=>0.0190326467, b=>-0.3283645213
Iteration 5740: Achieved Loss=> 22.9443773127
Gradients: w=>0.0190225284, b=>-0.3281899527
Iteration 5741: Achieved Loss=> 22.9442692709
Gradients: w=>0.0190124154, b=>-0.3280154768
Iteration 5742: Achieved Loss=> 22.9441613440
Gradients: w=>0.0190023079, b=>-0.3278410937
Iteration 5743: Achieved Loss=> 22.9440535318
Gradients: w=>0.0189922056, b=>-0.3276668033
Iteration 5744: Achieved Loss=> 22.9439458342
Gradients: w=>0.0189821088, b=>-0.3274926056
Iteration 5745: Achieved Loss=> 22.9438382510
Gradients: w=>0.0189720173, b=>-0.3273185005
Iteration 5746: Achieved Loss=> 22.9437307823
Gradients: w=>0.0189619312, b=>-0.3271444879
Iteration 5747: Achieved Loss=> 22.9436234277
Gradients: w=>0.0189518505, b=>-0.3269705679
Iteration 5748: Achieved Loss=> 22.9435161873
Gradients: w=>0.0189417751, b=>-0.3267967403
Iteration 5749: Achieved Loss=> 22.9434090609
Gradients: w=>0.0189317051, b=>-0.3266230051
Iteration 5750: Achieved Loss=> 22.9433020484
Gradients: w=>0.0189216404, b=>-0.3264493623
Iteration 5751: Achieved Loss=> 22.9431951496
Gradients: w=>0.0189115811, b=>-0.3262758118
Iteration 5752: Achieved Loss=> 22.9430883644
Gradients: w=>0.0189015271, b=>-0.3261023535
Iteration 5753: Achieved Loss=> 22.9429816928
Gradients: w=>0.0188914785, b=>-0.3259289875
Iteration 5754: Achieved Loss=> 22.9428751345
Gradients: w=>0.0188814352, b=>-0.3257557137
Iteration 5755: Achieved Loss=> 22.9427686895
Gradients: w=>0.0188713972, b=>-0.3255825319
Iteration 5756: Achieved Loss=> 22.9426623577
Gradients: w=>0.0188613646, b=>-0.3254094423
Iteration 5757: Achieved Loss=> 22.9425561389
Gradients: w=>0.0188513373, b=>-0.3252364446
Iteration 5758: Achieved Loss=> 22.9424500329
Gradients: w=>0.0188413154, b=>-0.3250635389
Iteration 5759: Achieved Loss=> 22.9423440398
Gradients: w=>0.0188312988, b=>-0.3248907252
Iteration 5760: Achieved Loss=> 22.9422381594
Gradients: w=>0.0188212875, b=>-0.3247180033
Iteration 5761: Achieved Loss=> 22.9421323915
Gradients: w=>0.0188112815, b=>-0.3245453732
Iteration 5762: Achieved Loss=> 22.9420267360
Gradients: w=>0.0188012808, b=>-0.3243728350
Iteration 5763: Achieved Loss=> 22.9419211928
Gradients: w=>0.0187912855, b=>-0.3242003884
Iteration 5764: Achieved Loss=> 22.9418157619
Gradients: w=>0.0187812955, b=>-0.3240280335
Iteration 5765: Achieved Loss=> 22.9417104430
Gradients: w=>0.0187713108, b=>-0.3238557703
Iteration 5766: Achieved Loss=> 22.9416052360
Gradients: w=>0.0187613313, b=>-0.3236835986
Iteration 5767: Achieved Loss=> 22.9415001409
Gradients: w=>0.0187513572, b=>-0.3235115185
Iteration 5768: Achieved Loss=> 22.9413951575
Gradients: w=>0.0187413885, b=>-0.3233395298
Iteration 5769: Achieved Loss=> 22.9412902857
Gradients: w=>0.0187314250, b=>-0.3231676326
Iteration 5770: Achieved Loss=> 22.9411855254
Gradients: w=>0.0187214668, b=>-0.3229958268
Iteration 5771: Achieved Loss=> 22.9410808764
Gradients: w=>0.0187115138, b=>-0.3228241123
Iteration 5772: Achieved Loss=> 22.9409763387
Gradients: w=>0.0187015662, b=>-0.3226524891
Iteration 5773: Achieved Loss=> 22.9408719121
Gradients: w=>0.0186916239, b=>-0.3224809571
Iteration 5774: Achieved Loss=> 22.9407675964
Gradients: w=>0.0186816869, b=>-0.3223095163
Iteration 5775: Achieved Loss=> 22.9406633917
Gradients: w=>0.0186717551, b=>-0.3221381667
Iteration 5776: Achieved Loss=> 22.9405592978
Gradients: w=>0.0186618286, b=>-0.3219669081
Iteration 5777: Achieved Loss=> 22.9404553145
Gradients: w=>0.0186519074, b=>-0.3217957407
Iteration 5778: Achieved Loss=> 22.9403514417
Gradients: w=>0.0186419915, b=>-0.3216246642
Iteration 5779: Achieved Loss=> 22.9402476793
Gradients: w=>0.0186320808, b=>-0.3214536786
Iteration 5780: Achieved Loss=> 22.9401440273
Gradients: w=>0.0186221755, b=>-0.3212827840
Iteration 5781: Achieved Loss=> 22.9400404854
Gradients: w=>0.0186122753, b=>-0.3211119802
Iteration 5782: Achieved Loss=> 22.9399370536
Gradients: w=>0.0186023805, b=>-0.3209412672
Iteration 5783: Achieved Loss=> 22.9398337317
Gradients: w=>0.0185924909, b=>-0.3207706450
Iteration 5784: Achieved Loss=> 22.9397305196
Gradients: w=>0.0185826065, b=>-0.3206001135
Iteration 5785: Achieved Loss=> 22.9396274173
Gradients: w=>0.0185727275, b=>-0.3204296726
Iteration 5786: Achieved Loss=> 22.9395244246
Gradients: w=>0.0185628536, b=>-0.3202593223
Iteration 5787: Achieved Loss=> 22.9394215413
Gradients: w=>0.0185529850, b=>-0.3200890627
Iteration 5788: Achieved Loss=> 22.9393187674
Gradients: w=>0.0185431217, b=>-0.3199188935
Iteration 5789: Achieved Loss=> 22.9392161028
Gradients: w=>0.0185332636, b=>-0.3197488148
Iteration 5790: Achieved Loss=> 22.9391135473
Gradients: w=>0.0185234108, b=>-0.3195788265
Iteration 5791: Achieved Loss=> 22.9390111008
Gradients: w=>0.0185135632, b=>-0.3194089286
Iteration 5792: Achieved Loss=> 22.9389087631
Gradients: w=>0.0185037208, b=>-0.3192391210
Iteration 5793: Achieved Loss=> 22.9388065343
Gradients: w=>0.0184938836, b=>-0.3190694037
Iteration 5794: Achieved Loss=> 22.9387044142
Gradients: w=>0.0184840517, b=>-0.3188997766
Iteration 5795: Achieved Loss=> 22.9386024026
Gradients: w=>0.0184742250, b=>-0.3187302397
Iteration 5796: Achieved Loss=> 22.9385004994
Gradients: w=>0.0184644036, b=>-0.3185607929
Iteration 5797: Achieved Loss=> 22.9383987045
Gradients: w=>0.0184545873, b=>-0.3183914362
Iteration 5798: Achieved Loss=> 22.9382970179
Gradients: w=>0.0184447763, b=>-0.3182221695
Iteration 5799: Achieved Loss=> 22.9381954394
Gradients: w=>0.0184349705, b=>-0.3180529929
Iteration 5800: Achieved Loss=> 22.9380939688
Gradients: w=>0.0184251699, b=>-0.3178839061
Iteration 5801: Achieved Loss=> 22.9379926061
Gradients: w=>0.0184153745, b=>-0.3177149093
Iteration 5802: Achieved Loss=> 22.9378913511
Gradients: w=>0.0184055843, b=>-0.3175460023
Iteration 5803: Achieved Loss=> 22.9377902038
Gradients: w=>0.0183957993, b=>-0.3173771851
Iteration 5804: Achieved Loss=> 22.9376891639
Gradients: w=>0.0183860196, b=>-0.3172084576
Iteration 5805: Achieved Loss=> 22.9375882315
Gradients: w=>0.0183762450, b=>-0.3170398199
Iteration 5806: Achieved Loss=> 22.9374874064
Gradients: w=>0.0183664756, b=>-0.3168712718
Iteration 5807: Achieved Loss=> 22.9373866885
Gradients: w=>0.0183567114, b=>-0.3167028133
Iteration 5808: Achieved Loss=> 22.9372860776
Gradients: w=>0.0183469525, b=>-0.3165344444
Iteration 5809: Achieved Loss=> 22.9371855736
Gradients: w=>0.0183371987, b=>-0.3163661649
Iteration 5810: Achieved Loss=> 22.9370851765
Gradients: w=>0.0183274500, b=>-0.3161979750
Iteration 5811: Achieved Loss=> 22.9369848861
Gradients: w=>0.0183177066, b=>-0.3160298744
Iteration 5812: Achieved Loss=> 22.9368847023
Gradients: w=>0.0183079683, b=>-0.3158618632
Iteration 5813: Achieved Loss=> 22.9367846251
Gradients: w=>0.0182982353, b=>-0.3156939414
Iteration 5814: Achieved Loss=> 22.9366846541
Gradients: w=>0.0182885074, b=>-0.3155261088
Iteration 5815: Achieved Loss=> 22.9365847895
Gradients: w=>0.0182787846, b=>-0.3153583654
Iteration 5816: Achieved Loss=> 22.9364850310
Gradients: w=>0.0182690671, b=>-0.3151907112
Iteration 5817: Achieved Loss=> 22.9363853786
Gradients: w=>0.0182593547, b=>-0.3150231462
Iteration 5818: Achieved Loss=> 22.9362858320
Gradients: w=>0.0182496474, b=>-0.3148556702
Iteration 5819: Achieved Loss=> 22.9361863913
Gradients: w=>0.0182399454, b=>-0.3146882833
Iteration 5820: Achieved Loss=> 22.9360870563
Gradients: w=>0.0182302484, b=>-0.3145209853
Iteration 5821: Achieved Loss=> 22.9359878269
Gradients: w=>0.0182205567, b=>-0.3143537763
Iteration 5822: Achieved Loss=> 22.9358887030
Gradients: w=>0.0182108701, b=>-0.3141866562
Iteration 5823: Achieved Loss=> 22.9357896844
Gradients: w=>0.0182011886, b=>-0.3140196249
Iteration 5824: Achieved Loss=> 22.9356907711
Gradients: w=>0.0181915123, b=>-0.3138526824
Iteration 5825: Achieved Loss=> 22.9355919630
Gradients: w=>0.0181818411, b=>-0.3136858287
Iteration 5826: Achieved Loss=> 22.9354932598
Gradients: w=>0.0181721751, b=>-0.3135190637
Iteration 5827: Achieved Loss=> 22.9353946616
Gradients: w=>0.0181625142, b=>-0.3133523874
Iteration 5828: Achieved Loss=> 22.9352961682
Gradients: w=>0.0181528585, b=>-0.3131857996
Iteration 5829: Achieved Loss=> 22.9351977795
Gradients: w=>0.0181432079, b=>-0.3130193004
Iteration 5830: Achieved Loss=> 22.9350994954
Gradients: w=>0.0181335624, b=>-0.3128528898
Iteration 5831: Achieved Loss=> 22.9350013157
Gradients: w=>0.0181239220, b=>-0.3126865676
Iteration 5832: Achieved Loss=> 22.9349032404
Gradients: w=>0.0181142868, b=>-0.3125203338
Iteration 5833: Achieved Loss=> 22.9348052694
Gradients: w=>0.0181046567, b=>-0.3123541884
Iteration 5834: Achieved Loss=> 22.9347074025
Gradients: w=>0.0180950317, b=>-0.3121881313
Iteration 5835: Achieved Loss=> 22.9346096396
Gradients: w=>0.0180854118, b=>-0.3120221625
Iteration 5836: Achieved Loss=> 22.9345119807
Gradients: w=>0.0180757971, b=>-0.3118562819
Iteration 5837: Achieved Loss=> 22.9344144256
Gradients: w=>0.0180661874, b=>-0.3116904896
Iteration 5838: Achieved Loss=> 22.9343169741
Gradients: w=>0.0180565829, b=>-0.3115247853
Iteration 5839: Achieved Loss=> 22.9342196263
Gradients: w=>0.0180469834, b=>-0.3113591692
Iteration 5840: Achieved Loss=> 22.9341223819
Gradients: w=>0.0180373891, b=>-0.3111936411
Iteration 5841: Achieved Loss=> 22.9340252409
Gradients: w=>0.0180277999, b=>-0.3110282010
Iteration 5842: Achieved Loss=> 22.9339282032
Gradients: w=>0.0180182157, b=>-0.3108628489
Iteration 5843: Achieved Loss=> 22.9338312686
Gradients: w=>0.0180086367, b=>-0.3106975847
Iteration 5844: Achieved Loss=> 22.9337344370
Gradients: w=>0.0179990628, b=>-0.3105324083
Iteration 5845: Achieved Loss=> 22.9336377084
Gradients: w=>0.0179894939, b=>-0.3103673197
Iteration 5846: Achieved Loss=> 22.9335410826
Gradients: w=>0.0179799301, b=>-0.3102023189
Iteration 5847: Achieved Loss=> 22.9334445595
Gradients: w=>0.0179703715, b=>-0.3100374059
Iteration 5848: Achieved Loss=> 22.9333481390
Gradients: w=>0.0179608179, b=>-0.3098725805
Iteration 5849: Achieved Loss=> 22.9332518210
Gradients: w=>0.0179512693, b=>-0.3097078427
Iteration 5850: Achieved Loss=> 22.9331556054
Gradients: w=>0.0179417259, b=>-0.3095431925
Iteration 5851: Achieved Loss=> 22.9330594920
Gradients: w=>0.0179321875, b=>-0.3093786298
Iteration 5852: Achieved Loss=> 22.9329634809
Gradients: w=>0.0179226542, b=>-0.3092141546
Iteration 5853: Achieved Loss=> 22.9328675718
Gradients: w=>0.0179131260, b=>-0.3090497669
Iteration 5854: Achieved Loss=> 22.9327717646
Gradients: w=>0.0179036028, b=>-0.3088854666
Iteration 5855: Achieved Loss=> 22.9326760593
Gradients: w=>0.0178940847, b=>-0.3087212536
Iteration 5856: Achieved Loss=> 22.9325804557
Gradients: w=>0.0178845717, b=>-0.3085571279
Iteration 5857: Achieved Loss=> 22.9324849537
Gradients: w=>0.0178750637, b=>-0.3083930894
Iteration 5858: Achieved Loss=> 22.9323895533
Gradients: w=>0.0178655607, b=>-0.3082291382
Iteration 5859: Achieved Loss=> 22.9322942542
Gradients: w=>0.0178560629, b=>-0.3080652741
Iteration 5860: Achieved Loss=> 22.9321990565
Gradients: w=>0.0178465700, b=>-0.3079014972
Iteration 5861: Achieved Loss=> 22.9321039599
Gradients: w=>0.0178370822, b=>-0.3077378073
Iteration 5862: Achieved Loss=> 22.9320089645
Gradients: w=>0.0178275995, b=>-0.3075742044
Iteration 5863: Achieved Loss=> 22.9319140700
Gradients: w=>0.0178181218, b=>-0.3074106885
Iteration 5864: Achieved Loss=> 22.9318192764
Gradients: w=>0.0178086491, b=>-0.3072472596
Iteration 5865: Achieved Loss=> 22.9317245835
Gradients: w=>0.0177991815, b=>-0.3070839175
Iteration 5866: Achieved Loss=> 22.9316299913
Gradients: w=>0.0177897189, b=>-0.3069206623
Iteration 5867: Achieved Loss=> 22.9315354997
Gradients: w=>0.0177802614, b=>-0.3067574938
Iteration 5868: Achieved Loss=> 22.9314411085
Gradients: w=>0.0177708088, b=>-0.3065944121
Iteration 5869: Achieved Loss=> 22.9313468176
Gradients: w=>0.0177613613, b=>-0.3064314171
Iteration 5870: Achieved Loss=> 22.9312526270
Gradients: w=>0.0177519188, b=>-0.3062685088
Iteration 5871: Achieved Loss=> 22.9311585365
Gradients: w=>0.0177424814, b=>-0.3061056870
Iteration 5872: Achieved Loss=> 22.9310645460
Gradients: w=>0.0177330489, b=>-0.3059429519
Iteration 5873: Achieved Loss=> 22.9309706554
Gradients: w=>0.0177236215, b=>-0.3057803032
Iteration 5874: Achieved Loss=> 22.9308768646
Gradients: w=>0.0177141991, b=>-0.3056177410
Iteration 5875: Achieved Loss=> 22.9307831735
Gradients: w=>0.0177047817, b=>-0.3054552652
Iteration 5876: Achieved Loss=> 22.9306895820
Gradients: w=>0.0176953693, b=>-0.3052928758
Iteration 5877: Achieved Loss=> 22.9305960900
Gradients: w=>0.0176859619, b=>-0.3051305728
Iteration 5878: Achieved Loss=> 22.9305026974
Gradients: w=>0.0176765595, b=>-0.3049683560
Iteration 5879: Achieved Loss=> 22.9304094040
Gradients: w=>0.0176671621, b=>-0.3048062254
Iteration 5880: Achieved Loss=> 22.9303162099
Gradients: w=>0.0176577697, b=>-0.3046441811
Iteration 5881: Achieved Loss=> 22.9302231147
Gradients: w=>0.0176483822, b=>-0.3044822229
Iteration 5882: Achieved Loss=> 22.9301301186
Gradients: w=>0.0176389998, b=>-0.3043203508
Iteration 5883: Achieved Loss=> 22.9300372213
Gradients: w=>0.0176296224, b=>-0.3041585648
Iteration 5884: Achieved Loss=> 22.9299444227
Gradients: w=>0.0176202499, b=>-0.3039968647
Iteration 5885: Achieved Loss=> 22.9298517228
Gradients: w=>0.0176108825, b=>-0.3038352507
Iteration 5886: Achieved Loss=> 22.9297591214
Gradients: w=>0.0176015200, b=>-0.3036737225
Iteration 5887: Achieved Loss=> 22.9296666185
Gradients: w=>0.0175921625, b=>-0.3035122803
Iteration 5888: Achieved Loss=> 22.9295742138
Gradients: w=>0.0175828099, b=>-0.3033509238
Iteration 5889: Achieved Loss=> 22.9294819074
Gradients: w=>0.0175734624, b=>-0.3031896532
Iteration 5890: Achieved Loss=> 22.9293896992
Gradients: w=>0.0175641198, b=>-0.3030284682
Iteration 5891: Achieved Loss=> 22.9292975889
Gradients: w=>0.0175547822, b=>-0.3028673690
Iteration 5892: Achieved Loss=> 22.9292055766
Gradients: w=>0.0175454495, b=>-0.3027063554
Iteration 5893: Achieved Loss=> 22.9291136620
Gradients: w=>0.0175361218, b=>-0.3025454274
Iteration 5894: Achieved Loss=> 22.9290218452
Gradients: w=>0.0175267991, b=>-0.3023845850
Iteration 5895: Achieved Loss=> 22.9289301259
Gradients: w=>0.0175174813, b=>-0.3022238280
Iteration 5896: Achieved Loss=> 22.9288385042
Gradients: w=>0.0175081684, b=>-0.3020631566
Iteration 5897: Achieved Loss=> 22.9287469798
Gradients: w=>0.0174988606, b=>-0.3019025705
Iteration 5898: Achieved Loss=> 22.9286555528
Gradients: w=>0.0174895576, b=>-0.3017420699
Iteration 5899: Achieved Loss=> 22.9285642229
Gradients: w=>0.0174802597, b=>-0.3015816545
Iteration 5900: Achieved Loss=> 22.9284729901
Gradients: w=>0.0174709666, b=>-0.3014213245
Iteration 5901: Achieved Loss=> 22.9283818543
Gradients: w=>0.0174616785, b=>-0.3012610796
Iteration 5902: Achieved Loss=> 22.9282908153
Gradients: w=>0.0174523953, b=>-0.3011009200
Iteration 5903: Achieved Loss=> 22.9281998732
Gradients: w=>0.0174431171, b=>-0.3009408455
Iteration 5904: Achieved Loss=> 22.9281090277
Gradients: w=>0.0174338438, b=>-0.3007808561
Iteration 5905: Achieved Loss=> 22.9280182787
Gradients: w=>0.0174245755, b=>-0.3006209518
Iteration 5906: Achieved Loss=> 22.9279276263
Gradients: w=>0.0174153120, b=>-0.3004611324
Iteration 5907: Achieved Loss=> 22.9278370702
Gradients: w=>0.0174060535, b=>-0.3003013981
Iteration 5908: Achieved Loss=> 22.9277466103
Gradients: w=>0.0173967999, b=>-0.3001417486
Iteration 5909: Achieved Loss=> 22.9276562466
Gradients: w=>0.0173875512, b=>-0.2999821841
Iteration 5910: Achieved Loss=> 22.9275659790
Gradients: w=>0.0173783075, b=>-0.2998227043
Iteration 5911: Achieved Loss=> 22.9274758073
Gradients: w=>0.0173690686, b=>-0.2996633094
Iteration 5912: Achieved Loss=> 22.9273857315
Gradients: w=>0.0173598347, b=>-0.2995039992
Iteration 5913: Achieved Loss=> 22.9272957514
Gradients: w=>0.0173506057, b=>-0.2993447737
Iteration 5914: Achieved Loss=> 22.9272058669
Gradients: w=>0.0173413816, b=>-0.2991856328
Iteration 5915: Achieved Loss=> 22.9271160781
Gradients: w=>0.0173321624, b=>-0.2990265765
Iteration 5916: Achieved Loss=> 22.9270263846
Gradients: w=>0.0173229480, b=>-0.2988676048
Iteration 5917: Achieved Loss=> 22.9269367865
Gradients: w=>0.0173137386, b=>-0.2987087176
Iteration 5918: Achieved Loss=> 22.9268472836
Gradients: w=>0.0173045341, b=>-0.2985499149
Iteration 5919: Achieved Loss=> 22.9267578759
Gradients: w=>0.0172953345, b=>-0.2983911966
Iteration 5920: Achieved Loss=> 22.9266685632
Gradients: w=>0.0172861398, b=>-0.2982325627
Iteration 5921: Achieved Loss=> 22.9265793455
Gradients: w=>0.0172769499, b=>-0.2980740131
Iteration 5922: Achieved Loss=> 22.9264902226
Gradients: w=>0.0172677650, b=>-0.2979155478
Iteration 5923: Achieved Loss=> 22.9264011944
Gradients: w=>0.0172585849, b=>-0.2977571668
Iteration 5924: Achieved Loss=> 22.9263122608
Gradients: w=>0.0172494097, b=>-0.2975988699
Iteration 5925: Achieved Loss=> 22.9262234218
Gradients: w=>0.0172402394, b=>-0.2974406572
Iteration 5926: Achieved Loss=> 22.9261346773
Gradients: w=>0.0172310739, b=>-0.2972825286
Iteration 5927: Achieved Loss=> 22.9260460270
Gradients: w=>0.0172219134, b=>-0.2971244841
Iteration 5928: Achieved Loss=> 22.9259574710
Gradients: w=>0.0172127577, b=>-0.2969665236
Iteration 5929: Achieved Loss=> 22.9258690091
Gradients: w=>0.0172036069, b=>-0.2968086471
Iteration 5930: Achieved Loss=> 22.9257806413
Gradients: w=>0.0171944609, b=>-0.2966508545
Iteration 5931: Achieved Loss=> 22.9256923674
Gradients: w=>0.0171853198, b=>-0.2964931458
Iteration 5932: Achieved Loss=> 22.9256041873
Gradients: w=>0.0171761835, b=>-0.2963355209
Iteration 5933: Achieved Loss=> 22.9255161010
Gradients: w=>0.0171670522, b=>-0.2961779799
Iteration 5934: Achieved Loss=> 22.9254281083
Gradients: w=>0.0171579256, b=>-0.2960205226
Iteration 5935: Achieved Loss=> 22.9253402091
Gradients: w=>0.0171488039, b=>-0.2958631490
Iteration 5936: Achieved Loss=> 22.9252524033
Gradients: w=>0.0171396871, b=>-0.2957058591
Iteration 5937: Achieved Loss=> 22.9251646909
Gradients: w=>0.0171305751, b=>-0.2955486527
Iteration 5938: Achieved Loss=> 22.9250770718
Gradients: w=>0.0171214680, b=>-0.2953915300
Iteration 5939: Achieved Loss=> 22.9249895457
Gradients: w=>0.0171123657, b=>-0.2952344908
Iteration 5940: Achieved Loss=> 22.9249021128
Gradients: w=>0.0171032682, b=>-0.2950775351
Iteration 5941: Achieved Loss=> 22.9248147727
Gradients: w=>0.0170941756, b=>-0.2949206628
Iteration 5942: Achieved Loss=> 22.9247275255
Gradients: w=>0.0170850878, b=>-0.2947638739
Iteration 5943: Achieved Loss=> 22.9246403710
Gradients: w=>0.0170760049, b=>-0.2946071684
Iteration 5944: Achieved Loss=> 22.9245533092
Gradients: w=>0.0170669267, b=>-0.2944505462
Iteration 5945: Achieved Loss=> 22.9244663399
Gradients: w=>0.0170578534, b=>-0.2942940073
Iteration 5946: Achieved Loss=> 22.9243794631
Gradients: w=>0.0170487850, b=>-0.2941375515
Iteration 5947: Achieved Loss=> 22.9242926786
Gradients: w=>0.0170397213, b=>-0.2939811790
Iteration 5948: Achieved Loss=> 22.9242059864
Gradients: w=>0.0170306625, b=>-0.2938248896
Iteration 5949: Achieved Loss=> 22.9241193863
Gradients: w=>0.0170216084, b=>-0.2936686832
Iteration 5950: Achieved Loss=> 22.9240328783
Gradients: w=>0.0170125592, b=>-0.2935125599
Iteration 5951: Achieved Loss=> 22.9239464622
Gradients: w=>0.0170035148, b=>-0.2933565197
Iteration 5952: Achieved Loss=> 22.9238601380
Gradients: w=>0.0169944752, b=>-0.2932005623
Iteration 5953: Achieved Loss=> 22.9237739055
Gradients: w=>0.0169854405, b=>-0.2930446879
Iteration 5954: Achieved Loss=> 22.9236877647
Gradients: w=>0.0169764105, b=>-0.2928888964
Iteration 5955: Achieved Loss=> 22.9236017155
Gradients: w=>0.0169673853, b=>-0.2927331876
Iteration 5956: Achieved Loss=> 22.9235157577
Gradients: w=>0.0169583649, b=>-0.2925775617
Iteration 5957: Achieved Loss=> 22.9234298914
Gradients: w=>0.0169493493, b=>-0.2924220185
Iteration 5958: Achieved Loss=> 22.9233441162
Gradients: w=>0.0169403385, b=>-0.2922665579
Iteration 5959: Achieved Loss=> 22.9232584323
Gradients: w=>0.0169313325, b=>-0.2921111801
Iteration 5960: Achieved Loss=> 22.9231728395
Gradients: w=>0.0169223313, b=>-0.2919558848
Iteration 5961: Achieved Loss=> 22.9230873376
Gradients: w=>0.0169133349, b=>-0.2918006721
Iteration 5962: Achieved Loss=> 22.9230019266
Gradients: w=>0.0169043432, b=>-0.2916455419
Iteration 5963: Achieved Loss=> 22.9229166064
Gradients: w=>0.0168953564, b=>-0.2914904942
Iteration 5964: Achieved Loss=> 22.9228313769
Gradients: w=>0.0168863743, b=>-0.2913355289
Iteration 5965: Achieved Loss=> 22.9227462380
Gradients: w=>0.0168773970, b=>-0.2911806460
Iteration 5966: Achieved Loss=> 22.9226611896
Gradients: w=>0.0168684244, b=>-0.2910258454
Iteration 5967: Achieved Loss=> 22.9225762316
Gradients: w=>0.0168594566, b=>-0.2908711271
Iteration 5968: Achieved Loss=> 22.9224913639
Gradients: w=>0.0168504936, b=>-0.2907164911
Iteration 5969: Achieved Loss=> 22.9224065864
Gradients: w=>0.0168415354, b=>-0.2905619373
Iteration 5970: Achieved Loss=> 22.9223218991
Gradients: w=>0.0168325819, b=>-0.2904074657
Iteration 5971: Achieved Loss=> 22.9222373017
Gradients: w=>0.0168236332, b=>-0.2902530761
Iteration 5972: Achieved Loss=> 22.9221527943
Gradients: w=>0.0168146892, b=>-0.2900987687
Iteration 5973: Achieved Loss=> 22.9220683767
Gradients: w=>0.0168057500, b=>-0.2899445433
Iteration 5974: Achieved Loss=> 22.9219840489
Gradients: w=>0.0167968156, b=>-0.2897903999
Iteration 5975: Achieved Loss=> 22.9218998107
Gradients: w=>0.0167878859, b=>-0.2896363384
Iteration 5976: Achieved Loss=> 22.9218156620
Gradients: w=>0.0167789609, b=>-0.2894823588
Iteration 5977: Achieved Loss=> 22.9217316028
Gradients: w=>0.0167700407, b=>-0.2893284611
Iteration 5978: Achieved Loss=> 22.9216476329
Gradients: w=>0.0167611252, b=>-0.2891746452
Iteration 5979: Achieved Loss=> 22.9215637523
Gradients: w=>0.0167522145, b=>-0.2890209111
Iteration 5980: Achieved Loss=> 22.9214799609
Gradients: w=>0.0167433085, b=>-0.2888672587
Iteration 5981: Achieved Loss=> 22.9213962585
Gradients: w=>0.0167344073, b=>-0.2887136880
Iteration 5982: Achieved Loss=> 22.9213126451
Gradients: w=>0.0167255107, b=>-0.2885601990
Iteration 5983: Achieved Loss=> 22.9212291206
Gradients: w=>0.0167166189, b=>-0.2884067915
Iteration 5984: Achieved Loss=> 22.9211456848
Gradients: w=>0.0167077319, b=>-0.2882534656
Iteration 5985: Achieved Loss=> 22.9210623378
Gradients: w=>0.0166988495, b=>-0.2881002212
Iteration 5986: Achieved Loss=> 22.9209790793
Gradients: w=>0.0166899719, b=>-0.2879470583
Iteration 5987: Achieved Loss=> 22.9208959094
Gradients: w=>0.0166810990, b=>-0.2877939768
Iteration 5988: Achieved Loss=> 22.9208128278
Gradients: w=>0.0166722308, b=>-0.2876409767
Iteration 5989: Achieved Loss=> 22.9207298346
Gradients: w=>0.0166633674, b=>-0.2874880579
Iteration 5990: Achieved Loss=> 22.9206469296
Gradients: w=>0.0166545086, b=>-0.2873352204
Iteration 5991: Achieved Loss=> 22.9205641127
Gradients: w=>0.0166456546, b=>-0.2871824642
Iteration 5992: Achieved Loss=> 22.9204813839
Gradients: w=>0.0166368052, b=>-0.2870297892
Iteration 5993: Achieved Loss=> 22.9203987430
Gradients: w=>0.0166279606, b=>-0.2868771953
Iteration 5994: Achieved Loss=> 22.9203161899
Gradients: w=>0.0166191206, b=>-0.2867246826
Iteration 5995: Achieved Loss=> 22.9202337246
Gradients: w=>0.0166102854, b=>-0.2865722510
Iteration 5996: Achieved Loss=> 22.9201513469
Gradients: w=>0.0166014549, b=>-0.2864199004
Iteration 5997: Achieved Loss=> 22.9200690569
Gradients: w=>0.0165926290, b=>-0.2862676308
Iteration 5998: Achieved Loss=> 22.9199868542
Gradients: w=>0.0165838079, b=>-0.2861154421
Iteration 5999: Achieved Loss=> 22.9199047390
Gradients: w=>0.0165749914, b=>-0.2859633344
Iteration 6000: Achieved Loss=> 22.9198227111
Gradients: w=>0.0165661796, b=>-0.2858113075
Iteration 6001: Achieved Loss=> 22.9197407703
Gradients: w=>0.0165573726, b=>-0.2856593614
Iteration 6002: Achieved Loss=> 22.9196589166
Gradients: w=>0.0165485701, b=>-0.2855074961
Iteration 6003: Achieved Loss=> 22.9195771500
Gradients: w=>0.0165397724, b=>-0.2853557116
Iteration 6004: Achieved Loss=> 22.9194954703
Gradients: w=>0.0165309794, b=>-0.2852040077
Iteration 6005: Achieved Loss=> 22.9194138774
Gradients: w=>0.0165221910, b=>-0.2850523845
Iteration 6006: Achieved Loss=> 22.9193323712
Gradients: w=>0.0165134073, b=>-0.2849008419
Iteration 6007: Achieved Loss=> 22.9192509517
Gradients: w=>0.0165046283, b=>-0.2847493799
Iteration 6008: Achieved Loss=> 22.9191696187
Gradients: w=>0.0164958539, b=>-0.2845979984
Iteration 6009: Achieved Loss=> 22.9190883721
Gradients: w=>0.0164870842, b=>-0.2844466973
Iteration 6010: Achieved Loss=> 22.9190072120
Gradients: w=>0.0164783191, b=>-0.2842954767
Iteration 6011: Achieved Loss=> 22.9189261381
Gradients: w=>0.0164695588, b=>-0.2841443365
Iteration 6012: Achieved Loss=> 22.9188451504
Gradients: w=>0.0164608030, b=>-0.2839932767
Iteration 6013: Achieved Loss=> 22.9187642487
Gradients: w=>0.0164520520, b=>-0.2838422971
Iteration 6014: Achieved Loss=> 22.9186834331
Gradients: w=>0.0164433056, b=>-0.2836913979
Iteration 6015: Achieved Loss=> 22.9186027034
Gradients: w=>0.0164345638, b=>-0.2835405788
Iteration 6016: Achieved Loss=> 22.9185220595
Gradients: w=>0.0164258267, b=>-0.2833898399
Iteration 6017: Achieved Loss=> 22.9184415013
Gradients: w=>0.0164170942, b=>-0.2832391812
Iteration 6018: Achieved Loss=> 22.9183610287
Gradients: w=>0.0164083664, b=>-0.2830886026
Iteration 6019: Achieved Loss=> 22.9182806417
Gradients: w=>0.0163996432, b=>-0.2829381040
Iteration 6020: Achieved Loss=> 22.9182003401
Gradients: w=>0.0163909246, b=>-0.2827876854
Iteration 6021: Achieved Loss=> 22.9181201239
Gradients: w=>0.0163822107, b=>-0.2826373468
Iteration 6022: Achieved Loss=> 22.9180399930
Gradients: w=>0.0163735014, b=>-0.2824870881
Iteration 6023: Achieved Loss=> 22.9179599472
Gradients: w=>0.0163647968, b=>-0.2823369093
Iteration 6024: Achieved Loss=> 22.9178799865
Gradients: w=>0.0163560967, b=>-0.2821868103
Iteration 6025: Achieved Loss=> 22.9178001109
Gradients: w=>0.0163474013, b=>-0.2820367911
Iteration 6026: Achieved Loss=> 22.9177203201
Gradients: w=>0.0163387106, b=>-0.2818868517
Iteration 6027: Achieved Loss=> 22.9176406141
Gradients: w=>0.0163300244, b=>-0.2817369920
Iteration 6028: Achieved Loss=> 22.9175609929
Gradients: w=>0.0163213428, b=>-0.2815872120
Iteration 6029: Achieved Loss=> 22.9174814563
Gradients: w=>0.0163126659, b=>-0.2814375116
Iteration 6030: Achieved Loss=> 22.9174020042
Gradients: w=>0.0163039936, b=>-0.2812878908
Iteration 6031: Achieved Loss=> 22.9173226366
Gradients: w=>0.0162953259, b=>-0.2811383495
Iteration 6032: Achieved Loss=> 22.9172433534
Gradients: w=>0.0162866628, b=>-0.2809888877
Iteration 6033: Achieved Loss=> 22.9171641545
Gradients: w=>0.0162780043, b=>-0.2808395054
Iteration 6034: Achieved Loss=> 22.9170850397
Gradients: w=>0.0162693504, b=>-0.2806902025
Iteration 6035: Achieved Loss=> 22.9170060090
Gradients: w=>0.0162607011, b=>-0.2805409790
Iteration 6036: Achieved Loss=> 22.9169270624
Gradients: w=>0.0162520565, b=>-0.2803918348
Iteration 6037: Achieved Loss=> 22.9168481996
Gradients: w=>0.0162434164, b=>-0.2802427699
Iteration 6038: Achieved Loss=> 22.9167694207
Gradients: w=>0.0162347809, b=>-0.2800937842
Iteration 6039: Achieved Loss=> 22.9166907255
Gradients: w=>0.0162261500, b=>-0.2799448778
Iteration 6040: Achieved Loss=> 22.9166121140
Gradients: w=>0.0162175236, b=>-0.2797960505
Iteration 6041: Achieved Loss=> 22.9165335861
Gradients: w=>0.0162089019, b=>-0.2796473023
Iteration 6042: Achieved Loss=> 22.9164551416
Gradients: w=>0.0162002848, b=>-0.2794986332
Iteration 6043: Achieved Loss=> 22.9163767805
Gradients: w=>0.0161916722, b=>-0.2793500432
Iteration 6044: Achieved Loss=> 22.9162985027
Gradients: w=>0.0161830642, b=>-0.2792015321
Iteration 6045: Achieved Loss=> 22.9162203081
Gradients: w=>0.0161744608, b=>-0.2790531000
Iteration 6046: Achieved Loss=> 22.9161421966
Gradients: w=>0.0161658619, b=>-0.2789047468
Iteration 6047: Achieved Loss=> 22.9160641682
Gradients: w=>0.0161572677, b=>-0.2787564725
Iteration 6048: Achieved Loss=> 22.9159862227
Gradients: w=>0.0161486780, b=>-0.2786082770
Iteration 6049: Achieved Loss=> 22.9159083600
Gradients: w=>0.0161400928, b=>-0.2784601603
Iteration 6050: Achieved Loss=> 22.9158305801
Gradients: w=>0.0161315123, b=>-0.2783121223
Iteration 6051: Achieved Loss=> 22.9157528829
Gradients: w=>0.0161229363, b=>-0.2781641630
Iteration 6052: Achieved Loss=> 22.9156752683
Gradients: w=>0.0161143648, b=>-0.2780162824
Iteration 6053: Achieved Loss=> 22.9155977362
Gradients: w=>0.0161057979, b=>-0.2778684804
Iteration 6054: Achieved Loss=> 22.9155202865
Gradients: w=>0.0160972356, b=>-0.2777207570
Iteration 6055: Achieved Loss=> 22.9154429191
Gradients: w=>0.0160886778, b=>-0.2775731121
Iteration 6056: Achieved Loss=> 22.9153656340
Gradients: w=>0.0160801246, b=>-0.2774255458
Iteration 6057: Achieved Loss=> 22.9152884310
Gradients: w=>0.0160715759, b=>-0.2772780578
Iteration 6058: Achieved Loss=> 22.9152113101
Gradients: w=>0.0160630317, b=>-0.2771306483
Iteration 6059: Achieved Loss=> 22.9151342712
Gradients: w=>0.0160544921, b=>-0.2769833171
Iteration 6060: Achieved Loss=> 22.9150573141
Gradients: w=>0.0160459571, b=>-0.2768360643
Iteration 6061: Achieved Loss=> 22.9149804389
Gradients: w=>0.0160374266, b=>-0.2766888898
Iteration 6062: Achieved Loss=> 22.9149036454
Gradients: w=>0.0160289006, b=>-0.2765417934
Iteration 6063: Achieved Loss=> 22.9148269335
Gradients: w=>0.0160203791, b=>-0.2763947753
Iteration 6064: Achieved Loss=> 22.9147503031
Gradients: w=>0.0160118622, b=>-0.2762478354
Iteration 6065: Achieved Loss=> 22.9146737542
Gradients: w=>0.0160033498, b=>-0.2761009736
Iteration 6066: Achieved Loss=> 22.9145972867
Gradients: w=>0.0159948419, b=>-0.2759541898
Iteration 6067: Achieved Loss=> 22.9145209005
Gradients: w=>0.0159863386, b=>-0.2758074841
Iteration 6068: Achieved Loss=> 22.9144445954
Gradients: w=>0.0159778397, b=>-0.2756608564
Iteration 6069: Achieved Loss=> 22.9143683715
Gradients: w=>0.0159693454, b=>-0.2755143066
Iteration 6070: Achieved Loss=> 22.9142922286
Gradients: w=>0.0159608556, b=>-0.2753678347
Iteration 6071: Achieved Loss=> 22.9142161666
Gradients: w=>0.0159523704, b=>-0.2752214407
Iteration 6072: Achieved Loss=> 22.9141401855
Gradients: w=>0.0159438896, b=>-0.2750751246
Iteration 6073: Achieved Loss=> 22.9140642852
Gradients: w=>0.0159354133, b=>-0.2749288862
Iteration 6074: Achieved Loss=> 22.9139884655
Gradients: w=>0.0159269416, b=>-0.2747827256
Iteration 6075: Achieved Loss=> 22.9139127264
Gradients: w=>0.0159184743, b=>-0.2746366426
Iteration 6076: Achieved Loss=> 22.9138370678
Gradients: w=>0.0159100116, b=>-0.2744906374
Iteration 6077: Achieved Loss=> 22.9137614897
Gradients: w=>0.0159015533, b=>-0.2743447097
Iteration 6078: Achieved Loss=> 22.9136859919
Gradients: w=>0.0158930996, b=>-0.2741988596
Iteration 6079: Achieved Loss=> 22.9136105743
Gradients: w=>0.0158846503, b=>-0.2740530871
Iteration 6080: Achieved Loss=> 22.9135352370
Gradients: w=>0.0158762055, b=>-0.2739073921
Iteration 6081: Achieved Loss=> 22.9134599797
Gradients: w=>0.0158677653, b=>-0.2737617745
Iteration 6082: Achieved Loss=> 22.9133848023
Gradients: w=>0.0158593295, b=>-0.2736162343
Iteration 6083: Achieved Loss=> 22.9133097049
Gradients: w=>0.0158508982, b=>-0.2734707716
Iteration 6084: Achieved Loss=> 22.9132346874
Gradients: w=>0.0158424713, b=>-0.2733253861
Iteration 6085: Achieved Loss=> 22.9131597496
Gradients: w=>0.0158340490, b=>-0.2731800779
Iteration 6086: Achieved Loss=> 22.9130848914
Gradients: w=>0.0158256311, b=>-0.2730348470
Iteration 6087: Achieved Loss=> 22.9130101128
Gradients: w=>0.0158172177, b=>-0.2728896933
Iteration 6088: Achieved Loss=> 22.9129354137
Gradients: w=>0.0158088088, b=>-0.2727446168
Iteration 6089: Achieved Loss=> 22.9128607940
Gradients: w=>0.0158004044, b=>-0.2725996174
Iteration 6090: Achieved Loss=> 22.9127862536
Gradients: w=>0.0157920044, b=>-0.2724546950
Iteration 6091: Achieved Loss=> 22.9127117924
Gradients: w=>0.0157836089, b=>-0.2723098498
Iteration 6092: Achieved Loss=> 22.9126374104
Gradients: w=>0.0157752178, b=>-0.2721650815
Iteration 6093: Achieved Loss=> 22.9125631075
Gradients: w=>0.0157668312, b=>-0.2720203902
Iteration 6094: Achieved Loss=> 22.9124888836
Gradients: w=>0.0157584491, b=>-0.2718757758
Iteration 6095: Achieved Loss=> 22.9124147385
Gradients: w=>0.0157500714, b=>-0.2717312383
Iteration 6096: Achieved Loss=> 22.9123406723
Gradients: w=>0.0157416982, b=>-0.2715867776
Iteration 6097: Achieved Loss=> 22.9122666848
Gradients: w=>0.0157333294, b=>-0.2714423937
Iteration 6098: Achieved Loss=> 22.9121927759
Gradients: w=>0.0157249651, b=>-0.2712980866
Iteration 6099: Achieved Loss=> 22.9121189456
Gradients: w=>0.0157166052, b=>-0.2711538562
Iteration 6100: Achieved Loss=> 22.9120451938
Gradients: w=>0.0157082498, b=>-0.2710097025
Iteration 6101: Achieved Loss=> 22.9119715204
Gradients: w=>0.0156998988, b=>-0.2708656255
Iteration 6102: Achieved Loss=> 22.9118979253
Gradients: w=>0.0156915523, b=>-0.2707216250
Iteration 6103: Achieved Loss=> 22.9118244084
Gradients: w=>0.0156832102, b=>-0.2705777010
Iteration 6104: Achieved Loss=> 22.9117509697
Gradients: w=>0.0156748725, b=>-0.2704338536
Iteration 6105: Achieved Loss=> 22.9116776090
Gradients: w=>0.0156665392, b=>-0.2702900827
Iteration 6106: Achieved Loss=> 22.9116043263
Gradients: w=>0.0156582104, b=>-0.2701463882
Iteration 6107: Achieved Loss=> 22.9115311215
Gradients: w=>0.0156498860, b=>-0.2700027701
Iteration 6108: Achieved Loss=> 22.9114579946
Gradients: w=>0.0156415661, b=>-0.2698592283
Iteration 6109: Achieved Loss=> 22.9113849453
Gradients: w=>0.0156332506, b=>-0.2697157629
Iteration 6110: Achieved Loss=> 22.9113119737
Gradients: w=>0.0156249394, b=>-0.2695723737
Iteration 6111: Achieved Loss=> 22.9112390797
Gradients: w=>0.0156166327, b=>-0.2694290607
Iteration 6112: Achieved Loss=> 22.9111662632
Gradients: w=>0.0156083305, b=>-0.2692858240
Iteration 6113: Achieved Loss=> 22.9110935240
Gradients: w=>0.0156000326, b=>-0.2691426634
Iteration 6114: Achieved Loss=> 22.9110208622
Gradients: w=>0.0155917391, b=>-0.2689995789
Iteration 6115: Achieved Loss=> 22.9109482776
Gradients: w=>0.0155834501, b=>-0.2688565704
Iteration 6116: Achieved Loss=> 22.9108757702
Gradients: w=>0.0155751654, b=>-0.2687136380
Iteration 6117: Achieved Loss=> 22.9108033399
Gradients: w=>0.0155668852, b=>-0.2685707816
Iteration 6118: Achieved Loss=> 22.9107309865
Gradients: w=>0.0155586094, b=>-0.2684280012
Iteration 6119: Achieved Loss=> 22.9106587101
Gradients: w=>0.0155503379, b=>-0.2682852966
Iteration 6120: Achieved Loss=> 22.9105865105
Gradients: w=>0.0155420709, b=>-0.2681426679
Iteration 6121: Achieved Loss=> 22.9105143876
Gradients: w=>0.0155338083, b=>-0.2680001150
Iteration 6122: Achieved Loss=> 22.9104423414
Gradients: w=>0.0155255500, b=>-0.2678576379
Iteration 6123: Achieved Loss=> 22.9103703718
Gradients: w=>0.0155172962, b=>-0.2677152366
Iteration 6124: Achieved Loss=> 22.9102984786
Gradients: w=>0.0155090467, b=>-0.2675729109
Iteration 6125: Achieved Loss=> 22.9102266619
Gradients: w=>0.0155008016, b=>-0.2674306610
Iteration 6126: Achieved Loss=> 22.9101549216
Gradients: w=>0.0154925609, b=>-0.2672884866
Iteration 6127: Achieved Loss=> 22.9100832575
Gradients: w=>0.0154843246, b=>-0.2671463879
Iteration 6128: Achieved Loss=> 22.9100116696
Gradients: w=>0.0154760926, b=>-0.2670043646
Iteration 6129: Achieved Loss=> 22.9099401577
Gradients: w=>0.0154678651, b=>-0.2668624169
Iteration 6130: Achieved Loss=> 22.9098687219
Gradients: w=>0.0154596419, b=>-0.2667205447
Iteration 6131: Achieved Loss=> 22.9097973621
Gradients: w=>0.0154514231, b=>-0.2665787478
Iteration 6132: Achieved Loss=> 22.9097260780
Gradients: w=>0.0154432086, b=>-0.2664370264
Iteration 6133: Achieved Loss=> 22.9096548698
Gradients: w=>0.0154349985, b=>-0.2662953803
Iteration 6134: Achieved Loss=> 22.9095837372
Gradients: w=>0.0154267928, b=>-0.2661538095
Iteration 6135: Achieved Loss=> 22.9095126803
Gradients: w=>0.0154185914, b=>-0.2660123140
Iteration 6136: Achieved Loss=> 22.9094416989
Gradients: w=>0.0154103945, b=>-0.2658708937
Iteration 6137: Achieved Loss=> 22.9093707929
Gradients: w=>0.0154022018, b=>-0.2657295485
Iteration 6138: Achieved Loss=> 22.9092999623
Gradients: w=>0.0153940135, b=>-0.2655882785
Iteration 6139: Achieved Loss=> 22.9092292070
Gradients: w=>0.0153858296, b=>-0.2654470837
Iteration 6140: Achieved Loss=> 22.9091585269
Gradients: w=>0.0153776500, b=>-0.2653059638
Iteration 6141: Achieved Loss=> 22.9090879220
Gradients: w=>0.0153694748, b=>-0.2651649191
Iteration 6142: Achieved Loss=> 22.9090173921
Gradients: w=>0.0153613039, b=>-0.2650239492
Iteration 6143: Achieved Loss=> 22.9089469372
Gradients: w=>0.0153531374, b=>-0.2648830544
Iteration 6144: Achieved Loss=> 22.9088765571
Gradients: w=>0.0153449752, b=>-0.2647422344
Iteration 6145: Achieved Loss=> 22.9088062519
Gradients: w=>0.0153368173, b=>-0.2646014893
Iteration 6146: Achieved Loss=> 22.9087360214
Gradients: w=>0.0153286638, b=>-0.2644608190
Iteration 6147: Achieved Loss=> 22.9086658656
Gradients: w=>0.0153205146, b=>-0.2643202236
Iteration 6148: Achieved Loss=> 22.9085957843
Gradients: w=>0.0153123697, b=>-0.2641797028
Iteration 6149: Achieved Loss=> 22.9085257775
Gradients: w=>0.0153042292, b=>-0.2640392568
Iteration 6150: Achieved Loss=> 22.9084558452
Gradients: w=>0.0152960930, b=>-0.2638988854
Iteration 6151: Achieved Loss=> 22.9083859872
Gradients: w=>0.0152879611, b=>-0.2637585886
Iteration 6152: Achieved Loss=> 22.9083162034
Gradients: w=>0.0152798336, b=>-0.2636183665
Iteration 6153: Achieved Loss=> 22.9082464938
Gradients: w=>0.0152717104, b=>-0.2634782189
Iteration 6154: Achieved Loss=> 22.9081768583
Gradients: w=>0.0152635915, b=>-0.2633381458
Iteration 6155: Achieved Loss=> 22.9081072969
Gradients: w=>0.0152554769, b=>-0.2631981471
Iteration 6156: Achieved Loss=> 22.9080378094
Gradients: w=>0.0152473666, b=>-0.2630582229
Iteration 6157: Achieved Loss=> 22.9079683957
Gradients: w=>0.0152392606, b=>-0.2629183731
Iteration 6158: Achieved Loss=> 22.9078990558
Gradients: w=>0.0152311590, b=>-0.2627785976
Iteration 6159: Achieved Loss=> 22.9078297897
Gradients: w=>0.0152230616, b=>-0.2626388964
Iteration 6160: Achieved Loss=> 22.9077605971
Gradients: w=>0.0152149686, b=>-0.2624992695
Iteration 6161: Achieved Loss=> 22.9076914781
Gradients: w=>0.0152068798, b=>-0.2623597169
Iteration 6162: Achieved Loss=> 22.9076224326
Gradients: w=>0.0151987954, b=>-0.2622202384
Iteration 6163: Achieved Loss=> 22.9075534605
Gradients: w=>0.0151907152, b=>-0.2620808341
Iteration 6164: Achieved Loss=> 22.9074845617
Gradients: w=>0.0151826394, b=>-0.2619415038
Iteration 6165: Achieved Loss=> 22.9074157362
Gradients: w=>0.0151745678, b=>-0.2618022477
Iteration 6166: Achieved Loss=> 22.9073469837
Gradients: w=>0.0151665005, b=>-0.2616630656
Iteration 6167: Achieved Loss=> 22.9072783044
Gradients: w=>0.0151584376, b=>-0.2615239575
Iteration 6168: Achieved Loss=> 22.9072096981
Gradients: w=>0.0151503789, b=>-0.2613849233
Iteration 6169: Achieved Loss=> 22.9071411647
Gradients: w=>0.0151423245, b=>-0.2612459631
Iteration 6170: Achieved Loss=> 22.9070727042
Gradients: w=>0.0151342743, b=>-0.2611070767
Iteration 6171: Achieved Loss=> 22.9070043164
Gradients: w=>0.0151262285, b=>-0.2609682642
Iteration 6172: Achieved Loss=> 22.9069360013
Gradients: w=>0.0151181869, b=>-0.2608295254
Iteration 6173: Achieved Loss=> 22.9068677589
Gradients: w=>0.0151101496, b=>-0.2606908604
Iteration 6174: Achieved Loss=> 22.9067995890
Gradients: w=>0.0151021166, b=>-0.2605522692
Iteration 6175: Achieved Loss=> 22.9067314915
Gradients: w=>0.0150940879, b=>-0.2604137516
Iteration 6176: Achieved Loss=> 22.9066634664
Gradients: w=>0.0150860634, b=>-0.2602753076
Iteration 6177: Achieved Loss=> 22.9065955137
Gradients: w=>0.0150780432, b=>-0.2601369373
Iteration 6178: Achieved Loss=> 22.9065276332
Gradients: w=>0.0150700272, b=>-0.2599986405
Iteration 6179: Achieved Loss=> 22.9064598248
Gradients: w=>0.0150620155, b=>-0.2598604173
Iteration 6180: Achieved Loss=> 22.9063920885
Gradients: w=>0.0150540081, b=>-0.2597222675
Iteration 6181: Achieved Loss=> 22.9063244242
Gradients: w=>0.0150460049, b=>-0.2595841912
Iteration 6182: Achieved Loss=> 22.9062568318
Gradients: w=>0.0150380060, b=>-0.2594461882
Iteration 6183: Achieved Loss=> 22.9061893113
Gradients: w=>0.0150300114, b=>-0.2593082587
Iteration 6184: Achieved Loss=> 22.9061218626
Gradients: w=>0.0150220209, b=>-0.2591704024
Iteration 6185: Achieved Loss=> 22.9060544856
Gradients: w=>0.0150140348, b=>-0.2590326195
Iteration 6186: Achieved Loss=> 22.9059871801
Gradients: w=>0.0150060528, b=>-0.2588949098
Iteration 6187: Achieved Loss=> 22.9059199462
Gradients: w=>0.0149980752, b=>-0.2587572733
Iteration 6188: Achieved Loss=> 22.9058527838
Gradients: w=>0.0149901017, b=>-0.2586197100
Iteration 6189: Achieved Loss=> 22.9057856928
Gradients: w=>0.0149821325, b=>-0.2584822198
Iteration 6190: Achieved Loss=> 22.9057186731
Gradients: w=>0.0149741676, b=>-0.2583448028
Iteration 6191: Achieved Loss=> 22.9056517247
Gradients: w=>0.0149662068, b=>-0.2582074587
Iteration 6192: Achieved Loss=> 22.9055848474
Gradients: w=>0.0149582504, b=>-0.2580701877
Iteration 6193: Achieved Loss=> 22.9055180411
Gradients: w=>0.0149502981, b=>-0.2579329897
Iteration 6194: Achieved Loss=> 22.9054513060
Gradients: w=>0.0149423500, b=>-0.2577958646
Iteration 6195: Achieved Loss=> 22.9053846417
Gradients: w=>0.0149344062, b=>-0.2576588124
Iteration 6196: Achieved Loss=> 22.9053180483
Gradients: w=>0.0149264666, b=>-0.2575218331
Iteration 6197: Achieved Loss=> 22.9052515257
Gradients: w=>0.0149185313, b=>-0.2573849266
Iteration 6198: Achieved Loss=> 22.9051850738
Gradients: w=>0.0149106001, b=>-0.2572480928
Iteration 6199: Achieved Loss=> 22.9051186925
Gradients: w=>0.0149026732, b=>-0.2571113319
Iteration 6200: Achieved Loss=> 22.9050523818
Gradients: w=>0.0148947505, b=>-0.2569746436
Iteration 6201: Achieved Loss=> 22.9049861416
Gradients: w=>0.0148868320, b=>-0.2568380280
Iteration 6202: Achieved Loss=> 22.9049199718
Gradients: w=>0.0148789177, b=>-0.2567014850
Iteration 6203: Achieved Loss=> 22.9048538724
Gradients: w=>0.0148710076, b=>-0.2565650146
Iteration 6204: Achieved Loss=> 22.9047878432
Gradients: w=>0.0148631017, b=>-0.2564286168
Iteration 6205: Achieved Loss=> 22.9047218842
Gradients: w=>0.0148552000, b=>-0.2562922914
Iteration 6206: Achieved Loss=> 22.9046559953
Gradients: w=>0.0148473025, b=>-0.2561560386
Iteration 6207: Achieved Loss=> 22.9045901764
Gradients: w=>0.0148394093, b=>-0.2560198582
Iteration 6208: Achieved Loss=> 22.9045244275
Gradients: w=>0.0148315202, b=>-0.2558837502
Iteration 6209: Achieved Loss=> 22.9044587485
Gradients: w=>0.0148236353, b=>-0.2557477145
Iteration 6210: Achieved Loss=> 22.9043931393
Gradients: w=>0.0148157546, b=>-0.2556117512
Iteration 6211: Achieved Loss=> 22.9043275999
Gradients: w=>0.0148078781, b=>-0.2554758601
Iteration 6212: Achieved Loss=> 22.9042621301
Gradients: w=>0.0148000058, b=>-0.2553400413
Iteration 6213: Achieved Loss=> 22.9041967299
Gradients: w=>0.0147921376, b=>-0.2552042947
Iteration 6214: Achieved Loss=> 22.9041313992
Gradients: w=>0.0147842737, b=>-0.2550686203
Iteration 6215: Achieved Loss=> 22.9040661380
Gradients: w=>0.0147764139, b=>-0.2549330180
Iteration 6216: Achieved Loss=> 22.9040009462
Gradients: w=>0.0147685583, b=>-0.2547974877
Iteration 6217: Achieved Loss=> 22.9039358236
Gradients: w=>0.0147607069, b=>-0.2546620296
Iteration 6218: Achieved Loss=> 22.9038707703
Gradients: w=>0.0147528596, b=>-0.2545266434
Iteration 6219: Achieved Loss=> 22.9038057861
Gradients: w=>0.0147450166, b=>-0.2543913292
Iteration 6220: Achieved Loss=> 22.9037408710
Gradients: w=>0.0147371777, b=>-0.2542560870
Iteration 6221: Achieved Loss=> 22.9036760249
Gradients: w=>0.0147293429, b=>-0.2541209167
Iteration 6222: Achieved Loss=> 22.9036112477
Gradients: w=>0.0147215123, b=>-0.2539858182
Iteration 6223: Achieved Loss=> 22.9035465394
Gradients: w=>0.0147136859, b=>-0.2538507915
Iteration 6224: Achieved Loss=> 22.9034818999
Gradients: w=>0.0147058637, b=>-0.2537158367
Iteration 6225: Achieved Loss=> 22.9034173291
Gradients: w=>0.0146980456, b=>-0.2535809535
Iteration 6226: Achieved Loss=> 22.9033528269
Gradients: w=>0.0146902317, b=>-0.2534461421
Iteration 6227: Achieved Loss=> 22.9032883933
Gradients: w=>0.0146824219, b=>-0.2533114024
Iteration 6228: Achieved Loss=> 22.9032240281
Gradients: w=>0.0146746163, b=>-0.2531767343
Iteration 6229: Achieved Loss=> 22.9031597314
Gradients: w=>0.0146668148, b=>-0.2530421377
Iteration 6230: Achieved Loss=> 22.9030955031
Gradients: w=>0.0146590175, b=>-0.2529076128
Iteration 6231: Achieved Loss=> 22.9030313430
Gradients: w=>0.0146512243, b=>-0.2527731593
Iteration 6232: Achieved Loss=> 22.9029672511
Gradients: w=>0.0146434352, b=>-0.2526387773
Iteration 6233: Achieved Loss=> 22.9029032273
Gradients: w=>0.0146356503, b=>-0.2525044668
Iteration 6234: Achieved Loss=> 22.9028392716
Gradients: w=>0.0146278696, b=>-0.2523702277
Iteration 6235: Achieved Loss=> 22.9027753839
Gradients: w=>0.0146200930, b=>-0.2522360599
Iteration 6236: Achieved Loss=> 22.9027115641
Gradients: w=>0.0146123205, b=>-0.2521019635
Iteration 6237: Achieved Loss=> 22.9026478121
Gradients: w=>0.0146045521, b=>-0.2519679384
Iteration 6238: Achieved Loss=> 22.9025841279
Gradients: w=>0.0145967879, b=>-0.2518339845
Iteration 6239: Achieved Loss=> 22.9025205114
Gradients: w=>0.0145890278, b=>-0.2517001018
Iteration 6240: Achieved Loss=> 22.9024569625
Gradients: w=>0.0145812718, b=>-0.2515662903
Iteration 6241: Achieved Loss=> 22.9023934812
Gradients: w=>0.0145735199, b=>-0.2514325499
Iteration 6242: Achieved Loss=> 22.9023300674
Gradients: w=>0.0145657722, b=>-0.2512988807
Iteration 6243: Achieved Loss=> 22.9022667209
Gradients: w=>0.0145580286, b=>-0.2511652825
Iteration 6244: Achieved Loss=> 22.9022034418
Gradients: w=>0.0145502891, b=>-0.2510317553
Iteration 6245: Achieved Loss=> 22.9021402300
Gradients: w=>0.0145425537, b=>-0.2508982991
Iteration 6246: Achieved Loss=> 22.9020770853
Gradients: w=>0.0145348225, b=>-0.2507649139
Iteration 6247: Achieved Loss=> 22.9020140078
Gradients: w=>0.0145270953, b=>-0.2506315995
Iteration 6248: Achieved Loss=> 22.9019509973
Gradients: w=>0.0145193723, b=>-0.2504983561
Iteration 6249: Achieved Loss=> 22.9018880538
Gradients: w=>0.0145116533, b=>-0.2503651835
Iteration 6250: Achieved Loss=> 22.9018251772
Gradients: w=>0.0145039385, b=>-0.2502320817
Iteration 6251: Achieved Loss=> 22.9017623674
Gradients: w=>0.0144962277, b=>-0.2500990506
Iteration 6252: Achieved Loss=> 22.9016996244
Gradients: w=>0.0144885211, b=>-0.2499660903
Iteration 6253: Achieved Loss=> 22.9016369481
Gradients: w=>0.0144808185, b=>-0.2498332006
Iteration 6254: Achieved Loss=> 22.9015743385
Gradients: w=>0.0144731201, b=>-0.2497003816
Iteration 6255: Achieved Loss=> 22.9015117953
Gradients: w=>0.0144654257, b=>-0.2495676332
Iteration 6256: Achieved Loss=> 22.9014493187
Gradients: w=>0.0144577355, b=>-0.2494349554
Iteration 6257: Achieved Loss=> 22.9013869085
Gradients: w=>0.0144500493, b=>-0.2493023481
Iteration 6258: Achieved Loss=> 22.9013245646
Gradients: w=>0.0144423672, b=>-0.2491698114
Iteration 6259: Achieved Loss=> 22.9012622870
Gradients: w=>0.0144346892, b=>-0.2490373450
Iteration 6260: Achieved Loss=> 22.9012000755
Gradients: w=>0.0144270153, b=>-0.2489049492
Iteration 6261: Achieved Loss=> 22.9011379303
Gradients: w=>0.0144193454, b=>-0.2487726236
Iteration 6262: Achieved Loss=> 22.9010758510
Gradients: w=>0.0144116797, b=>-0.2486403685
Iteration 6263: Achieved Loss=> 22.9010138378
Gradients: w=>0.0144040180, b=>-0.2485081836
Iteration 6264: Achieved Loss=> 22.9009518905
Gradients: w=>0.0143963604, b=>-0.2483760690
Iteration 6265: Achieved Loss=> 22.9008900090
Gradients: w=>0.0143887068, b=>-0.2482440247
Iteration 6266: Achieved Loss=> 22.9008281933
Gradients: w=>0.0143810573, b=>-0.2481120506
Iteration 6267: Achieved Loss=> 22.9007664433
Gradients: w=>0.0143734119, b=>-0.2479801466
Iteration 6268: Achieved Loss=> 22.9007047590
Gradients: w=>0.0143657706, b=>-0.2478483127
Iteration 6269: Achieved Loss=> 22.9006431402
Gradients: w=>0.0143581333, b=>-0.2477165490
Iteration 6270: Achieved Loss=> 22.9005815869
Gradients: w=>0.0143505001, b=>-0.2475848552
Iteration 6271: Achieved Loss=> 22.9005200991
Gradients: w=>0.0143428709, b=>-0.2474532315
Iteration 6272: Achieved Loss=> 22.9004586766
Gradients: w=>0.0143352458, b=>-0.2473216778
Iteration 6273: Achieved Loss=> 22.9003973194
Gradients: w=>0.0143276247, b=>-0.2471901940
Iteration 6274: Achieved Loss=> 22.9003360274
Gradients: w=>0.0143200077, b=>-0.2470587801
Iteration 6275: Achieved Loss=> 22.9002748006
Gradients: w=>0.0143123948, b=>-0.2469274361
Iteration 6276: Achieved Loss=> 22.9002136388
Gradients: w=>0.0143047859, b=>-0.2467961619
Iteration 6277: Achieved Loss=> 22.9001525421
Gradients: w=>0.0142971810, b=>-0.2466649574
Iteration 6278: Achieved Loss=> 22.9000915103
Gradients: w=>0.0142895802, b=>-0.2465338228
Iteration 6279: Achieved Loss=> 22.9000305434
Gradients: w=>0.0142819834, b=>-0.2464027578
Iteration 6280: Achieved Loss=> 22.8999696413
Gradients: w=>0.0142743907, b=>-0.2462717626
Iteration 6281: Achieved Loss=> 22.8999088039
Gradients: w=>0.0142668020, b=>-0.2461408369
Iteration 6282: Achieved Loss=> 22.8998480313
Gradients: w=>0.0142592173, b=>-0.2460099809
Iteration 6283: Achieved Loss=> 22.8997873232
Gradients: w=>0.0142516367, b=>-0.2458791945
Iteration 6284: Achieved Loss=> 22.8997266796
Gradients: w=>0.0142440600, b=>-0.2457484775
Iteration 6285: Achieved Loss=> 22.8996661005
Gradients: w=>0.0142364875, b=>-0.2456178301
Iteration 6286: Achieved Loss=> 22.8996055858
Gradients: w=>0.0142289189, b=>-0.2454872521
Iteration 6287: Achieved Loss=> 22.8995451354
Gradients: w=>0.0142213544, b=>-0.2453567436
Iteration 6288: Achieved Loss=> 22.8994847493
Gradients: w=>0.0142137939, b=>-0.2452263044
Iteration 6289: Achieved Loss=> 22.8994244274
Gradients: w=>0.0142062374, b=>-0.2450959346
Iteration 6290: Achieved Loss=> 22.8993641695
Gradients: w=>0.0141986849, b=>-0.2449656340
Iteration 6291: Achieved Loss=> 22.8993039758
Gradients: w=>0.0141911365, b=>-0.2448354028
Iteration 6292: Achieved Loss=> 22.8992438460
Gradients: w=>0.0141835920, b=>-0.2447052408
Iteration 6293: Achieved Loss=> 22.8991837801
Gradients: w=>0.0141760516, b=>-0.2445751480
Iteration 6294: Achieved Loss=> 22.8991237781
Gradients: w=>0.0141685152, b=>-0.2444451243
Iteration 6295: Achieved Loss=> 22.8990638399
Gradients: w=>0.0141609827, b=>-0.2443151698
Iteration 6296: Achieved Loss=> 22.8990039654
Gradients: w=>0.0141534543, b=>-0.2441852843
Iteration 6297: Achieved Loss=> 22.8989441545
Gradients: w=>0.0141459299, b=>-0.2440554679
Iteration 6298: Achieved Loss=> 22.8988844072
Gradients: w=>0.0141384095, b=>-0.2439257205
Iteration 6299: Achieved Loss=> 22.8988247234
Gradients: w=>0.0141308931, b=>-0.2437960421
Iteration 6300: Achieved Loss=> 22.8987651031
Gradients: w=>0.0141233807, b=>-0.2436664327
Iteration 6301: Achieved Loss=> 22.8987055461
Gradients: w=>0.0141158723, b=>-0.2435368921
Iteration 6302: Achieved Loss=> 22.8986460525
Gradients: w=>0.0141083678, b=>-0.2434074204
Iteration 6303: Achieved Loss=> 22.8985866221
Gradients: w=>0.0141008674, b=>-0.2432780176
Iteration 6304: Achieved Loss=> 22.8985272548
Gradients: w=>0.0140933709, b=>-0.2431486835
Iteration 6305: Achieved Loss=> 22.8984679507
Gradients: w=>0.0140858785, b=>-0.2430194182
Iteration 6306: Achieved Loss=> 22.8984087096
Gradients: w=>0.0140783900, b=>-0.2428902216
Iteration 6307: Achieved Loss=> 22.8983495315
Gradients: w=>0.0140709055, b=>-0.2427610937
Iteration 6308: Achieved Loss=> 22.8982904162
Gradients: w=>0.0140634249, b=>-0.2426320345
Iteration 6309: Achieved Loss=> 22.8982313639
Gradients: w=>0.0140559484, b=>-0.2425030439
Iteration 6310: Achieved Loss=> 22.8981723742
Gradients: w=>0.0140484758, b=>-0.2423741218
Iteration 6311: Achieved Loss=> 22.8981134473
Gradients: w=>0.0140410072, b=>-0.2422452683
Iteration 6312: Achieved Loss=> 22.8980545831
Gradients: w=>0.0140335426, b=>-0.2421164833
Iteration 6313: Achieved Loss=> 22.8979957814
Gradients: w=>0.0140260819, b=>-0.2419877667
Iteration 6314: Achieved Loss=> 22.8979370422
Gradients: w=>0.0140186252, b=>-0.2418591186
Iteration 6315: Achieved Loss=> 22.8978783654
Gradients: w=>0.0140111725, b=>-0.2417305388
Iteration 6316: Achieved Loss=> 22.8978197510
Gradients: w=>0.0140037237, b=>-0.2416020275
Iteration 6317: Achieved Loss=> 22.8977611990
Gradients: w=>0.0139962789, b=>-0.2414735844
Iteration 6318: Achieved Loss=> 22.8977027091
Gradients: w=>0.0139888381, b=>-0.2413452096
Iteration 6319: Achieved Loss=> 22.8976442815
Gradients: w=>0.0139814012, b=>-0.2412169031
Iteration 6320: Achieved Loss=> 22.8975859159
Gradients: w=>0.0139739682, b=>-0.2410886648
Iteration 6321: Achieved Loss=> 22.8975276124
Gradients: w=>0.0139665392, b=>-0.2409604947
Iteration 6322: Achieved Loss=> 22.8974693709
Gradients: w=>0.0139591142, b=>-0.2408323927
Iteration 6323: Achieved Loss=> 22.8974111912
Gradients: w=>0.0139516931, b=>-0.2407043588
Iteration 6324: Achieved Loss=> 22.8973530735
Gradients: w=>0.0139442760, b=>-0.2405763930
Iteration 6325: Achieved Loss=> 22.8972950174
Gradients: w=>0.0139368628, b=>-0.2404484952
Iteration 6326: Achieved Loss=> 22.8972370231
Gradients: w=>0.0139294535, b=>-0.2403206654
Iteration 6327: Achieved Loss=> 22.8971790905
Gradients: w=>0.0139220482, b=>-0.2401929035
Iteration 6328: Achieved Loss=> 22.8971212194
Gradients: w=>0.0139146468, b=>-0.2400652096
Iteration 6329: Achieved Loss=> 22.8970634099
Gradients: w=>0.0139072493, b=>-0.2399375836
Iteration 6330: Achieved Loss=> 22.8970056618
Gradients: w=>0.0138998558, b=>-0.2398100254
Iteration 6331: Achieved Loss=> 22.8969479751
Gradients: w=>0.0138924662, b=>-0.2396825350
Iteration 6332: Achieved Loss=> 22.8968903497
Gradients: w=>0.0138850806, b=>-0.2395551124
Iteration 6333: Achieved Loss=> 22.8968327855
Gradients: w=>0.0138776988, b=>-0.2394277576
Iteration 6334: Achieved Loss=> 22.8967752826
Gradients: w=>0.0138703210, b=>-0.2393004704
Iteration 6335: Achieved Loss=> 22.8967178407
Gradients: w=>0.0138629471, b=>-0.2391732510
Iteration 6336: Achieved Loss=> 22.8966604600
Gradients: w=>0.0138555772, b=>-0.2390460991
Iteration 6337: Achieved Loss=> 22.8966031402
Gradients: w=>0.0138482111, b=>-0.2389190149
Iteration 6338: Achieved Loss=> 22.8965458814
Gradients: w=>0.0138408490, b=>-0.2387919982
Iteration 6339: Achieved Loss=> 22.8964886834
Gradients: w=>0.0138334907, b=>-0.2386650490
Iteration 6340: Achieved Loss=> 22.8964315462
Gradients: w=>0.0138261364, b=>-0.2385381674
Iteration 6341: Achieved Loss=> 22.8963744698
Gradients: w=>0.0138187860, b=>-0.2384113532
Iteration 6342: Achieved Loss=> 22.8963174540
Gradients: w=>0.0138114395, b=>-0.2382846064
Iteration 6343: Achieved Loss=> 22.8962604988
Gradients: w=>0.0138040970, b=>-0.2381579270
Iteration 6344: Achieved Loss=> 22.8962036042
Gradients: w=>0.0137967583, b=>-0.2380313149
Iteration 6345: Achieved Loss=> 22.8961467700
Gradients: w=>0.0137894235, b=>-0.2379047701
Iteration 6346: Achieved Loss=> 22.8960899963
Gradients: w=>0.0137820926, b=>-0.2377782927
Iteration 6347: Achieved Loss=> 22.8960332829
Gradients: w=>0.0137747656, b=>-0.2376518824
Iteration 6348: Achieved Loss=> 22.8959766298
Gradients: w=>0.0137674425, b=>-0.2375255394
Iteration 6349: Achieved Loss=> 22.8959200370
Gradients: w=>0.0137601234, b=>-0.2373992635
Iteration 6350: Achieved Loss=> 22.8958635042
Gradients: w=>0.0137528081, b=>-0.2372730548
Iteration 6351: Achieved Loss=> 22.8958070316
Gradients: w=>0.0137454966, b=>-0.2371469131
Iteration 6352: Achieved Loss=> 22.8957506190
Gradients: w=>0.0137381891, b=>-0.2370208386
Iteration 6353: Achieved Loss=> 22.8956942664
Gradients: w=>0.0137308855, b=>-0.2368948310
Iteration 6354: Achieved Loss=> 22.8956379736
Gradients: w=>0.0137235857, b=>-0.2367688904
Iteration 6355: Achieved Loss=> 22.8955817408
Gradients: w=>0.0137162898, b=>-0.2366430168
Iteration 6356: Achieved Loss=> 22.8955255676
Gradients: w=>0.0137089978, b=>-0.2365172101
Iteration 6357: Achieved Loss=> 22.8954694542
Gradients: w=>0.0137017097, b=>-0.2363914703
Iteration 6358: Achieved Loss=> 22.8954134005
Gradients: w=>0.0136944255, b=>-0.2362657974
Iteration 6359: Achieved Loss=> 22.8953574063
Gradients: w=>0.0136871451, b=>-0.2361401912
Iteration 6360: Achieved Loss=> 22.8953014716
Gradients: w=>0.0136798686, b=>-0.2360146519
Iteration 6361: Achieved Loss=> 22.8952455964
Gradients: w=>0.0136725959, b=>-0.2358891792
Iteration 6362: Achieved Loss=> 22.8951897806
Gradients: w=>0.0136653272, b=>-0.2357637733
Iteration 6363: Achieved Loss=> 22.8951340242
Gradients: w=>0.0136580623, b=>-0.2356384340
Iteration 6364: Achieved Loss=> 22.8950783270
Gradients: w=>0.0136508012, b=>-0.2355131614
Iteration 6365: Achieved Loss=> 22.8950226890
Gradients: w=>0.0136435440, b=>-0.2353879554
Iteration 6366: Achieved Loss=> 22.8949671101
Gradients: w=>0.0136362907, b=>-0.2352628159
Iteration 6367: Achieved Loss=> 22.8949115903
Gradients: w=>0.0136290412, b=>-0.2351377430
Iteration 6368: Achieved Loss=> 22.8948561296
Gradients: w=>0.0136217956, b=>-0.2350127365
Iteration 6369: Achieved Loss=> 22.8948007277
Gradients: w=>0.0136145538, b=>-0.2348877966
Iteration 6370: Achieved Loss=> 22.8947453848
Gradients: w=>0.0136073159, b=>-0.2347629230
Iteration 6371: Achieved Loss=> 22.8946901007
Gradients: w=>0.0136000819, b=>-0.2346381158
Iteration 6372: Achieved Loss=> 22.8946348754
Gradients: w=>0.0135928516, b=>-0.2345133750
Iteration 6373: Achieved Loss=> 22.8945797088
Gradients: w=>0.0135856253, b=>-0.2343887005
Iteration 6374: Achieved Loss=> 22.8945246008
Gradients: w=>0.0135784027, b=>-0.2342640923
Iteration 6375: Achieved Loss=> 22.8944695514
Gradients: w=>0.0135711840, b=>-0.2341395503
Iteration 6376: Achieved Loss=> 22.8944145605
Gradients: w=>0.0135639692, b=>-0.2340150745
Iteration 6377: Achieved Loss=> 22.8943596281
Gradients: w=>0.0135567582, b=>-0.2338906649
Iteration 6378: Achieved Loss=> 22.8943047541
Gradients: w=>0.0135495510, b=>-0.2337663215
Iteration 6379: Achieved Loss=> 22.8942499384
Gradients: w=>0.0135423476, b=>-0.2336420441
Iteration 6380: Achieved Loss=> 22.8941951809
Gradients: w=>0.0135351481, b=>-0.2335178328
Iteration 6381: Achieved Loss=> 22.8941404817
Gradients: w=>0.0135279524, b=>-0.2333936876
Iteration 6382: Achieved Loss=> 22.8940858406
Gradients: w=>0.0135207605, b=>-0.2332696083
Iteration 6383: Achieved Loss=> 22.8940312576
Gradients: w=>0.0135135725, b=>-0.2331455950
Iteration 6384: Achieved Loss=> 22.8939767326
Gradients: w=>0.0135063882, b=>-0.2330216477
Iteration 6385: Achieved Loss=> 22.8939222656
Gradients: w=>0.0134992078, b=>-0.2328977662
Iteration 6386: Achieved Loss=> 22.8938678564
Gradients: w=>0.0134920312, b=>-0.2327739506
Iteration 6387: Achieved Loss=> 22.8938135051
Gradients: w=>0.0134848584, b=>-0.2326502008
Iteration 6388: Achieved Loss=> 22.8937592116
Gradients: w=>0.0134776895, b=>-0.2325265169
Iteration 6389: Achieved Loss=> 22.8937049758
Gradients: w=>0.0134705243, b=>-0.2324028986
Iteration 6390: Achieved Loss=> 22.8936507976
Gradients: w=>0.0134633630, b=>-0.2322793461
Iteration 6391: Achieved Loss=> 22.8935966771
Gradients: w=>0.0134562054, b=>-0.2321558593
Iteration 6392: Achieved Loss=> 22.8935426140
Gradients: w=>0.0134490517, b=>-0.2320324381
Iteration 6393: Achieved Loss=> 22.8934886085
Gradients: w=>0.0134419018, b=>-0.2319090825
Iteration 6394: Achieved Loss=> 22.8934346603
Gradients: w=>0.0134347557, b=>-0.2317857925
Iteration 6395: Achieved Loss=> 22.8933807695
Gradients: w=>0.0134276133, b=>-0.2316625681
Iteration 6396: Achieved Loss=> 22.8933269360
Gradients: w=>0.0134204748, b=>-0.2315394092
Iteration 6397: Achieved Loss=> 22.8932731597
Gradients: w=>0.0134133401, b=>-0.2314163157
Iteration 6398: Achieved Loss=> 22.8932194405
Gradients: w=>0.0134062091, b=>-0.2312932877
Iteration 6399: Achieved Loss=> 22.8931657785
Gradients: w=>0.0133990820, b=>-0.2311703251
Iteration 6400: Achieved Loss=> 22.8931121735
Gradients: w=>0.0133919586, b=>-0.2310474278
Iteration 6401: Achieved Loss=> 22.8930586254
Gradients: w=>0.0133848390, b=>-0.2309245959
Iteration 6402: Achieved Loss=> 22.8930051343
Gradients: w=>0.0133777232, b=>-0.2308018293
Iteration 6403: Achieved Loss=> 22.8929517001
Gradients: w=>0.0133706112, b=>-0.2306791280
Iteration 6404: Achieved Loss=> 22.8928983227
Gradients: w=>0.0133635030, b=>-0.2305564919
Iteration 6405: Achieved Loss=> 22.8928450020
Gradients: w=>0.0133563985, b=>-0.2304339210
Iteration 6406: Achieved Loss=> 22.8927917379
Gradients: w=>0.0133492979, b=>-0.2303114152
Iteration 6407: Achieved Loss=> 22.8927385305
Gradients: w=>0.0133422010, b=>-0.2301889746
Iteration 6408: Achieved Loss=> 22.8926853797
Gradients: w=>0.0133351079, b=>-0.2300665991
Iteration 6409: Achieved Loss=> 22.8926322853
Gradients: w=>0.0133280185, b=>-0.2299442886
Iteration 6410: Achieved Loss=> 22.8925792474
Gradients: w=>0.0133209329, b=>-0.2298220432
Iteration 6411: Achieved Loss=> 22.8925262659
Gradients: w=>0.0133138511, b=>-0.2296998627
Iteration 6412: Achieved Loss=> 22.8924733407
Gradients: w=>0.0133067730, b=>-0.2295777472
Iteration 6413: Achieved Loss=> 22.8924204717
Gradients: w=>0.0132996988, b=>-0.2294556967
Iteration 6414: Achieved Loss=> 22.8923676590
Gradients: w=>0.0132926282, b=>-0.2293337110
Iteration 6415: Achieved Loss=> 22.8923149023
Gradients: w=>0.0132855615, b=>-0.2292117901
Iteration 6416: Achieved Loss=> 22.8922622018
Gradients: w=>0.0132784984, b=>-0.2290899341
Iteration 6417: Achieved Loss=> 22.8922095573
Gradients: w=>0.0132714392, b=>-0.2289681429
Iteration 6418: Achieved Loss=> 22.8921569687
Gradients: w=>0.0132643837, b=>-0.2288464164
Iteration 6419: Achieved Loss=> 22.8921044361
Gradients: w=>0.0132573319, b=>-0.2287247546
Iteration 6420: Achieved Loss=> 22.8920519593
Gradients: w=>0.0132502839, b=>-0.2286031575
Iteration 6421: Achieved Loss=> 22.8919995382
Gradients: w=>0.0132432397, b=>-0.2284816250
Iteration 6422: Achieved Loss=> 22.8919471729
Gradients: w=>0.0132361992, b=>-0.2283601572
Iteration 6423: Achieved Loss=> 22.8918948633
Gradients: w=>0.0132291624, b=>-0.2282387539
Iteration 6424: Achieved Loss=> 22.8918426092
Gradients: w=>0.0132221294, b=>-0.2281174152
Iteration 6425: Achieved Loss=> 22.8917904107
Gradients: w=>0.0132151001, b=>-0.2279961410
Iteration 6426: Achieved Loss=> 22.8917382677
Gradients: w=>0.0132080745, b=>-0.2278749312
Iteration 6427: Achieved Loss=> 22.8916861801
Gradients: w=>0.0132010527, b=>-0.2277537859
Iteration 6428: Achieved Loss=> 22.8916341479
Gradients: w=>0.0131940346, b=>-0.2276327050
Iteration 6429: Achieved Loss=> 22.8915821710
Gradients: w=>0.0131870203, b=>-0.2275116885
Iteration 6430: Achieved Loss=> 22.8915302493
Gradients: w=>0.0131800096, b=>-0.2273907363
Iteration 6431: Achieved Loss=> 22.8914783828
Gradients: w=>0.0131730027, b=>-0.2272698484
Iteration 6432: Achieved Loss=> 22.8914265715
Gradients: w=>0.0131659996, b=>-0.2271490248
Iteration 6433: Achieved Loss=> 22.8913748152
Gradients: w=>0.0131590001, b=>-0.2270282654
Iteration 6434: Achieved Loss=> 22.8913231140
Gradients: w=>0.0131520044, b=>-0.2269075702
Iteration 6435: Achieved Loss=> 22.8912714677
Gradients: w=>0.0131450124, b=>-0.2267869392
Iteration 6436: Achieved Loss=> 22.8912198763
Gradients: w=>0.0131380241, b=>-0.2266663723
Iteration 6437: Achieved Loss=> 22.8911683398
Gradients: w=>0.0131310395, b=>-0.2265458695
Iteration 6438: Achieved Loss=> 22.8911168580
Gradients: w=>0.0131240586, b=>-0.2264254307
Iteration 6439: Achieved Loss=> 22.8910654310
Gradients: w=>0.0131170815, b=>-0.2263050560
Iteration 6440: Achieved Loss=> 22.8910140586
Gradients: w=>0.0131101081, b=>-0.2261847453
Iteration 6441: Achieved Loss=> 22.8909627408
Gradients: w=>0.0131031383, b=>-0.2260644985
Iteration 6442: Achieved Loss=> 22.8909114776
Gradients: w=>0.0130961723, b=>-0.2259443157
Iteration 6443: Achieved Loss=> 22.8908602689
Gradients: w=>0.0130892100, b=>-0.2258241968
Iteration 6444: Achieved Loss=> 22.8908091146
Gradients: w=>0.0130822513, b=>-0.2257041417
Iteration 6445: Achieved Loss=> 22.8907580147
Gradients: w=>0.0130752964, b=>-0.2255841505
Iteration 6446: Achieved Loss=> 22.8907069691
Gradients: w=>0.0130683452, b=>-0.2254642230
Iteration 6447: Achieved Loss=> 22.8906559777
Gradients: w=>0.0130613976, b=>-0.2253443593
Iteration 6448: Achieved Loss=> 22.8906050406
Gradients: w=>0.0130544538, b=>-0.2252245593
Iteration 6449: Achieved Loss=> 22.8905541576
Gradients: w=>0.0130475137, b=>-0.2251048230
Iteration 6450: Achieved Loss=> 22.8905033287
Gradients: w=>0.0130405772, b=>-0.2249851504
Iteration 6451: Achieved Loss=> 22.8904525538
Gradients: w=>0.0130336444, b=>-0.2248655414
Iteration 6452: Achieved Loss=> 22.8904018329
Gradients: w=>0.0130267153, b=>-0.2247459960
Iteration 6453: Achieved Loss=> 22.8903511659
Gradients: w=>0.0130197899, b=>-0.2246265141
Iteration 6454: Achieved Loss=> 22.8903005528
Gradients: w=>0.0130128682, b=>-0.2245070957
Iteration 6455: Achieved Loss=> 22.8902499935
Gradients: w=>0.0130059502, b=>-0.2243877409
Iteration 6456: Achieved Loss=> 22.8901994879
Gradients: w=>0.0129990358, b=>-0.2242684495
Iteration 6457: Achieved Loss=> 22.8901490360
Gradients: w=>0.0129921251, b=>-0.2241492215
Iteration 6458: Achieved Loss=> 22.8900986377
Gradients: w=>0.0129852181, b=>-0.2240300569
Iteration 6459: Achieved Loss=> 22.8900482930
Gradients: w=>0.0129783148, b=>-0.2239109556
Iteration 6460: Achieved Loss=> 22.8899980018
Gradients: w=>0.0129714151, b=>-0.2237919177
Iteration 6461: Achieved Loss=> 22.8899477641
Gradients: w=>0.0129645191, b=>-0.2236729430
Iteration 6462: Achieved Loss=> 22.8898975798
Gradients: w=>0.0129576268, b=>-0.2235540316
Iteration 6463: Achieved Loss=> 22.8898474488
Gradients: w=>0.0129507381, b=>-0.2234351834
Iteration 6464: Achieved Loss=> 22.8897973711
Gradients: w=>0.0129438531, b=>-0.2233163984
Iteration 6465: Achieved Loss=> 22.8897473467
Gradients: w=>0.0129369717, b=>-0.2231976766
Iteration 6466: Achieved Loss=> 22.8896973754
Gradients: w=>0.0129300940, b=>-0.2230790178
Iteration 6467: Achieved Loss=> 22.8896474572
Gradients: w=>0.0129232200, b=>-0.2229604222
Iteration 6468: Achieved Loss=> 22.8895975921
Gradients: w=>0.0129163496, b=>-0.2228418896
Iteration 6469: Achieved Loss=> 22.8895477800
Gradients: w=>0.0129094829, b=>-0.2227234200
Iteration 6470: Achieved Loss=> 22.8894980209
Gradients: w=>0.0129026198, b=>-0.2226050134
Iteration 6471: Achieved Loss=> 22.8894483146
Gradients: w=>0.0128957604, b=>-0.2224866697
Iteration 6472: Achieved Loss=> 22.8893986612
Gradients: w=>0.0128889046, b=>-0.2223683890
Iteration 6473: Achieved Loss=> 22.8893490606
Gradients: w=>0.0128820525, b=>-0.2222501711
Iteration 6474: Achieved Loss=> 22.8892995127
Gradients: w=>0.0128752040, b=>-0.2221320161
Iteration 6475: Achieved Loss=> 22.8892500174
Gradients: w=>0.0128683591, b=>-0.2220139239
Iteration 6476: Achieved Loss=> 22.8892005748
Gradients: w=>0.0128615179, b=>-0.2218958945
Iteration 6477: Achieved Loss=> 22.8891511847
Gradients: w=>0.0128546803, b=>-0.2217779278
Iteration 6478: Achieved Loss=> 22.8891018471
Gradients: w=>0.0128478464, b=>-0.2216600239
Iteration 6479: Achieved Loss=> 22.8890525620
Gradients: w=>0.0128410161, b=>-0.2215421826
Iteration 6480: Achieved Loss=> 22.8890033293
Gradients: w=>0.0128341894, b=>-0.2214244040
Iteration 6481: Achieved Loss=> 22.8889541489
Gradients: w=>0.0128273664, b=>-0.2213066880
Iteration 6482: Achieved Loss=> 22.8889050207
Gradients: w=>0.0128205469, b=>-0.2211890345
Iteration 6483: Achieved Loss=> 22.8888559448
Gradients: w=>0.0128137311, b=>-0.2210714437
Iteration 6484: Achieved Loss=> 22.8888069211
Gradients: w=>0.0128069190, b=>-0.2209539153
Iteration 6485: Achieved Loss=> 22.8887579495
Gradients: w=>0.0128001104, b=>-0.2208364494
Iteration 6486: Achieved Loss=> 22.8887090299
Gradients: w=>0.0127933055, b=>-0.2207190460
Iteration 6487: Achieved Loss=> 22.8886601623
Gradients: w=>0.0127865042, b=>-0.2206017050
Iteration 6488: Achieved Loss=> 22.8886113467
Gradients: w=>0.0127797065, b=>-0.2204844263
Iteration 6489: Achieved Loss=> 22.8885625830
Gradients: w=>0.0127729124, b=>-0.2203672100
Iteration 6490: Achieved Loss=> 22.8885138711
Gradients: w=>0.0127661219, b=>-0.2202500561
Iteration 6491: Achieved Loss=> 22.8884652109
Gradients: w=>0.0127593351, b=>-0.2201329644
Iteration 6492: Achieved Loss=> 22.8884166025
Gradients: w=>0.0127525518, b=>-0.2200159349
Iteration 6493: Achieved Loss=> 22.8883680458
Gradients: w=>0.0127457721, b=>-0.2198989677
Iteration 6494: Achieved Loss=> 22.8883195407
Gradients: w=>0.0127389961, b=>-0.2197820627
Iteration 6495: Achieved Loss=> 22.8882710871
Gradients: w=>0.0127322237, b=>-0.2196652198
Iteration 6496: Achieved Loss=> 22.8882226851
Gradients: w=>0.0127254548, b=>-0.2195484390
Iteration 6497: Achieved Loss=> 22.8881743345
Gradients: w=>0.0127186896, b=>-0.2194317203
Iteration 6498: Achieved Loss=> 22.8881260353
Gradients: w=>0.0127119279, b=>-0.2193150637
Iteration 6499: Achieved Loss=> 22.8880777874
Gradients: w=>0.0127051699, b=>-0.2191984691
Iteration 6500: Achieved Loss=> 22.8880295908
Gradients: w=>0.0126984154, b=>-0.2190819364
Iteration 6501: Achieved Loss=> 22.8879814455
Gradients: w=>0.0126916646, b=>-0.2189654657
Iteration 6502: Achieved Loss=> 22.8879333513
Gradients: w=>0.0126849173, b=>-0.2188490570
Iteration 6503: Achieved Loss=> 22.8878853083
Gradients: w=>0.0126781736, b=>-0.2187327101
Iteration 6504: Achieved Loss=> 22.8878373163
Gradients: w=>0.0126714335, b=>-0.2186164251
Iteration 6505: Achieved Loss=> 22.8877893754
Gradients: w=>0.0126646970, b=>-0.2185002019
Iteration 6506: Achieved Loss=> 22.8877414854
Gradients: w=>0.0126579640, b=>-0.2183840405
Iteration 6507: Achieved Loss=> 22.8876936463
Gradients: w=>0.0126512347, b=>-0.2182679408
Iteration 6508: Achieved Loss=> 22.8876458580
Gradients: w=>0.0126445089, b=>-0.2181519029
Iteration 6509: Achieved Loss=> 22.8875981206
Gradients: w=>0.0126377867, b=>-0.2180359266
Iteration 6510: Achieved Loss=> 22.8875504339
Gradients: w=>0.0126310680, b=>-0.2179200120
Iteration 6511: Achieved Loss=> 22.8875027979
Gradients: w=>0.0126243530, b=>-0.2178041591
Iteration 6512: Achieved Loss=> 22.8874552125
Gradients: w=>0.0126176415, b=>-0.2176883677
Iteration 6513: Achieved Loss=> 22.8874076777
Gradients: w=>0.0126109335, b=>-0.2175726379
Iteration 6514: Achieved Loss=> 22.8873601934
Gradients: w=>0.0126042292, b=>-0.2174569696
Iteration 6515: Achieved Loss=> 22.8873127597
Gradients: w=>0.0125975284, b=>-0.2173413628
Iteration 6516: Achieved Loss=> 22.8872653763
Gradients: w=>0.0125908312, b=>-0.2172258174
Iteration 6517: Achieved Loss=> 22.8872180433
Gradients: w=>0.0125841375, b=>-0.2171103335
Iteration 6518: Achieved Loss=> 22.8871707606
Gradients: w=>0.0125774474, b=>-0.2169949110
Iteration 6519: Achieved Loss=> 22.8871235282
Gradients: w=>0.0125707608, b=>-0.2168795498
Iteration 6520: Achieved Loss=> 22.8870763460
Gradients: w=>0.0125640778, b=>-0.2167642500
Iteration 6521: Achieved Loss=> 22.8870292139
Gradients: w=>0.0125573984, b=>-0.2166490115
Iteration 6522: Achieved Loss=> 22.8869821319
Gradients: w=>0.0125507225, b=>-0.2165338342
Iteration 6523: Achieved Loss=> 22.8869351000
Gradients: w=>0.0125440501, b=>-0.2164187182
Iteration 6524: Achieved Loss=> 22.8868881181
Gradients: w=>0.0125373813, b=>-0.2163036634
Iteration 6525: Achieved Loss=> 22.8868411861
Gradients: w=>0.0125307161, b=>-0.2161886697
Iteration 6526: Achieved Loss=> 22.8867943040
Gradients: w=>0.0125240543, b=>-0.2160737372
Iteration 6527: Achieved Loss=> 22.8867474718
Gradients: w=>0.0125173962, b=>-0.2159588657
Iteration 6528: Achieved Loss=> 22.8867006893
Gradients: w=>0.0125107415, b=>-0.2158440554
Iteration 6529: Achieved Loss=> 22.8866539565
Gradients: w=>0.0125040904, b=>-0.2157293060
Iteration 6530: Achieved Loss=> 22.8866072735
Gradients: w=>0.0124974429, b=>-0.2156146177
Iteration 6531: Achieved Loss=> 22.8865606400
Gradients: w=>0.0124907989, b=>-0.2154999904
Iteration 6532: Achieved Loss=> 22.8865140561
Gradients: w=>0.0124841584, b=>-0.2153854240
Iteration 6533: Achieved Loss=> 22.8864675218
Gradients: w=>0.0124775214, b=>-0.2152709185
Iteration 6534: Achieved Loss=> 22.8864210369
Gradients: w=>0.0124708880, b=>-0.2151564738
Iteration 6535: Achieved Loss=> 22.8863746014
Gradients: w=>0.0124642581, b=>-0.2150420900
Iteration 6536: Achieved Loss=> 22.8863282153
Gradients: w=>0.0124576317, b=>-0.2149277671
Iteration 6537: Achieved Loss=> 22.8862818784
Gradients: w=>0.0124510088, b=>-0.2148135049
Iteration 6538: Achieved Loss=> 22.8862355909
Gradients: w=>0.0124443895, b=>-0.2146993034
Iteration 6539: Achieved Loss=> 22.8861893525
Gradients: w=>0.0124377737, b=>-0.2145851627
Iteration 6540: Achieved Loss=> 22.8861431633
Gradients: w=>0.0124311614, b=>-0.2144710826
Iteration 6541: Achieved Loss=> 22.8860970232
Gradients: w=>0.0124245526, b=>-0.2143570632
Iteration 6542: Achieved Loss=> 22.8860509321
Gradients: w=>0.0124179473, b=>-0.2142431044
Iteration 6543: Achieved Loss=> 22.8860048901
Gradients: w=>0.0124113455, b=>-0.2141292062
Iteration 6544: Achieved Loss=> 22.8859588969
Gradients: w=>0.0124047473, b=>-0.2140153685
Iteration 6545: Achieved Loss=> 22.8859129527
Gradients: w=>0.0123981525, b=>-0.2139015914
Iteration 6546: Achieved Loss=> 22.8858670573
Gradients: w=>0.0123915613, b=>-0.2137878748
Iteration 6547: Achieved Loss=> 22.8858212107
Gradients: w=>0.0123849736, b=>-0.2136742186
Iteration 6548: Achieved Loss=> 22.8857754128
Gradients: w=>0.0123783893, b=>-0.2135606228
Iteration 6549: Achieved Loss=> 22.8857296636
Gradients: w=>0.0123718086, b=>-0.2134470874
Iteration 6550: Achieved Loss=> 22.8856839630
Gradients: w=>0.0123652314, b=>-0.2133336124
Iteration 6551: Achieved Loss=> 22.8856383110
Gradients: w=>0.0123586576, b=>-0.2132201977
Iteration 6552: Achieved Loss=> 22.8855927076
Gradients: w=>0.0123520874, b=>-0.2131068433
Iteration 6553: Achieved Loss=> 22.8855471526
Gradients: w=>0.0123455206, b=>-0.2129935491
Iteration 6554: Achieved Loss=> 22.8855016460
Gradients: w=>0.0123389574, b=>-0.2128803152
Iteration 6555: Achieved Loss=> 22.8854561878
Gradients: w=>0.0123323976, b=>-0.2127671415
Iteration 6556: Achieved Loss=> 22.8854107780
Gradients: w=>0.0123258413, b=>-0.2126540280
Iteration 6557: Achieved Loss=> 22.8853654164
Gradients: w=>0.0123192885, b=>-0.2125409746
Iteration 6558: Achieved Loss=> 22.8853201030
Gradients: w=>0.0123127392, b=>-0.2124279813
Iteration 6559: Achieved Loss=> 22.8852748378
Gradients: w=>0.0123061934, b=>-0.2123150480
Iteration 6560: Achieved Loss=> 22.8852296207
Gradients: w=>0.0122996511, b=>-0.2122021748
Iteration 6561: Achieved Loss=> 22.8851844516
Gradients: w=>0.0122931122, b=>-0.2120893616
Iteration 6562: Achieved Loss=> 22.8851393306
Gradients: w=>0.0122865768, b=>-0.2119766084
Iteration 6563: Achieved Loss=> 22.8850942575
Gradients: w=>0.0122800449, b=>-0.2118639152
Iteration 6564: Achieved Loss=> 22.8850492324
Gradients: w=>0.0122735164, b=>-0.2117512818
Iteration 6565: Achieved Loss=> 22.8850042551
Gradients: w=>0.0122669914, b=>-0.2116387083
Iteration 6566: Achieved Loss=> 22.8849593256
Gradients: w=>0.0122604699, b=>-0.2115261947
Iteration 6567: Achieved Loss=> 22.8849144439
Gradients: w=>0.0122539519, b=>-0.2114137409
Iteration 6568: Achieved Loss=> 22.8848696099
Gradients: w=>0.0122474373, b=>-0.2113013468
Iteration 6569: Achieved Loss=> 22.8848248236
Gradients: w=>0.0122409262, b=>-0.2111890126
Iteration 6570: Achieved Loss=> 22.8847800848
Gradients: w=>0.0122344185, b=>-0.2110767380
Iteration 6571: Achieved Loss=> 22.8847353936
Gradients: w=>0.0122279144, b=>-0.2109645231
Iteration 6572: Achieved Loss=> 22.8846907499
Gradients: w=>0.0122214136, b=>-0.2108523679
Iteration 6573: Achieved Loss=> 22.8846461537
Gradients: w=>0.0122149163, b=>-0.2107402723
Iteration 6574: Achieved Loss=> 22.8846016049
Gradients: w=>0.0122084225, b=>-0.2106282363
Iteration 6575: Achieved Loss=> 22.8845571034
Gradients: w=>0.0122019321, b=>-0.2105162599
Iteration 6576: Achieved Loss=> 22.8845126493
Gradients: w=>0.0121954452, b=>-0.2104043430
Iteration 6577: Achieved Loss=> 22.8844682424
Gradients: w=>0.0121889617, b=>-0.2102924856
Iteration 6578: Achieved Loss=> 22.8844238827
Gradients: w=>0.0121824817, b=>-0.2101806876
Iteration 6579: Achieved Loss=> 22.8843795701
Gradients: w=>0.0121760051, b=>-0.2100689491
Iteration 6580: Achieved Loss=> 22.8843353047
Gradients: w=>0.0121695320, b=>-0.2099572700
Iteration 6581: Achieved Loss=> 22.8842910863
Gradients: w=>0.0121630623, b=>-0.2098456503
Iteration 6582: Achieved Loss=> 22.8842469149
Gradients: w=>0.0121565960, b=>-0.2097340899
Iteration 6583: Achieved Loss=> 22.8842027904
Gradients: w=>0.0121501332, b=>-0.2096225888
Iteration 6584: Achieved Loss=> 22.8841587129
Gradients: w=>0.0121436738, b=>-0.2095111470
Iteration 6585: Achieved Loss=> 22.8841146822
Gradients: w=>0.0121372179, b=>-0.2093997645
Iteration 6586: Achieved Loss=> 22.8840706983
Gradients: w=>0.0121307654, b=>-0.2092884411
Iteration 6587: Achieved Loss=> 22.8840267612
Gradients: w=>0.0121243163, b=>-0.2091771770
Iteration 6588: Achieved Loss=> 22.8839828708
Gradients: w=>0.0121178706, b=>-0.2090659720
Iteration 6589: Achieved Loss=> 22.8839390270
Gradients: w=>0.0121114284, b=>-0.2089548261
Iteration 6590: Achieved Loss=> 22.8838952299
Gradients: w=>0.0121049896, b=>-0.2088437393
Iteration 6591: Achieved Loss=> 22.8838514792
Gradients: w=>0.0120985542, b=>-0.2087327115
Iteration 6592: Achieved Loss=> 22.8838077751
Gradients: w=>0.0120921222, b=>-0.2086217428
Iteration 6593: Achieved Loss=> 22.8837641175
Gradients: w=>0.0120856937, b=>-0.2085108331
Iteration 6594: Achieved Loss=> 22.8837205063
Gradients: w=>0.0120792686, b=>-0.2083999823
Iteration 6595: Achieved Loss=> 22.8836769414
Gradients: w=>0.0120728469, b=>-0.2082891905
Iteration 6596: Achieved Loss=> 22.8836334228
Gradients: w=>0.0120664286, b=>-0.2081784576
Iteration 6597: Achieved Loss=> 22.8835899505
Gradients: w=>0.0120600137, b=>-0.2080677835
Iteration 6598: Achieved Loss=> 22.8835465244
Gradients: w=>0.0120536022, b=>-0.2079571683
Iteration 6599: Achieved Loss=> 22.8835031445
Gradients: w=>0.0120471941, b=>-0.2078466119
Iteration 6600: Achieved Loss=> 22.8834598106
Gradients: w=>0.0120407895, b=>-0.2077361142
Iteration 6601: Achieved Loss=> 22.8834165229
Gradients: w=>0.0120343882, b=>-0.2076256753
Iteration 6602: Achieved Loss=> 22.8833732811
Gradients: w=>0.0120279904, b=>-0.2075152952
Iteration 6603: Achieved Loss=> 22.8833300853
Gradients: w=>0.0120215959, b=>-0.2074049737
Iteration 6604: Achieved Loss=> 22.8832869355
Gradients: w=>0.0120152049, b=>-0.2072947108
Iteration 6605: Achieved Loss=> 22.8832438315
Gradients: w=>0.0120088172, b=>-0.2071845066
Iteration 6606: Achieved Loss=> 22.8832007733
Gradients: w=>0.0120024330, b=>-0.2070743609
Iteration 6607: Achieved Loss=> 22.8831577609
Gradients: w=>0.0119960521, b=>-0.2069642739
Iteration 6608: Achieved Loss=> 22.8831147942
Gradients: w=>0.0119896746, b=>-0.2068542453
Iteration 6609: Achieved Loss=> 22.8830718732
Gradients: w=>0.0119833006, b=>-0.2067442752
Iteration 6610: Achieved Loss=> 22.8830289978
Gradients: w=>0.0119769299, b=>-0.2066343636
Iteration 6611: Achieved Loss=> 22.8829861680
Gradients: w=>0.0119705626, b=>-0.2065245105
Iteration 6612: Achieved Loss=> 22.8829433837
Gradients: w=>0.0119641987, b=>-0.2064147157
Iteration 6613: Achieved Loss=> 22.8829006448
Gradients: w=>0.0119578381, b=>-0.2063049793
Iteration 6614: Achieved Loss=> 22.8828579515
Gradients: w=>0.0119514810, b=>-0.2061953012
Iteration 6615: Achieved Loss=> 22.8828153035
Gradients: w=>0.0119451272, b=>-0.2060856815
Iteration 6616: Achieved Loss=> 22.8827727008
Gradients: w=>0.0119387768, b=>-0.2059761200
Iteration 6617: Achieved Loss=> 22.8827301434
Gradients: w=>0.0119324298, b=>-0.2058666168
Iteration 6618: Achieved Loss=> 22.8826876313
Gradients: w=>0.0119260861, b=>-0.2057571718
Iteration 6619: Achieved Loss=> 22.8826451643
Gradients: w=>0.0119197459, b=>-0.2056477849
Iteration 6620: Achieved Loss=> 22.8826027425
Gradients: w=>0.0119134090, b=>-0.2055384563
Iteration 6621: Achieved Loss=> 22.8825603658
Gradients: w=>0.0119070754, b=>-0.2054291857
Iteration 6622: Achieved Loss=> 22.8825180341
Gradients: w=>0.0119007453, b=>-0.2053199732
Iteration 6623: Achieved Loss=> 22.8824757474
Gradients: w=>0.0118944185, b=>-0.2052108188
Iteration 6624: Achieved Loss=> 22.8824335057
Gradients: w=>0.0118880950, b=>-0.2051017225
Iteration 6625: Achieved Loss=> 22.8823913089
Gradients: w=>0.0118817749, b=>-0.2049926841
Iteration 6626: Achieved Loss=> 22.8823491569
Gradients: w=>0.0118754582, b=>-0.2048837037
Iteration 6627: Achieved Loss=> 22.8823070498
Gradients: w=>0.0118691449, b=>-0.2047747812
Iteration 6628: Achieved Loss=> 22.8822649874
Gradients: w=>0.0118628349, b=>-0.2046659167
Iteration 6629: Achieved Loss=> 22.8822229697
Gradients: w=>0.0118565282, b=>-0.2045571100
Iteration 6630: Achieved Loss=> 22.8821809966
Gradients: w=>0.0118502249, b=>-0.2044483612
Iteration 6631: Achieved Loss=> 22.8821390682
Gradients: w=>0.0118439250, b=>-0.2043396701
Iteration 6632: Achieved Loss=> 22.8820971844
Gradients: w=>0.0118376284, b=>-0.2042310369
Iteration 6633: Achieved Loss=> 22.8820553451
Gradients: w=>0.0118313351, b=>-0.2041224614
Iteration 6634: Achieved Loss=> 22.8820135502
Gradients: w=>0.0118250452, b=>-0.2040139436
Iteration 6635: Achieved Loss=> 22.8819717998
Gradients: w=>0.0118187587, b=>-0.2039054836
Iteration 6636: Achieved Loss=> 22.8819300938
Gradients: w=>0.0118124755, b=>-0.2037970811
Iteration 6637: Achieved Loss=> 22.8818884320
Gradients: w=>0.0118061956, b=>-0.2036887364
Iteration 6638: Achieved Loss=> 22.8818468146
Gradients: w=>0.0117999191, b=>-0.2035804492
Iteration 6639: Achieved Loss=> 22.8818052414
Gradients: w=>0.0117936459, b=>-0.2034722196
Iteration 6640: Achieved Loss=> 22.8817637124
Gradients: w=>0.0117873760, b=>-0.2033640475
Iteration 6641: Achieved Loss=> 22.8817222276
Gradients: w=>0.0117811095, b=>-0.2032559329
Iteration 6642: Achieved Loss=> 22.8816807868
Gradients: w=>0.0117748463, b=>-0.2031478758
Iteration 6643: Achieved Loss=> 22.8816393901
Gradients: w=>0.0117685864, b=>-0.2030398762
Iteration 6644: Achieved Loss=> 22.8815980375
Gradients: w=>0.0117623298, b=>-0.2029319339
Iteration 6645: Achieved Loss=> 22.8815567287
Gradients: w=>0.0117560766, b=>-0.2028240491
Iteration 6646: Achieved Loss=> 22.8815154639
Gradients: w=>0.0117498267, b=>-0.2027162216
Iteration 6647: Achieved Loss=> 22.8814742429
Gradients: w=>0.0117435802, b=>-0.2026084514
Iteration 6648: Achieved Loss=> 22.8814330658
Gradients: w=>0.0117373369, b=>-0.2025007386
Iteration 6649: Achieved Loss=> 22.8813919324
Gradients: w=>0.0117310970, b=>-0.2023930830
Iteration 6650: Achieved Loss=> 22.8813508427
Gradients: w=>0.0117248604, b=>-0.2022854846
Iteration 6651: Achieved Loss=> 22.8813097968
Gradients: w=>0.0117186271, b=>-0.2021779434
Iteration 6652: Achieved Loss=> 22.8812687944
Gradients: w=>0.0117123971, b=>-0.2020704594
Iteration 6653: Achieved Loss=> 22.8812278357
Gradients: w=>0.0117061704, b=>-0.2019630326
Iteration 6654: Achieved Loss=> 22.8811869204
Gradients: w=>0.0116999471, b=>-0.2018556628
Iteration 6655: Achieved Loss=> 22.8811460487
Gradients: w=>0.0116937270, b=>-0.2017483501
Iteration 6656: Achieved Loss=> 22.8811052204
Gradients: w=>0.0116875103, b=>-0.2016410945
Iteration 6657: Achieved Loss=> 22.8810644355
Gradients: w=>0.0116812968, b=>-0.2015338959
Iteration 6658: Achieved Loss=> 22.8810236940
Gradients: w=>0.0116750867, b=>-0.2014267543
Iteration 6659: Achieved Loss=> 22.8809829958
Gradients: w=>0.0116688799, b=>-0.2013196697
Iteration 6660: Achieved Loss=> 22.8809423408
Gradients: w=>0.0116626763, b=>-0.2012126420
Iteration 6661: Achieved Loss=> 22.8809017291
Gradients: w=>0.0116564761, b=>-0.2011056711
Iteration 6662: Achieved Loss=> 22.8808611605
Gradients: w=>0.0116502792, b=>-0.2009987572
Iteration 6663: Achieved Loss=> 22.8808206350
Gradients: w=>0.0116440855, b=>-0.2008919001
Iteration 6664: Achieved Loss=> 22.8807801527
Gradients: w=>0.0116378952, b=>-0.2007850998
Iteration 6665: Achieved Loss=> 22.8807397133
Gradients: w=>0.0116317081, b=>-0.2006783563
Iteration 6666: Achieved Loss=> 22.8806993170
Gradients: w=>0.0116255243, b=>-0.2005716695
Iteration 6667: Achieved Loss=> 22.8806589635
Gradients: w=>0.0116193438, b=>-0.2004650394
Iteration 6668: Achieved Loss=> 22.8806186530
Gradients: w=>0.0116131666, b=>-0.2003584661
Iteration 6669: Achieved Loss=> 22.8805783853
Gradients: w=>0.0116069927, b=>-0.2002519494
Iteration 6670: Achieved Loss=> 22.8805381605
Gradients: w=>0.0116008221, b=>-0.2001454893
Iteration 6671: Achieved Loss=> 22.8804979784
Gradients: w=>0.0115946547, b=>-0.2000390858
Iteration 6672: Achieved Loss=> 22.8804578390
Gradients: w=>0.0115884907, b=>-0.1999327389
Iteration 6673: Achieved Loss=> 22.8804177422
Gradients: w=>0.0115823299, b=>-0.1998264485
Iteration 6674: Achieved Loss=> 22.8803776881
Gradients: w=>0.0115761723, b=>-0.1997202146
Iteration 6675: Achieved Loss=> 22.8803376766
Gradients: w=>0.0115700181, b=>-0.1996140372
Iteration 6676: Achieved Loss=> 22.8802977076
Gradients: w=>0.0115638671, b=>-0.1995079163
Iteration 6677: Achieved Loss=> 22.8802577811
Gradients: w=>0.0115577194, b=>-0.1994018517
Iteration 6678: Achieved Loss=> 22.8802178970
Gradients: w=>0.0115515750, b=>-0.1992958436
Iteration 6679: Achieved Loss=> 22.8801780553
Gradients: w=>0.0115454338, b=>-0.1991898918
Iteration 6680: Achieved Loss=> 22.8801382560
Gradients: w=>0.0115392959, b=>-0.1990839963
Iteration 6681: Achieved Loss=> 22.8800984990
Gradients: w=>0.0115331613, b=>-0.1989781572
Iteration 6682: Achieved Loss=> 22.8800587842
Gradients: w=>0.0115270299, b=>-0.1988723743
Iteration 6683: Achieved Loss=> 22.8800191117
Gradients: w=>0.0115209017, b=>-0.1987666476
Iteration 6684: Achieved Loss=> 22.8799794813
Gradients: w=>0.0115147769, b=>-0.1986609772
Iteration 6685: Achieved Loss=> 22.8799398930
Gradients: w=>0.0115086553, b=>-0.1985553629
Iteration 6686: Achieved Loss=> 22.8799003469
Gradients: w=>0.0115025369, b=>-0.1984498048
Iteration 6687: Achieved Loss=> 22.8798608427
Gradients: w=>0.0114964218, b=>-0.1983443028
Iteration 6688: Achieved Loss=> 22.8798213806
Gradients: w=>0.0114903100, b=>-0.1982388569
Iteration 6689: Achieved Loss=> 22.8797819604
Gradients: w=>0.0114842014, b=>-0.1981334670
Iteration 6690: Achieved Loss=> 22.8797425821
Gradients: w=>0.0114780960, b=>-0.1980281332
Iteration 6691: Achieved Loss=> 22.8797032457
Gradients: w=>0.0114719939, b=>-0.1979228553
Iteration 6692: Achieved Loss=> 22.8796639511
Gradients: w=>0.0114658950, b=>-0.1978176335
Iteration 6693: Achieved Loss=> 22.8796246982
Gradients: w=>0.0114597994, b=>-0.1977124675
Iteration 6694: Achieved Loss=> 22.8795854871
Gradients: w=>0.0114537070, b=>-0.1976073575
Iteration 6695: Achieved Loss=> 22.8795463177
Gradients: w=>0.0114476179, b=>-0.1975023034
Iteration 6696: Achieved Loss=> 22.8795071899
Gradients: w=>0.0114415320, b=>-0.1973973051
Iteration 6697: Achieved Loss=> 22.8794681037
Gradients: w=>0.0114354493, b=>-0.1972923626
Iteration 6698: Achieved Loss=> 22.8794290590
Gradients: w=>0.0114293699, b=>-0.1971874760
Iteration 6699: Achieved Loss=> 22.8793900558
Gradients: w=>0.0114232937, b=>-0.1970826450
Iteration 6700: Achieved Loss=> 22.8793510941
Gradients: w=>0.0114172207, b=>-0.1969778699
Iteration 6701: Achieved Loss=> 22.8793121738
Gradients: w=>0.0114111510, b=>-0.1968731504
Iteration 6702: Achieved Loss=> 22.8792732949
Gradients: w=>0.0114050844, b=>-0.1967684866
Iteration 6703: Achieved Loss=> 22.8792344573
Gradients: w=>0.0113990212, b=>-0.1966638784
Iteration 6704: Achieved Loss=> 22.8791956610
Gradients: w=>0.0113929611, b=>-0.1965593258
Iteration 6705: Achieved Loss=> 22.8791569060
Gradients: w=>0.0113869042, b=>-0.1964548289
Iteration 6706: Achieved Loss=> 22.8791181921
Gradients: w=>0.0113808506, b=>-0.1963503875
Iteration 6707: Achieved Loss=> 22.8790795194
Gradients: w=>0.0113748002, b=>-0.1962460016
Iteration 6708: Achieved Loss=> 22.8790408878
Gradients: w=>0.0113687530, b=>-0.1961416712
Iteration 6709: Achieved Loss=> 22.8790022972
Gradients: w=>0.0113627090, b=>-0.1960373962
Iteration 6710: Achieved Loss=> 22.8789637477
Gradients: w=>0.0113566683, b=>-0.1959331767
Iteration 6711: Achieved Loss=> 22.8789252392
Gradients: w=>0.0113506307, b=>-0.1958290126
Iteration 6712: Achieved Loss=> 22.8788867716
Gradients: w=>0.0113445964, b=>-0.1957249039
Iteration 6713: Achieved Loss=> 22.8788483448
Gradients: w=>0.0113385652, b=>-0.1956208506
Iteration 6714: Achieved Loss=> 22.8788099590
Gradients: w=>0.0113325373, b=>-0.1955168525
Iteration 6715: Achieved Loss=> 22.8787716139
Gradients: w=>0.0113265126, b=>-0.1954129098
Iteration 6716: Achieved Loss=> 22.8787333096
Gradients: w=>0.0113204911, b=>-0.1953090223
Iteration 6717: Achieved Loss=> 22.8786950460
Gradients: w=>0.0113144727, b=>-0.1952051900
Iteration 6718: Achieved Loss=> 22.8786568231
Gradients: w=>0.0113084576, b=>-0.1951014129
Iteration 6719: Achieved Loss=> 22.8786186408
Gradients: w=>0.0113024457, b=>-0.1949976910
Iteration 6720: Achieved Loss=> 22.8785804991
Gradients: w=>0.0112964370, b=>-0.1948940243
Iteration 6721: Achieved Loss=> 22.8785423979
Gradients: w=>0.0112904314, b=>-0.1947904126
Iteration 6722: Achieved Loss=> 22.8785043373
Gradients: w=>0.0112844291, b=>-0.1946868560
Iteration 6723: Achieved Loss=> 22.8784663171
Gradients: w=>0.0112784299, b=>-0.1945833545
Iteration 6724: Achieved Loss=> 22.8784283373
Gradients: w=>0.0112724340, b=>-0.1944799081
Iteration 6725: Achieved Loss=> 22.8783903979
Gradients: w=>0.0112664412, b=>-0.1943765166
Iteration 6726: Achieved Loss=> 22.8783524988
Gradients: w=>0.0112604516, b=>-0.1942731800
Iteration 6727: Achieved Loss=> 22.8783146400
Gradients: w=>0.0112544652, b=>-0.1941698985
Iteration 6728: Achieved Loss=> 22.8782768214
Gradients: w=>0.0112484820, b=>-0.1940666718
Iteration 6729: Achieved Loss=> 22.8782390431
Gradients: w=>0.0112425020, b=>-0.1939635000
Iteration 6730: Achieved Loss=> 22.8782013049
Gradients: w=>0.0112365251, b=>-0.1938603830
Iteration 6731: Achieved Loss=> 22.8781636068
Gradients: w=>0.0112305514, b=>-0.1937573209
Iteration 6732: Achieved Loss=> 22.8781259488
Gradients: w=>0.0112245809, b=>-0.1936543136
Iteration 6733: Achieved Loss=> 22.8780883308
Gradients: w=>0.0112186136, b=>-0.1935513610
Iteration 6734: Achieved Loss=> 22.8780507528
Gradients: w=>0.0112126494, b=>-0.1934484631
Iteration 6735: Achieved Loss=> 22.8780132147
Gradients: w=>0.0112066884, b=>-0.1933456200
Iteration 6736: Achieved Loss=> 22.8779757166
Gradients: w=>0.0112007306, b=>-0.1932428315
Iteration 6737: Achieved Loss=> 22.8779382583
Gradients: w=>0.0111947760, b=>-0.1931400977
Iteration 6738: Achieved Loss=> 22.8779008398
Gradients: w=>0.0111888245, b=>-0.1930374185
Iteration 6739: Achieved Loss=> 22.8778634611
Gradients: w=>0.0111828762, b=>-0.1929347939
Iteration 6740: Achieved Loss=> 22.8778261222
Gradients: w=>0.0111769310, b=>-0.1928322238
Iteration 6741: Achieved Loss=> 22.8777888229
Gradients: w=>0.0111709890, b=>-0.1927297083
Iteration 6742: Achieved Loss=> 22.8777515633
Gradients: w=>0.0111650502, b=>-0.1926272473
Iteration 6743: Achieved Loss=> 22.8777143433
Gradients: w=>0.0111591145, b=>-0.1925248407
Iteration 6744: Achieved Loss=> 22.8776771628
Gradients: w=>0.0111531819, b=>-0.1924224886
Iteration 6745: Achieved Loss=> 22.8776400219
Gradients: w=>0.0111472526, b=>-0.1923201909
Iteration 6746: Achieved Loss=> 22.8776029204
Gradients: w=>0.0111413263, b=>-0.1922179476
Iteration 6747: Achieved Loss=> 22.8775658584
Gradients: w=>0.0111354033, b=>-0.1921157586
Iteration 6748: Achieved Loss=> 22.8775288358
Gradients: w=>0.0111294834, b=>-0.1920136240
Iteration 6749: Achieved Loss=> 22.8774918525
Gradients: w=>0.0111235666, b=>-0.1919115436
Iteration 6750: Achieved Loss=> 22.8774549086
Gradients: w=>0.0111176530, b=>-0.1918095176
Iteration 6751: Achieved Loss=> 22.8774180039
Gradients: w=>0.0111117425, b=>-0.1917075458
Iteration 6752: Achieved Loss=> 22.8773811384
Gradients: w=>0.0111058351, b=>-0.1916056281
Iteration 6753: Achieved Loss=> 22.8773443122
Gradients: w=>0.0110999309, b=>-0.1915037647
Iteration 6754: Achieved Loss=> 22.8773075250
Gradients: w=>0.0110940299, b=>-0.1914019554
Iteration 6755: Achieved Loss=> 22.8772707770
Gradients: w=>0.0110881319, b=>-0.1913002003
Iteration 6756: Achieved Loss=> 22.8772340681
Gradients: w=>0.0110822371, b=>-0.1911984992
Iteration 6757: Achieved Loss=> 22.8771973981
Gradients: w=>0.0110763455, b=>-0.1910968522
Iteration 6758: Achieved Loss=> 22.8771607672
Gradients: w=>0.0110704570, b=>-0.1909952593
Iteration 6759: Achieved Loss=> 22.8771241752
Gradients: w=>0.0110645716, b=>-0.1908937203
Iteration 6760: Achieved Loss=> 22.8770876221
Gradients: w=>0.0110586893, b=>-0.1907922354
Iteration 6761: Achieved Loss=> 22.8770511078
Gradients: w=>0.0110528102, b=>-0.1906908044
Iteration 6762: Achieved Loss=> 22.8770146324
Gradients: w=>0.0110469341, b=>-0.1905894273
Iteration 6763: Achieved Loss=> 22.8769781957
Gradients: w=>0.0110410613, b=>-0.1904881041
Iteration 6764: Achieved Loss=> 22.8769417977
Gradients: w=>0.0110351915, b=>-0.1903868348
Iteration 6765: Achieved Loss=> 22.8769054385
Gradients: w=>0.0110293248, b=>-0.1902856193
Iteration 6766: Achieved Loss=> 22.8768691179
Gradients: w=>0.0110234613, b=>-0.1901844576
Iteration 6767: Achieved Loss=> 22.8768328359
Gradients: w=>0.0110176009, b=>-0.1900833497
Iteration 6768: Achieved Loss=> 22.8767965924
Gradients: w=>0.0110117436, b=>-0.1899822956
Iteration 6769: Achieved Loss=> 22.8767603875
Gradients: w=>0.0110058894, b=>-0.1898812951
Iteration 6770: Achieved Loss=> 22.8767242211
Gradients: w=>0.0110000384, b=>-0.1897803484
Iteration 6771: Achieved Loss=> 22.8766880931
Gradients: w=>0.0109941904, b=>-0.1896794554
Iteration 6772: Achieved Loss=> 22.8766520036
Gradients: w=>0.0109883455, b=>-0.1895786159
Iteration 6773: Achieved Loss=> 22.8766159524
Gradients: w=>0.0109825038, b=>-0.1894778301
Iteration 6774: Achieved Loss=> 22.8765799395
Gradients: w=>0.0109766652, b=>-0.1893770979
Iteration 6775: Achieved Loss=> 22.8765439649
Gradients: w=>0.0109708296, b=>-0.1892764192
Iteration 6776: Achieved Loss=> 22.8765080285
Gradients: w=>0.0109649972, b=>-0.1891757941
Iteration 6777: Achieved Loss=> 22.8764721303
Gradients: w=>0.0109591679, b=>-0.1890752224
Iteration 6778: Achieved Loss=> 22.8764362703
Gradients: w=>0.0109533416, b=>-0.1889747042
Iteration 6779: Achieved Loss=> 22.8764004484
Gradients: w=>0.0109475185, b=>-0.1888742395
Iteration 6780: Achieved Loss=> 22.8763646646
Gradients: w=>0.0109416985, b=>-0.1887738281
Iteration 6781: Achieved Loss=> 22.8763289188
Gradients: w=>0.0109358815, b=>-0.1886734702
Iteration 6782: Achieved Loss=> 22.8762932111
Gradients: w=>0.0109300677, b=>-0.1885731655
Iteration 6783: Achieved Loss=> 22.8762575412
Gradients: w=>0.0109242569, b=>-0.1884729143
Iteration 6784: Achieved Loss=> 22.8762219093
Gradients: w=>0.0109184493, b=>-0.1883727163
Iteration 6785: Achieved Loss=> 22.8761863153
Gradients: w=>0.0109126447, b=>-0.1882725716
Iteration 6786: Achieved Loss=> 22.8761507591
Gradients: w=>0.0109068432, b=>-0.1881724801
Iteration 6787: Achieved Loss=> 22.8761152407
Gradients: w=>0.0109010448, b=>-0.1880724418
Iteration 6788: Achieved Loss=> 22.8760797601
Gradients: w=>0.0108952494, b=>-0.1879724567
Iteration 6789: Achieved Loss=> 22.8760443172
Gradients: w=>0.0108894572, b=>-0.1878725248
Iteration 6790: Achieved Loss=> 22.8760089119
Gradients: w=>0.0108836680, b=>-0.1877726460
Iteration 6791: Achieved Loss=> 22.8759735443
Gradients: w=>0.0108778819, b=>-0.1876728203
Iteration 6792: Achieved Loss=> 22.8759382143
Gradients: w=>0.0108720989, b=>-0.1875730477
Iteration 6793: Achieved Loss=> 22.8759029218
Gradients: w=>0.0108663190, b=>-0.1874733281
Iteration 6794: Achieved Loss=> 22.8758676668
Gradients: w=>0.0108605421, b=>-0.1873736615
Iteration 6795: Achieved Loss=> 22.8758324494
Gradients: w=>0.0108547683, b=>-0.1872740479
Iteration 6796: Achieved Loss=> 22.8757972693
Gradients: w=>0.0108489976, b=>-0.1871744873
Iteration 6797: Achieved Loss=> 22.8757621267
Gradients: w=>0.0108432299, b=>-0.1870749796
Iteration 6798: Achieved Loss=> 22.8757270214
Gradients: w=>0.0108374653, b=>-0.1869755248
Iteration 6799: Achieved Loss=> 22.8756919534
Gradients: w=>0.0108317038, b=>-0.1868761228
Iteration 6800: Achieved Loss=> 22.8756569227
Gradients: w=>0.0108259453, b=>-0.1867767738
Iteration 6801: Achieved Loss=> 22.8756219293
Gradients: w=>0.0108201899, b=>-0.1866774775
Iteration 6802: Achieved Loss=> 22.8755869730
Gradients: w=>0.0108144376, b=>-0.1865782340
Iteration 6803: Achieved Loss=> 22.8755520539
Gradients: w=>0.0108086883, b=>-0.1864790433
Iteration 6804: Achieved Loss=> 22.8755171719
Gradients: w=>0.0108029421, b=>-0.1863799053
Iteration 6805: Achieved Loss=> 22.8754823270
Gradients: w=>0.0107971989, b=>-0.1862808200
Iteration 6806: Achieved Loss=> 22.8754475191
Gradients: w=>0.0107914588, b=>-0.1861817874
Iteration 6807: Achieved Loss=> 22.8754127483
Gradients: w=>0.0107857217, b=>-0.1860828075
Iteration 6808: Achieved Loss=> 22.8753780143
Gradients: w=>0.0107799877, b=>-0.1859838802
Iteration 6809: Achieved Loss=> 22.8753433174
Gradients: w=>0.0107742567, b=>-0.1858850054
Iteration 6810: Achieved Loss=> 22.8753086573
Gradients: w=>0.0107685288, b=>-0.1857861832
Iteration 6811: Achieved Loss=> 22.8752740340
Gradients: w=>0.0107628039, b=>-0.1856874136
Iteration 6812: Achieved Loss=> 22.8752394475
Gradients: w=>0.0107570821, b=>-0.1855886965
Iteration 6813: Achieved Loss=> 22.8752048978
Gradients: w=>0.0107513633, b=>-0.1854900318
Iteration 6814: Achieved Loss=> 22.8751703849
Gradients: w=>0.0107456475, b=>-0.1853914196
Iteration 6815: Achieved Loss=> 22.8751359086
Gradients: w=>0.0107399348, b=>-0.1852928599
Iteration 6816: Achieved Loss=> 22.8751014690
Gradients: w=>0.0107342251, b=>-0.1851943525
Iteration 6817: Achieved Loss=> 22.8750670659
Gradients: w=>0.0107285185, b=>-0.1850958975
Iteration 6818: Achieved Loss=> 22.8750326995
Gradients: w=>0.0107228149, b=>-0.1849974948
Iteration 6819: Achieved Loss=> 22.8749983696
Gradients: w=>0.0107171143, b=>-0.1848991445
Iteration 6820: Achieved Loss=> 22.8749640761
Gradients: w=>0.0107114167, b=>-0.1848008464
Iteration 6821: Achieved Loss=> 22.8749298192
Gradients: w=>0.0107057222, b=>-0.1847026006
Iteration 6822: Achieved Loss=> 22.8748955986
Gradients: w=>0.0107000307, b=>-0.1846044070
Iteration 6823: Achieved Loss=> 22.8748614144
Gradients: w=>0.0106943423, b=>-0.1845062657
Iteration 6824: Achieved Loss=> 22.8748272665
Gradients: w=>0.0106886568, b=>-0.1844081765
Iteration 6825: Achieved Loss=> 22.8747931550
Gradients: w=>0.0106829744, b=>-0.1843101394
Iteration 6826: Achieved Loss=> 22.8747590797
Gradients: w=>0.0106772950, b=>-0.1842121545
Iteration 6827: Achieved Loss=> 22.8747250406
Gradients: w=>0.0106716186, b=>-0.1841142217
Iteration 6828: Achieved Loss=> 22.8746910377
Gradients: w=>0.0106659453, b=>-0.1840163409
Iteration 6829: Achieved Loss=> 22.8746570710
Gradients: w=>0.0106602749, b=>-0.1839185122
Iteration 6830: Achieved Loss=> 22.8746231404
Gradients: w=>0.0106546076, b=>-0.1838207354
Iteration 6831: Achieved Loss=> 22.8745892458
Gradients: w=>0.0106489433, b=>-0.1837230107
Iteration 6832: Achieved Loss=> 22.8745553872
Gradients: w=>0.0106432820, b=>-0.1836253379
Iteration 6833: Achieved Loss=> 22.8745215647
Gradients: w=>0.0106376237, b=>-0.1835277170
Iteration 6834: Achieved Loss=> 22.8744877781
Gradients: w=>0.0106319684, b=>-0.1834301481
Iteration 6835: Achieved Loss=> 22.8744540274
Gradients: w=>0.0106263161, b=>-0.1833326310
Iteration 6836: Achieved Loss=> 22.8744203126
Gradients: w=>0.0106206668, b=>-0.1832351657
Iteration 6837: Achieved Loss=> 22.8743866336
Gradients: w=>0.0106150206, b=>-0.1831377523
Iteration 6838: Achieved Loss=> 22.8743529905
Gradients: w=>0.0106093773, b=>-0.1830403906
Iteration 6839: Achieved Loss=> 22.8743193831
Gradients: w=>0.0106037370, b=>-0.1829430807
Iteration 6840: Achieved Loss=> 22.8742858114
Gradients: w=>0.0105980997, b=>-0.1828458226
Iteration 6841: Achieved Loss=> 22.8742522754
Gradients: w=>0.0105924655, b=>-0.1827486161
Iteration 6842: Achieved Loss=> 22.8742187750
Gradients: w=>0.0105868342, b=>-0.1826514614
Iteration 6843: Achieved Loss=> 22.8741853103
Gradients: w=>0.0105812059, b=>-0.1825543582
Iteration 6844: Achieved Loss=> 22.8741518811
Gradients: w=>0.0105755806, b=>-0.1824573067
Iteration 6845: Achieved Loss=> 22.8741184875
Gradients: w=>0.0105699583, b=>-0.1823603068
Iteration 6846: Achieved Loss=> 22.8740851293
Gradients: w=>0.0105643390, b=>-0.1822633585
Iteration 6847: Achieved Loss=> 22.8740518067
Gradients: w=>0.0105587227, b=>-0.1821664617
Iteration 6848: Achieved Loss=> 22.8740185194
Gradients: w=>0.0105531093, b=>-0.1820696164
Iteration 6849: Achieved Loss=> 22.8739852675
Gradients: w=>0.0105474990, b=>-0.1819728226
Iteration 6850: Achieved Loss=> 22.8739520510
Gradients: w=>0.0105418916, b=>-0.1818760803
Iteration 6851: Achieved Loss=> 22.8739188698
Gradients: w=>0.0105362872, b=>-0.1817793894
Iteration 6852: Achieved Loss=> 22.8738857239
Gradients: w=>0.0105306858, b=>-0.1816827499
Iteration 6853: Achieved Loss=> 22.8738526131
Gradients: w=>0.0105250873, b=>-0.1815861617
Iteration 6854: Achieved Loss=> 22.8738195376
Gradients: w=>0.0105194919, b=>-0.1814896250
Iteration 6855: Achieved Loss=> 22.8737864973
Gradients: w=>0.0105138994, b=>-0.1813931395
Iteration 6856: Achieved Loss=> 22.8737534920
Gradients: w=>0.0105083099, b=>-0.1812967053
Iteration 6857: Achieved Loss=> 22.8737205219
Gradients: w=>0.0105027234, b=>-0.1812003225
Iteration 6858: Achieved Loss=> 22.8736875868
Gradients: w=>0.0104971398, b=>-0.1811039908
Iteration 6859: Achieved Loss=> 22.8736546867
Gradients: w=>0.0104915592, b=>-0.1810077104
Iteration 6860: Achieved Loss=> 22.8736218215
Gradients: w=>0.0104859815, b=>-0.1809114811
Iteration 6861: Achieved Loss=> 22.8735889914
Gradients: w=>0.0104804069, b=>-0.1808153030
Iteration 6862: Achieved Loss=> 22.8735561961
Gradients: w=>0.0104748352, b=>-0.1807191760
Iteration 6863: Achieved Loss=> 22.8735234356
Gradients: w=>0.0104692664, b=>-0.1806231002
Iteration 6864: Achieved Loss=> 22.8734907100
Gradients: w=>0.0104637006, b=>-0.1805270754
Iteration 6865: Achieved Loss=> 22.8734580192
Gradients: w=>0.0104581378, b=>-0.1804311017
Iteration 6866: Achieved Loss=> 22.8734253631
Gradients: w=>0.0104525780, b=>-0.1803351790
Iteration 6867: Achieved Loss=> 22.8733927418
Gradients: w=>0.0104470210, b=>-0.1802393072
Iteration 6868: Achieved Loss=> 22.8733601551
Gradients: w=>0.0104414671, b=>-0.1801434865
Iteration 6869: Achieved Loss=> 22.8733276030
Gradients: w=>0.0104359161, b=>-0.1800477167
Iteration 6870: Achieved Loss=> 22.8732950856
Gradients: w=>0.0104303680, b=>-0.1799519978
Iteration 6871: Achieved Loss=> 22.8732626027
Gradients: w=>0.0104248229, b=>-0.1798563298
Iteration 6872: Achieved Loss=> 22.8732301544
Gradients: w=>0.0104192808, b=>-0.1797607126
Iteration 6873: Achieved Loss=> 22.8731977405
Gradients: w=>0.0104137416, b=>-0.1796651463
Iteration 6874: Achieved Loss=> 22.8731653611
Gradients: w=>0.0104082053, b=>-0.1795696308
Iteration 6875: Achieved Loss=> 22.8731330161
Gradients: w=>0.0104026720, b=>-0.1794741661
Iteration 6876: Achieved Loss=> 22.8731007055
Gradients: w=>0.0103971416, b=>-0.1793787521
Iteration 6877: Achieved Loss=> 22.8730684293
Gradients: w=>0.0103916142, b=>-0.1792833889
Iteration 6878: Achieved Loss=> 22.8730361873
Gradients: w=>0.0103860896, b=>-0.1791880763
Iteration 6879: Achieved Loss=> 22.8730039796
Gradients: w=>0.0103805681, b=>-0.1790928144
Iteration 6880: Achieved Loss=> 22.8729718062
Gradients: w=>0.0103750495, b=>-0.1789976032
Iteration 6881: Achieved Loss=> 22.8729396670
Gradients: w=>0.0103695338, b=>-0.1789024426
Iteration 6882: Achieved Loss=> 22.8729075619
Gradients: w=>0.0103640210, b=>-0.1788073325
Iteration 6883: Achieved Loss=> 22.8728754909
Gradients: w=>0.0103585112, b=>-0.1787122731
Iteration 6884: Achieved Loss=> 22.8728434541
Gradients: w=>0.0103530043, b=>-0.1786172641
Iteration 6885: Achieved Loss=> 22.8728114513
Gradients: w=>0.0103475003, b=>-0.1785223057
Iteration 6886: Achieved Loss=> 22.8727794825
Gradients: w=>0.0103419992, b=>-0.1784273978
Iteration 6887: Achieved Loss=> 22.8727475477
Gradients: w=>0.0103365011, b=>-0.1783325403
Iteration 6888: Achieved Loss=> 22.8727156468
Gradients: w=>0.0103310059, b=>-0.1782377333
Iteration 6889: Achieved Loss=> 22.8726837799
Gradients: w=>0.0103255136, b=>-0.1781429766
Iteration 6890: Achieved Loss=> 22.8726519468
Gradients: w=>0.0103200243, b=>-0.1780482703
Iteration 6891: Achieved Loss=> 22.8726201476
Gradients: w=>0.0103145378, b=>-0.1779536144
Iteration 6892: Achieved Loss=> 22.8725883822
Gradients: w=>0.0103090543, b=>-0.1778590088
Iteration 6893: Achieved Loss=> 22.8725566505
Gradients: w=>0.0103035737, b=>-0.1777644535
Iteration 6894: Achieved Loss=> 22.8725249526
Gradients: w=>0.0102980960, b=>-0.1776699485
Iteration 6895: Achieved Loss=> 22.8724932883
Gradients: w=>0.0102926212, b=>-0.1775754937
Iteration 6896: Achieved Loss=> 22.8724616577
Gradients: w=>0.0102871493, b=>-0.1774810891
Iteration 6897: Achieved Loss=> 22.8724300608
Gradients: w=>0.0102816804, b=>-0.1773867347
Iteration 6898: Achieved Loss=> 22.8723984974
Gradients: w=>0.0102762143, b=>-0.1772924304
Iteration 6899: Achieved Loss=> 22.8723669676
Gradients: w=>0.0102707512, b=>-0.1771981763
Iteration 6900: Achieved Loss=> 22.8723354713
Gradients: w=>0.0102652909, b=>-0.1771039724
Iteration 6901: Achieved Loss=> 22.8723040084
Gradients: w=>0.0102598336, b=>-0.1770098185
Iteration 6902: Achieved Loss=> 22.8722725791
Gradients: w=>0.0102543791, b=>-0.1769157146
Iteration 6903: Achieved Loss=> 22.8722411831
Gradients: w=>0.0102489276, b=>-0.1768216608
Iteration 6904: Achieved Loss=> 22.8722098205
Gradients: w=>0.0102434790, b=>-0.1767276570
Iteration 6905: Achieved Loss=> 22.8721784912
Gradients: w=>0.0102380332, b=>-0.1766337031
Iteration 6906: Achieved Loss=> 22.8721471953
Gradients: w=>0.0102325904, b=>-0.1765397992
Iteration 6907: Achieved Loss=> 22.8721159326
Gradients: w=>0.0102271504, b=>-0.1764459452
Iteration 6908: Achieved Loss=> 22.8720847031
Gradients: w=>0.0102217133, b=>-0.1763521411
Iteration 6909: Achieved Loss=> 22.8720535068
Gradients: w=>0.0102162792, b=>-0.1762583869
Iteration 6910: Achieved Loss=> 22.8720223437
Gradients: w=>0.0102108479, b=>-0.1761646826
Iteration 6911: Achieved Loss=> 22.8719912137
Gradients: w=>0.0102054195, b=>-0.1760710280
Iteration 6912: Achieved Loss=> 22.8719601169
Gradients: w=>0.0101999939, b=>-0.1759774233
Iteration 6913: Achieved Loss=> 22.8719290530
Gradients: w=>0.0101945713, b=>-0.1758838683
Iteration 6914: Achieved Loss=> 22.8718980222
Gradients: w=>0.0101891516, b=>-0.1757903630
Iteration 6915: Achieved Loss=> 22.8718670244
Gradients: w=>0.0101837347, b=>-0.1756969074
Iteration 6916: Achieved Loss=> 22.8718360595
Gradients: w=>0.0101783207, b=>-0.1756035016
Iteration 6917: Achieved Loss=> 22.8718051275
Gradients: w=>0.0101729096, b=>-0.1755101454
Iteration 6918: Achieved Loss=> 22.8717742285
Gradients: w=>0.0101675014, b=>-0.1754168388
Iteration 6919: Achieved Loss=> 22.8717433622
Gradients: w=>0.0101620960, b=>-0.1753235818
Iteration 6920: Achieved Loss=> 22.8717125288
Gradients: w=>0.0101566935, b=>-0.1752303744
Iteration 6921: Achieved Loss=> 22.8716817281
Gradients: w=>0.0101512939, b=>-0.1751372166
Iteration 6922: Achieved Loss=> 22.8716509602
Gradients: w=>0.0101458972, b=>-0.1750441082
Iteration 6923: Achieved Loss=> 22.8716202250
Gradients: w=>0.0101405033, b=>-0.1749510494
Iteration 6924: Achieved Loss=> 22.8715895225
Gradients: w=>0.0101351123, b=>-0.1748580401
Iteration 6925: Achieved Loss=> 22.8715588526
Gradients: w=>0.0101297242, b=>-0.1747650802
Iteration 6926: Achieved Loss=> 22.8715282153
Gradients: w=>0.0101243389, b=>-0.1746721697
Iteration 6927: Achieved Loss=> 22.8714976105
Gradients: w=>0.0101189565, b=>-0.1745793086
Iteration 6928: Achieved Loss=> 22.8714670383
Gradients: w=>0.0101135770, b=>-0.1744864969
Iteration 6929: Achieved Loss=> 22.8714364986
Gradients: w=>0.0101082003, b=>-0.1743937345
Iteration 6930: Achieved Loss=> 22.8714059914
Gradients: w=>0.0101028264, b=>-0.1743010214
Iteration 6931: Achieved Loss=> 22.8713755166
Gradients: w=>0.0100974555, b=>-0.1742083577
Iteration 6932: Achieved Loss=> 22.8713450742
Gradients: w=>0.0100920873, b=>-0.1741157432
Iteration 6933: Achieved Loss=> 22.8713146641
Gradients: w=>0.0100867221, b=>-0.1740231779
Iteration 6934: Achieved Loss=> 22.8712842864
Gradients: w=>0.0100813597, b=>-0.1739306618
Iteration 6935: Achieved Loss=> 22.8712539410
Gradients: w=>0.0100760001, b=>-0.1738381949
Iteration 6936: Achieved Loss=> 22.8712236278
Gradients: w=>0.0100706434, b=>-0.1737457772
Iteration 6937: Achieved Loss=> 22.8711933468
Gradients: w=>0.0100652895, b=>-0.1736534086
Iteration 6938: Achieved Loss=> 22.8711630980
Gradients: w=>0.0100599385, b=>-0.1735610892
Iteration 6939: Achieved Loss=> 22.8711328814
Gradients: w=>0.0100545904, b=>-0.1734688187
Iteration 6940: Achieved Loss=> 22.8711026969
Gradients: w=>0.0100492450, b=>-0.1733765974
Iteration 6941: Achieved Loss=> 22.8710725445
Gradients: w=>0.0100439025, b=>-0.1732844251
Iteration 6942: Achieved Loss=> 22.8710424241
Gradients: w=>0.0100385629, b=>-0.1731923018
Iteration 6943: Achieved Loss=> 22.8710123358
Gradients: w=>0.0100332261, b=>-0.1731002274
Iteration 6944: Achieved Loss=> 22.8709822794
Gradients: w=>0.0100278921, b=>-0.1730082020
Iteration 6945: Achieved Loss=> 22.8709522550
Gradients: w=>0.0100225610, b=>-0.1729162256
Iteration 6946: Achieved Loss=> 22.8709222625
Gradients: w=>0.0100172327, b=>-0.1728242980
Iteration 6947: Achieved Loss=> 22.8708923019
Gradients: w=>0.0100119072, b=>-0.1727324193
Iteration 6948: Achieved Loss=> 22.8708623731
Gradients: w=>0.0100065846, b=>-0.1726405894
Iteration 6949: Achieved Loss=> 22.8708324762
Gradients: w=>0.0100012648, b=>-0.1725488084
Iteration 6950: Achieved Loss=> 22.8708026110
Gradients: w=>0.0099959478, b=>-0.1724570761
Iteration 6951: Achieved Loss=> 22.8707727776
Gradients: w=>0.0099906336, b=>-0.1723653927
Iteration 6952: Achieved Loss=> 22.8707429758
Gradients: w=>0.0099853223, b=>-0.1722737579
Iteration 6953: Achieved Loss=> 22.8707132058
Gradients: w=>0.0099800138, b=>-0.1721821719
Iteration 6954: Achieved Loss=> 22.8706834674
Gradients: w=>0.0099747081, b=>-0.1720906346
Iteration 6955: Achieved Loss=> 22.8706537606
Gradients: w=>0.0099694053, b=>-0.1719991459
Iteration 6956: Achieved Loss=> 22.8706240854
Gradients: w=>0.0099641052, b=>-0.1719077059
Iteration 6957: Achieved Loss=> 22.8705944418
Gradients: w=>0.0099588080, b=>-0.1718163145
Iteration 6958: Achieved Loss=> 22.8705648296
Gradients: w=>0.0099535136, b=>-0.1717249717
Iteration 6959: Achieved Loss=> 22.8705352489
Gradients: w=>0.0099482220, b=>-0.1716336774
Iteration 6960: Achieved Loss=> 22.8705056997
Gradients: w=>0.0099429332, b=>-0.1715424317
Iteration 6961: Achieved Loss=> 22.8704761819
Gradients: w=>0.0099376472, b=>-0.1714512344
Iteration 6962: Achieved Loss=> 22.8704466955
Gradients: w=>0.0099323641, b=>-0.1713600857
Iteration 6963: Achieved Loss=> 22.8704172404
Gradients: w=>0.0099270837, b=>-0.1712689854
Iteration 6964: Achieved Loss=> 22.8703878166
Gradients: w=>0.0099218062, b=>-0.1711779336
Iteration 6965: Achieved Loss=> 22.8703584241
Gradients: w=>0.0099165314, b=>-0.1710869301
Iteration 6966: Achieved Loss=> 22.8703290628
Gradients: w=>0.0099112595, b=>-0.1709959751
Iteration 6967: Achieved Loss=> 22.8702997327
Gradients: w=>0.0099059904, b=>-0.1709050684
Iteration 6968: Achieved Loss=> 22.8702704338
Gradients: w=>0.0099007241, b=>-0.1708142100
Iteration 6969: Achieved Loss=> 22.8702411661
Gradients: w=>0.0098954605, b=>-0.1707233999
Iteration 6970: Achieved Loss=> 22.8702119295
Gradients: w=>0.0098901998, b=>-0.1706326381
Iteration 6971: Achieved Loss=> 22.8701827239
Gradients: w=>0.0098849419, b=>-0.1705419245
Iteration 6972: Achieved Loss=> 22.8701535494
Gradients: w=>0.0098796867, b=>-0.1704512592
Iteration 6973: Achieved Loss=> 22.8701244059
Gradients: w=>0.0098744344, b=>-0.1703606421
Iteration 6974: Achieved Loss=> 22.8700952934
Gradients: w=>0.0098691848, b=>-0.1702700732
Iteration 6975: Achieved Loss=> 22.8700662119
Gradients: w=>0.0098639381, b=>-0.1701795524
Iteration 6976: Achieved Loss=> 22.8700371612
Gradients: w=>0.0098586941, b=>-0.1700890797
Iteration 6977: Achieved Loss=> 22.8700081414
Gradients: w=>0.0098534529, b=>-0.1699986551
Iteration 6978: Achieved Loss=> 22.8699791525
Gradients: w=>0.0098482145, b=>-0.1699082786
Iteration 6979: Achieved Loss=> 22.8699501944
Gradients: w=>0.0098429789, b=>-0.1698179502
Iteration 6980: Achieved Loss=> 22.8699212671
Gradients: w=>0.0098377461, b=>-0.1697276697
Iteration 6981: Achieved Loss=> 22.8698923705
Gradients: w=>0.0098325160, b=>-0.1696374373
Iteration 6982: Achieved Loss=> 22.8698635046
Gradients: w=>0.0098272888, b=>-0.1695472528
Iteration 6983: Achieved Loss=> 22.8698346694
Gradients: w=>0.0098220643, b=>-0.1694571163
Iteration 6984: Achieved Loss=> 22.8698058649
Gradients: w=>0.0098168426, b=>-0.1693670277
Iteration 6985: Achieved Loss=> 22.8697770910
Gradients: w=>0.0098116236, b=>-0.1692769870
Iteration 6986: Achieved Loss=> 22.8697483477
Gradients: w=>0.0098064075, b=>-0.1691869941
Iteration 6987: Achieved Loss=> 22.8697196349
Gradients: w=>0.0098011941, b=>-0.1690970491
Iteration 6988: Achieved Loss=> 22.8696909527
Gradients: w=>0.0097959835, b=>-0.1690071519
Iteration 6989: Achieved Loss=> 22.8696623009
Gradients: w=>0.0097907756, b=>-0.1689173026
Iteration 6990: Achieved Loss=> 22.8696336796
Gradients: w=>0.0097855705, b=>-0.1688275009
Iteration 6991: Achieved Loss=> 22.8696050887
Gradients: w=>0.0097803682, b=>-0.1687377471
Iteration 6992: Achieved Loss=> 22.8695765282
Gradients: w=>0.0097751687, b=>-0.1686480409
Iteration 6993: Achieved Loss=> 22.8695479981
Gradients: w=>0.0097699719, b=>-0.1685583824
Iteration 6994: Achieved Loss=> 22.8695194983
Gradients: w=>0.0097647779, b=>-0.1684687716
Iteration 6995: Achieved Loss=> 22.8694910288
Gradients: w=>0.0097595866, b=>-0.1683792084
Iteration 6996: Achieved Loss=> 22.8694625895
Gradients: w=>0.0097543981, b=>-0.1682896929
Iteration 6997: Achieved Loss=> 22.8694341805
Gradients: w=>0.0097492124, b=>-0.1682002249
Iteration 6998: Achieved Loss=> 22.8694058017
Gradients: w=>0.0097440294, b=>-0.1681108045
Iteration 6999: Achieved Loss=> 22.8693774531
Gradients: w=>0.0097388492, b=>-0.1680214316
Iteration 7000: Achieved Loss=> 22.8693491345
Gradients: w=>0.0097336717, b=>-0.1679321063
Iteration 7001: Achieved Loss=> 22.8693208461
Gradients: w=>0.0097284970, b=>-0.1678428284
Iteration 7002: Achieved Loss=> 22.8692925878
Gradients: w=>0.0097233250, b=>-0.1677535980
Iteration 7003: Achieved Loss=> 22.8692643595
Gradients: w=>0.0097181558, b=>-0.1676644151
Iteration 7004: Achieved Loss=> 22.8692361612
Gradients: w=>0.0097129893, b=>-0.1675752795
Iteration 7005: Achieved Loss=> 22.8692079928
Gradients: w=>0.0097078256, b=>-0.1674861914
Iteration 7006: Achieved Loss=> 22.8691798545
Gradients: w=>0.0097026646, b=>-0.1673971506
Iteration 7007: Achieved Loss=> 22.8691517460
Gradients: w=>0.0096975064, b=>-0.1673081571
Iteration 7008: Achieved Loss=> 22.8691236674
Gradients: w=>0.0096923509, b=>-0.1672192109
Iteration 7009: Achieved Loss=> 22.8690956186
Gradients: w=>0.0096871982, b=>-0.1671303121
Iteration 7010: Achieved Loss=> 22.8690675997
Gradients: w=>0.0096820482, b=>-0.1670414605
Iteration 7011: Achieved Loss=> 22.8690396106
Gradients: w=>0.0096769009, b=>-0.1669526561
Iteration 7012: Achieved Loss=> 22.8690116512
Gradients: w=>0.0096717563, b=>-0.1668638990
Iteration 7013: Achieved Loss=> 22.8689837215
Gradients: w=>0.0096666145, b=>-0.1667751890
Iteration 7014: Achieved Loss=> 22.8689558215
Gradients: w=>0.0096614755, b=>-0.1666865262
Iteration 7015: Achieved Loss=> 22.8689279512
Gradients: w=>0.0096563391, b=>-0.1665979105
Iteration 7016: Achieved Loss=> 22.8689001105
Gradients: w=>0.0096512055, b=>-0.1665093419
Iteration 7017: Achieved Loss=> 22.8688722993
Gradients: w=>0.0096460747, b=>-0.1664208205
Iteration 7018: Achieved Loss=> 22.8688445178
Gradients: w=>0.0096409465, b=>-0.1663323460
Iteration 7019: Achieved Loss=> 22.8688167658
Gradients: w=>0.0096358211, b=>-0.1662439187
Iteration 7020: Achieved Loss=> 22.8687890433
Gradients: w=>0.0096306984, b=>-0.1661555383
Iteration 7021: Achieved Loss=> 22.8687613502
Gradients: w=>0.0096255784, b=>-0.1660672049
Iteration 7022: Achieved Loss=> 22.8687336866
Gradients: w=>0.0096204612, b=>-0.1659789185
Iteration 7023: Achieved Loss=> 22.8687060524
Gradients: w=>0.0096153466, b=>-0.1658906790
Iteration 7024: Achieved Loss=> 22.8686784476
Gradients: w=>0.0096102348, b=>-0.1658024864
Iteration 7025: Achieved Loss=> 22.8686508721
Gradients: w=>0.0096051257, b=>-0.1657143407
Iteration 7026: Achieved Loss=> 22.8686233259
Gradients: w=>0.0096000193, b=>-0.1656262419
Iteration 7027: Achieved Loss=> 22.8685958090
Gradients: w=>0.0095949157, b=>-0.1655381899
Iteration 7028: Achieved Loss=> 22.8685683213
Gradients: w=>0.0095898147, b=>-0.1654501847
Iteration 7029: Achieved Loss=> 22.8685408629
Gradients: w=>0.0095847165, b=>-0.1653622263
Iteration 7030: Achieved Loss=> 22.8685134337
Gradients: w=>0.0095796209, b=>-0.1652743147
Iteration 7031: Achieved Loss=> 22.8684860336
Gradients: w=>0.0095745281, b=>-0.1651864498
Iteration 7032: Achieved Loss=> 22.8684586626
Gradients: w=>0.0095694380, b=>-0.1650986316
Iteration 7033: Achieved Loss=> 22.8684313208
Gradients: w=>0.0095643506, b=>-0.1650108601
Iteration 7034: Achieved Loss=> 22.8684040080
Gradients: w=>0.0095592659, b=>-0.1649231353
Iteration 7035: Achieved Loss=> 22.8683767242
Gradients: w=>0.0095541839, b=>-0.1648354571
Iteration 7036: Achieved Loss=> 22.8683494695
Gradients: w=>0.0095491046, b=>-0.1647478255
Iteration 7037: Achieved Loss=> 22.8683222437
Gradients: w=>0.0095440280, b=>-0.1646602405
Iteration 7038: Achieved Loss=> 22.8682950468
Gradients: w=>0.0095389541, b=>-0.1645727020
Iteration 7039: Achieved Loss=> 22.8682678789
Gradients: w=>0.0095338829, b=>-0.1644852101
Iteration 7040: Achieved Loss=> 22.8682407398
Gradients: w=>0.0095288144, b=>-0.1643977647
Iteration 7041: Achieved Loss=> 22.8682136296
Gradients: w=>0.0095237486, b=>-0.1643103658
Iteration 7042: Achieved Loss=> 22.8681865482
Gradients: w=>0.0095186855, b=>-0.1642230134
Iteration 7043: Achieved Loss=> 22.8681594956
Gradients: w=>0.0095136251, b=>-0.1641357074
Iteration 7044: Achieved Loss=> 22.8681324717
Gradients: w=>0.0095085673, b=>-0.1640484478
Iteration 7045: Achieved Loss=> 22.8681054766
Gradients: w=>0.0095035123, b=>-0.1639612347
Iteration 7046: Achieved Loss=> 22.8680785102
Gradients: w=>0.0094984599, b=>-0.1638740678
Iteration 7047: Achieved Loss=> 22.8680515724
Gradients: w=>0.0094934102, b=>-0.1637869473
Iteration 7048: Achieved Loss=> 22.8680246633
Gradients: w=>0.0094883633, b=>-0.1636998732
Iteration 7049: Achieved Loss=> 22.8679977827
Gradients: w=>0.0094833189, b=>-0.1636128453
Iteration 7050: Achieved Loss=> 22.8679709308
Gradients: w=>0.0094782773, b=>-0.1635258637
Iteration 7051: Achieved Loss=> 22.8679441074
Gradients: w=>0.0094732384, b=>-0.1634389283
Iteration 7052: Achieved Loss=> 22.8679173125
Gradients: w=>0.0094682021, b=>-0.1633520392
Iteration 7053: Achieved Loss=> 22.8678905460
Gradients: w=>0.0094631685, b=>-0.1632651962
Iteration 7054: Achieved Loss=> 22.8678638081
Gradients: w=>0.0094581376, b=>-0.1631783994
Iteration 7055: Achieved Loss=> 22.8678370985
Gradients: w=>0.0094531094, b=>-0.1630916488
Iteration 7056: Achieved Loss=> 22.8678104174
Gradients: w=>0.0094480838, b=>-0.1630049442
Iteration 7057: Achieved Loss=> 22.8677837646
Gradients: w=>0.0094430609, b=>-0.1629182858
Iteration 7058: Achieved Loss=> 22.8677571401
Gradients: w=>0.0094380407, b=>-0.1628316735
Iteration 7059: Achieved Loss=> 22.8677305440
Gradients: w=>0.0094330232, b=>-0.1627451071
Iteration 7060: Achieved Loss=> 22.8677039761
Gradients: w=>0.0094280083, b=>-0.1626585868
Iteration 7061: Achieved Loss=> 22.8676774364
Gradients: w=>0.0094229961, b=>-0.1625721125
Iteration 7062: Achieved Loss=> 22.8676509250
Gradients: w=>0.0094179865, b=>-0.1624856842
Iteration 7063: Achieved Loss=> 22.8676244417
Gradients: w=>0.0094129796, b=>-0.1623993018
Iteration 7064: Achieved Loss=> 22.8675979866
Gradients: w=>0.0094079754, b=>-0.1623129654
Iteration 7065: Achieved Loss=> 22.8675715597
Gradients: w=>0.0094029738, b=>-0.1622266748
Iteration 7066: Achieved Loss=> 22.8675451608
Gradients: w=>0.0093979749, b=>-0.1621404302
Iteration 7067: Achieved Loss=> 22.8675187899
Gradients: w=>0.0093929786, b=>-0.1620542313
Iteration 7068: Achieved Loss=> 22.8674924471
Gradients: w=>0.0093879851, b=>-0.1619680783
Iteration 7069: Achieved Loss=> 22.8674661323
Gradients: w=>0.0093829941, b=>-0.1618819711
Iteration 7070: Achieved Loss=> 22.8674398455
Gradients: w=>0.0093780058, b=>-0.1617959097
Iteration 7071: Achieved Loss=> 22.8674135866
Gradients: w=>0.0093730202, b=>-0.1617098940
Iteration 7072: Achieved Loss=> 22.8673873557
Gradients: w=>0.0093680372, b=>-0.1616239241
Iteration 7073: Achieved Loss=> 22.8673611526
Gradients: w=>0.0093630569, b=>-0.1615379998
Iteration 7074: Achieved Loss=> 22.8673349773
Gradients: w=>0.0093580792, b=>-0.1614521213
Iteration 7075: Achieved Loss=> 22.8673088299
Gradients: w=>0.0093531041, b=>-0.1613662884
Iteration 7076: Achieved Loss=> 22.8672827103
Gradients: w=>0.0093481317, b=>-0.1612805011
Iteration 7077: Achieved Loss=> 22.8672566185
Gradients: w=>0.0093431620, b=>-0.1611947594
Iteration 7078: Achieved Loss=> 22.8672305544
Gradients: w=>0.0093381949, b=>-0.1611090634
Iteration 7079: Achieved Loss=> 22.8672045179
Gradients: w=>0.0093332304, b=>-0.1610234128
Iteration 7080: Achieved Loss=> 22.8671785092
Gradients: w=>0.0093282686, b=>-0.1609378078
Iteration 7081: Achieved Loss=> 22.8671525281
Gradients: w=>0.0093233094, b=>-0.1608522484
Iteration 7082: Achieved Loss=> 22.8671265747
Gradients: w=>0.0093183528, b=>-0.1607667344
Iteration 7083: Achieved Loss=> 22.8671006488
Gradients: w=>0.0093133989, b=>-0.1606812658
Iteration 7084: Achieved Loss=> 22.8670747505
Gradients: w=>0.0093084476, b=>-0.1605958428
Iteration 7085: Achieved Loss=> 22.8670488797
Gradients: w=>0.0093034990, b=>-0.1605104651
Iteration 7086: Achieved Loss=> 22.8670230364
Gradients: w=>0.0092985529, b=>-0.1604251328
Iteration 7087: Achieved Loss=> 22.8669972205
Gradients: w=>0.0092936095, b=>-0.1603398459
Iteration 7088: Achieved Loss=> 22.8669714322
Gradients: w=>0.0092886688, b=>-0.1602546043
Iteration 7089: Achieved Loss=> 22.8669456712
Gradients: w=>0.0092837306, b=>-0.1601694080
Iteration 7090: Achieved Loss=> 22.8669199376
Gradients: w=>0.0092787951, b=>-0.1600842570
Iteration 7091: Achieved Loss=> 22.8668942314
Gradients: w=>0.0092738622, b=>-0.1599991513
Iteration 7092: Achieved Loss=> 22.8668685525
Gradients: w=>0.0092689319, b=>-0.1599140909
Iteration 7093: Achieved Loss=> 22.8668429009
Gradients: w=>0.0092640043, b=>-0.1598290756
Iteration 7094: Achieved Loss=> 22.8668172765
Gradients: w=>0.0092590793, b=>-0.1597441056
Iteration 7095: Achieved Loss=> 22.8667916794
Gradients: w=>0.0092541569, b=>-0.1596591807
Iteration 7096: Achieved Loss=> 22.8667661095
Gradients: w=>0.0092492371, b=>-0.1595743010
Iteration 7097: Achieved Loss=> 22.8667405668
Gradients: w=>0.0092443199, b=>-0.1594894664
Iteration 7098: Achieved Loss=> 22.8667150513
Gradients: w=>0.0092394053, b=>-0.1594046769
Iteration 7099: Achieved Loss=> 22.8666895628
Gradients: w=>0.0092344934, b=>-0.1593199325
Iteration 7100: Achieved Loss=> 22.8666641015
Gradients: w=>0.0092295840, b=>-0.1592352331
Iteration 7101: Achieved Loss=> 22.8666386672
Gradients: w=>0.0092246773, b=>-0.1591505788
Iteration 7102: Achieved Loss=> 22.8666132599
Gradients: w=>0.0092197732, b=>-0.1590659694
Iteration 7103: Achieved Loss=> 22.8665878797
Gradients: w=>0.0092148717, b=>-0.1589814051
Iteration 7104: Achieved Loss=> 22.8665625264
Gradients: w=>0.0092099727, b=>-0.1588968857
Iteration 7105: Achieved Loss=> 22.8665372001
Gradients: w=>0.0092050764, b=>-0.1588124112
Iteration 7106: Achieved Loss=> 22.8665119007
Gradients: w=>0.0092001827, b=>-0.1587279817
Iteration 7107: Achieved Loss=> 22.8664866282
Gradients: w=>0.0091952916, b=>-0.1586435970
Iteration 7108: Achieved Loss=> 22.8664613826
Gradients: w=>0.0091904031, b=>-0.1585592572
Iteration 7109: Achieved Loss=> 22.8664361638
Gradients: w=>0.0091855172, b=>-0.1584749622
Iteration 7110: Achieved Loss=> 22.8664109718
Gradients: w=>0.0091806339, b=>-0.1583907121
Iteration 7111: Achieved Loss=> 22.8663858066
Gradients: w=>0.0091757532, b=>-0.1583065067
Iteration 7112: Achieved Loss=> 22.8663606682
Gradients: w=>0.0091708751, b=>-0.1582223461
Iteration 7113: Achieved Loss=> 22.8663355564
Gradients: w=>0.0091659996, b=>-0.1581382303
Iteration 7114: Achieved Loss=> 22.8663104714
Gradients: w=>0.0091611267, b=>-0.1580541591
Iteration 7115: Achieved Loss=> 22.8662854130
Gradients: w=>0.0091562563, b=>-0.1579701327
Iteration 7116: Achieved Loss=> 22.8662603812
Gradients: w=>0.0091513886, b=>-0.1578861509
Iteration 7117: Achieved Loss=> 22.8662353761
Gradients: w=>0.0091465234, b=>-0.1578022138
Iteration 7118: Achieved Loss=> 22.8662103976
Gradients: w=>0.0091416609, b=>-0.1577183213
Iteration 7119: Achieved Loss=> 22.8661854455
Gradients: w=>0.0091368009, b=>-0.1576344734
Iteration 7120: Achieved Loss=> 22.8661605201
Gradients: w=>0.0091319435, b=>-0.1575506701
Iteration 7121: Achieved Loss=> 22.8661356211
Gradients: w=>0.0091270887, b=>-0.1574669113
Iteration 7122: Achieved Loss=> 22.8661107486
Gradients: w=>0.0091222364, b=>-0.1573831971
Iteration 7123: Achieved Loss=> 22.8660859025
Gradients: w=>0.0091173867, b=>-0.1572995273
Iteration 7124: Achieved Loss=> 22.8660610828
Gradients: w=>0.0091125397, b=>-0.1572159021
Iteration 7125: Achieved Loss=> 22.8660362895
Gradients: w=>0.0091076952, b=>-0.1571323213
Iteration 7126: Achieved Loss=> 22.8660115226
Gradients: w=>0.0091028532, b=>-0.1570487849
Iteration 7127: Achieved Loss=> 22.8659867820
Gradients: w=>0.0090980139, b=>-0.1569652929
Iteration 7128: Achieved Loss=> 22.8659620677
Gradients: w=>0.0090931771, b=>-0.1568818454
Iteration 7129: Achieved Loss=> 22.8659373796
Gradients: w=>0.0090883429, b=>-0.1567984422
Iteration 7130: Achieved Loss=> 22.8659127179
Gradients: w=>0.0090835112, b=>-0.1567150833
Iteration 7131: Achieved Loss=> 22.8658880823
Gradients: w=>0.0090786822, b=>-0.1566317688
Iteration 7132: Achieved Loss=> 22.8658634729
Gradients: w=>0.0090738557, b=>-0.1565484985
Iteration 7133: Achieved Loss=> 22.8658388897
Gradients: w=>0.0090690317, b=>-0.1564652725
Iteration 7134: Achieved Loss=> 22.8658143326
Gradients: w=>0.0090642103, b=>-0.1563820908
Iteration 7135: Achieved Loss=> 22.8657898016
Gradients: w=>0.0090593915, b=>-0.1562989533
Iteration 7136: Achieved Loss=> 22.8657652966
Gradients: w=>0.0090545753, b=>-0.1562158599
Iteration 7137: Achieved Loss=> 22.8657408178
Gradients: w=>0.0090497616, b=>-0.1561328108
Iteration 7138: Achieved Loss=> 22.8657163649
Gradients: w=>0.0090449505, b=>-0.1560498058
Iteration 7139: Achieved Loss=> 22.8656919381
Gradients: w=>0.0090401419, b=>-0.1559668449
Iteration 7140: Achieved Loss=> 22.8656675372
Gradients: w=>0.0090353359, b=>-0.1558839282
Iteration 7141: Achieved Loss=> 22.8656431622
Gradients: w=>0.0090305324, b=>-0.1558010555
Iteration 7142: Achieved Loss=> 22.8656188132
Gradients: w=>0.0090257315, b=>-0.1557182269
Iteration 7143: Achieved Loss=> 22.8655944900
Gradients: w=>0.0090209331, b=>-0.1556354423
Iteration 7144: Achieved Loss=> 22.8655701927
Gradients: w=>0.0090161373, b=>-0.1555527017
Iteration 7145: Achieved Loss=> 22.8655459212
Gradients: w=>0.0090113441, b=>-0.1554700051
Iteration 7146: Achieved Loss=> 22.8655216755
Gradients: w=>0.0090065534, b=>-0.1553873525
Iteration 7147: Achieved Loss=> 22.8654974556
Gradients: w=>0.0090017652, b=>-0.1553047438
Iteration 7148: Achieved Loss=> 22.8654732615
Gradients: w=>0.0089969796, b=>-0.1552221790
Iteration 7149: Achieved Loss=> 22.8654490930
Gradients: w=>0.0089921965, b=>-0.1551396581
Iteration 7150: Achieved Loss=> 22.8654249503
Gradients: w=>0.0089874160, b=>-0.1550571811
Iteration 7151: Achieved Loss=> 22.8654008332
Gradients: w=>0.0089826380, b=>-0.1549747480
Iteration 7152: Achieved Loss=> 22.8653767417
Gradients: w=>0.0089778626, b=>-0.1548923586
Iteration 7153: Achieved Loss=> 22.8653526759
Gradients: w=>0.0089730897, b=>-0.1548100131
Iteration 7154: Achieved Loss=> 22.8653286356
Gradients: w=>0.0089683193, b=>-0.1547277114
Iteration 7155: Achieved Loss=> 22.8653046209
Gradients: w=>0.0089635515, b=>-0.1546454534
Iteration 7156: Achieved Loss=> 22.8652806317
Gradients: w=>0.0089587862, b=>-0.1545632391
Iteration 7157: Achieved Loss=> 22.8652566680
Gradients: w=>0.0089540234, b=>-0.1544810685
Iteration 7158: Achieved Loss=> 22.8652327298
Gradients: w=>0.0089492632, b=>-0.1543989416
Iteration 7159: Achieved Loss=> 22.8652088171
Gradients: w=>0.0089445055, b=>-0.1543168584
Iteration 7160: Achieved Loss=> 22.8651849297
Gradients: w=>0.0089397503, b=>-0.1542348189
Iteration 7161: Achieved Loss=> 22.8651610678
Gradients: w=>0.0089349976, b=>-0.1541528229
Iteration 7162: Achieved Loss=> 22.8651372312
Gradients: w=>0.0089302475, b=>-0.1540708705
Iteration 7163: Achieved Loss=> 22.8651134199
Gradients: w=>0.0089254999, b=>-0.1539889617
Iteration 7164: Achieved Loss=> 22.8650896340
Gradients: w=>0.0089207549, b=>-0.1539070965
Iteration 7165: Achieved Loss=> 22.8650658733
Gradients: w=>0.0089160123, b=>-0.1538252747
Iteration 7166: Achieved Loss=> 22.8650421379
Gradients: w=>0.0089112723, b=>-0.1537434965
Iteration 7167: Achieved Loss=> 22.8650184278
Gradients: w=>0.0089065348, b=>-0.1536617617
Iteration 7168: Achieved Loss=> 22.8649947428
Gradients: w=>0.0089017998, b=>-0.1535800704
Iteration 7169: Achieved Loss=> 22.8649710830
Gradients: w=>0.0088970673, b=>-0.1534984225
Iteration 7170: Achieved Loss=> 22.8649474484
Gradients: w=>0.0088923374, b=>-0.1534168181
Iteration 7171: Achieved Loss=> 22.8649238388
Gradients: w=>0.0088876099, b=>-0.1533352570
Iteration 7172: Achieved Loss=> 22.8649002544
Gradients: w=>0.0088828850, b=>-0.1532537392
Iteration 7173: Achieved Loss=> 22.8648766951
Gradients: w=>0.0088781626, b=>-0.1531722648
Iteration 7174: Achieved Loss=> 22.8648531608
Gradients: w=>0.0088734427, b=>-0.1530908338
Iteration 7175: Achieved Loss=> 22.8648296515
Gradients: w=>0.0088687253, b=>-0.1530094460
Iteration 7176: Achieved Loss=> 22.8648061672
Gradients: w=>0.0088640104, b=>-0.1529281015
Iteration 7177: Achieved Loss=> 22.8647827078
Gradients: w=>0.0088592980, b=>-0.1528468002
Iteration 7178: Achieved Loss=> 22.8647592734
Gradients: w=>0.0088545881, b=>-0.1527655421
Iteration 7179: Achieved Loss=> 22.8647358639
Gradients: w=>0.0088498808, b=>-0.1526843273
Iteration 7180: Achieved Loss=> 22.8647124793
Gradients: w=>0.0088451759, b=>-0.1526031556
Iteration 7181: Achieved Loss=> 22.8646891196
Gradients: w=>0.0088404735, b=>-0.1525220271
Iteration 7182: Achieved Loss=> 22.8646657847
Gradients: w=>0.0088357737, b=>-0.1524409417
Iteration 7183: Achieved Loss=> 22.8646424746
Gradients: w=>0.0088310763, b=>-0.1523598994
Iteration 7184: Achieved Loss=> 22.8646191892
Gradients: w=>0.0088263814, b=>-0.1522789002
Iteration 7185: Achieved Loss=> 22.8645959286
Gradients: w=>0.0088216891, b=>-0.1521979441
Iteration 7186: Achieved Loss=> 22.8645726928
Gradients: w=>0.0088169992, b=>-0.1521170310
Iteration 7187: Achieved Loss=> 22.8645494816
Gradients: w=>0.0088123118, b=>-0.1520361609
Iteration 7188: Achieved Loss=> 22.8645262951
Gradients: w=>0.0088076269, b=>-0.1519553338
Iteration 7189: Achieved Loss=> 22.8645031333
Gradients: w=>0.0088029445, b=>-0.1518745497
Iteration 7190: Achieved Loss=> 22.8644799961
Gradients: w=>0.0087982646, b=>-0.1517938085
Iteration 7191: Achieved Loss=> 22.8644568835
Gradients: w=>0.0087935871, b=>-0.1517131103
Iteration 7192: Achieved Loss=> 22.8644337954
Gradients: w=>0.0087889122, b=>-0.1516324549
Iteration 7193: Achieved Loss=> 22.8644107319
Gradients: w=>0.0087842397, b=>-0.1515518425
Iteration 7194: Achieved Loss=> 22.8643876929
Gradients: w=>0.0087795698, b=>-0.1514712728
Iteration 7195: Achieved Loss=> 22.8643646784
Gradients: w=>0.0087749023, b=>-0.1513907461
Iteration 7196: Achieved Loss=> 22.8643416883
Gradients: w=>0.0087702373, b=>-0.1513102621
Iteration 7197: Achieved Loss=> 22.8643187227
Gradients: w=>0.0087655748, b=>-0.1512298209
Iteration 7198: Achieved Loss=> 22.8642957815
Gradients: w=>0.0087609147, b=>-0.1511494225
Iteration 7199: Achieved Loss=> 22.8642728647
Gradients: w=>0.0087562571, b=>-0.1510690668
Iteration 7200: Achieved Loss=> 22.8642499723
Gradients: w=>0.0087516020, b=>-0.1509887539
Iteration 7201: Achieved Loss=> 22.8642271042
Gradients: w=>0.0087469494, b=>-0.1509084836
Iteration 7202: Achieved Loss=> 22.8642042604
Gradients: w=>0.0087422993, b=>-0.1508282560
Iteration 7203: Achieved Loss=> 22.8641814408
Gradients: w=>0.0087376516, b=>-0.1507480711
Iteration 7204: Achieved Loss=> 22.8641586456
Gradients: w=>0.0087330064, b=>-0.1506679288
Iteration 7205: Achieved Loss=> 22.8641358745
Gradients: w=>0.0087283637, b=>-0.1505878291
Iteration 7206: Achieved Loss=> 22.8641131277
Gradients: w=>0.0087237234, b=>-0.1505077720
Iteration 7207: Achieved Loss=> 22.8640904051
Gradients: w=>0.0087190856, b=>-0.1504277575
Iteration 7208: Achieved Loss=> 22.8640677066
Gradients: w=>0.0087144503, b=>-0.1503477854
Iteration 7209: Achieved Loss=> 22.8640450322
Gradients: w=>0.0087098174, b=>-0.1502678559
Iteration 7210: Achieved Loss=> 22.8640223819
Gradients: w=>0.0087051870, b=>-0.1501879689
Iteration 7211: Achieved Loss=> 22.8639997557
Gradients: w=>0.0087005590, b=>-0.1501081244
Iteration 7212: Achieved Loss=> 22.8639771536
Gradients: w=>0.0086959336, b=>-0.1500283223
Iteration 7213: Achieved Loss=> 22.8639545755
Gradients: w=>0.0086913105, b=>-0.1499485627
Iteration 7214: Achieved Loss=> 22.8639320214
Gradients: w=>0.0086866900, b=>-0.1498688454
Iteration 7215: Achieved Loss=> 22.8639094912
Gradients: w=>0.0086820719, b=>-0.1497891705
Iteration 7216: Achieved Loss=> 22.8638869850
Gradients: w=>0.0086774562, b=>-0.1497095380
Iteration 7217: Achieved Loss=> 22.8638645028
Gradients: w=>0.0086728430, b=>-0.1496299478
Iteration 7218: Achieved Loss=> 22.8638420444
Gradients: w=>0.0086682322, b=>-0.1495503999
Iteration 7219: Achieved Loss=> 22.8638196099
Gradients: w=>0.0086636239, b=>-0.1494708944
Iteration 7220: Achieved Loss=> 22.8637971993
Gradients: w=>0.0086590181, b=>-0.1493914310
Iteration 7221: Achieved Loss=> 22.8637748124
Gradients: w=>0.0086544147, b=>-0.1493120100
Iteration 7222: Achieved Loss=> 22.8637524494
Gradients: w=>0.0086498137, b=>-0.1492326311
Iteration 7223: Achieved Loss=> 22.8637301101
Gradients: w=>0.0086452152, b=>-0.1491532945
Iteration 7224: Achieved Loss=> 22.8637077946
Gradients: w=>0.0086406192, b=>-0.1490740000
Iteration 7225: Achieved Loss=> 22.8636855028
Gradients: w=>0.0086360256, b=>-0.1489947477
Iteration 7226: Achieved Loss=> 22.8636632347
Gradients: w=>0.0086314344, b=>-0.1489155375
Iteration 7227: Achieved Loss=> 22.8636409903
Gradients: w=>0.0086268456, b=>-0.1488363694
Iteration 7228: Achieved Loss=> 22.8636187695
Gradients: w=>0.0086222594, b=>-0.1487572435
Iteration 7229: Achieved Loss=> 22.8635965724
Gradients: w=>0.0086176755, b=>-0.1486781595
Iteration 7230: Achieved Loss=> 22.8635743988
Gradients: w=>0.0086130941, b=>-0.1485991177
Iteration 7231: Achieved Loss=> 22.8635522488
Gradients: w=>0.0086085151, b=>-0.1485201178
Iteration 7232: Achieved Loss=> 22.8635301224
Gradients: w=>0.0086039385, b=>-0.1484411600
Iteration 7233: Achieved Loss=> 22.8635080194
Gradients: w=>0.0085993644, b=>-0.1483622441
Iteration 7234: Achieved Loss=> 22.8634859400
Gradients: w=>0.0085947927, b=>-0.1482833702
Iteration 7235: Achieved Loss=> 22.8634638840
Gradients: w=>0.0085902235, b=>-0.1482045382
Iteration 7236: Achieved Loss=> 22.8634418515
Gradients: w=>0.0085856567, b=>-0.1481257481
Iteration 7237: Achieved Loss=> 22.8634198424
Gradients: w=>0.0085810923, b=>-0.1480469999
Iteration 7238: Achieved Loss=> 22.8633978567
Gradients: w=>0.0085765303, b=>-0.1479682936
Iteration 7239: Achieved Loss=> 22.8633758944
Gradients: w=>0.0085719707, b=>-0.1478896291
Iteration 7240: Achieved Loss=> 22.8633539554
Gradients: w=>0.0085674136, b=>-0.1478110064
Iteration 7241: Achieved Loss=> 22.8633320397
Gradients: w=>0.0085628589, b=>-0.1477324255
Iteration 7242: Achieved Loss=> 22.8633101474
Gradients: w=>0.0085583066, b=>-0.1476538865
Iteration 7243: Achieved Loss=> 22.8632882783
Gradients: w=>0.0085537568, b=>-0.1475753891
Iteration 7244: Achieved Loss=> 22.8632664324
Gradients: w=>0.0085492093, b=>-0.1474969335
Iteration 7245: Achieved Loss=> 22.8632446098
Gradients: w=>0.0085446643, b=>-0.1474185196
Iteration 7246: Achieved Loss=> 22.8632228103
Gradients: w=>0.0085401217, b=>-0.1473401474
Iteration 7247: Achieved Loss=> 22.8632010341
Gradients: w=>0.0085355815, b=>-0.1472618169
Iteration 7248: Achieved Loss=> 22.8631792810
Gradients: w=>0.0085310437, b=>-0.1471835280
Iteration 7249: Achieved Loss=> 22.8631575510
Gradients: w=>0.0085265084, b=>-0.1471052807
Iteration 7250: Achieved Loss=> 22.8631358441
Gradients: w=>0.0085219754, b=>-0.1470270750
Iteration 7251: Achieved Loss=> 22.8631141603
Gradients: w=>0.0085174449, b=>-0.1469489109
Iteration 7252: Achieved Loss=> 22.8630924995
Gradients: w=>0.0085129167, b=>-0.1468707883
Iteration 7253: Achieved Loss=> 22.8630708617
Gradients: w=>0.0085083910, b=>-0.1467927073
Iteration 7254: Achieved Loss=> 22.8630492470
Gradients: w=>0.0085038677, b=>-0.1467146678
Iteration 7255: Achieved Loss=> 22.8630276552
Gradients: w=>0.0084993468, b=>-0.1466366698
Iteration 7256: Achieved Loss=> 22.8630060864
Gradients: w=>0.0084948283, b=>-0.1465587132
Iteration 7257: Achieved Loss=> 22.8629845405
Gradients: w=>0.0084903121, b=>-0.1464807981
Iteration 7258: Achieved Loss=> 22.8629630175
Gradients: w=>0.0084857984, b=>-0.1464029244
Iteration 7259: Achieved Loss=> 22.8629415174
Gradients: w=>0.0084812871, b=>-0.1463250921
Iteration 7260: Achieved Loss=> 22.8629200402
Gradients: w=>0.0084767782, b=>-0.1462473012
Iteration 7261: Achieved Loss=> 22.8628985858
Gradients: w=>0.0084722717, b=>-0.1461695517
Iteration 7262: Achieved Loss=> 22.8628771541
Gradients: w=>0.0084677676, b=>-0.1460918435
Iteration 7263: Achieved Loss=> 22.8628557453
Gradients: w=>0.0084632659, b=>-0.1460141765
Iteration 7264: Achieved Loss=> 22.8628343592
Gradients: w=>0.0084587665, b=>-0.1459365509
Iteration 7265: Achieved Loss=> 22.8628129959
Gradients: w=>0.0084542696, b=>-0.1458589666
Iteration 7266: Achieved Loss=> 22.8627916552
Gradients: w=>0.0084497750, b=>-0.1457814235
Iteration 7267: Achieved Loss=> 22.8627703373
Gradients: w=>0.0084452829, b=>-0.1457039216
Iteration 7268: Achieved Loss=> 22.8627490420
Gradients: w=>0.0084407931, b=>-0.1456264609
Iteration 7269: Achieved Loss=> 22.8627277693
Gradients: w=>0.0084363057, b=>-0.1455490414
Iteration 7270: Achieved Loss=> 22.8627065193
Gradients: w=>0.0084318207, b=>-0.1454716631
Iteration 7271: Achieved Loss=> 22.8626852918
Gradients: w=>0.0084273381, b=>-0.1453943259
Iteration 7272: Achieved Loss=> 22.8626640869
Gradients: w=>0.0084228579, b=>-0.1453170298
Iteration 7273: Achieved Loss=> 22.8626429046
Gradients: w=>0.0084183800, b=>-0.1452397748
Iteration 7274: Achieved Loss=> 22.8626217447
Gradients: w=>0.0084139046, b=>-0.1451625609
Iteration 7275: Achieved Loss=> 22.8626006074
Gradients: w=>0.0084094315, b=>-0.1450853880
Iteration 7276: Achieved Loss=> 22.8625794925
Gradients: w=>0.0084049608, b=>-0.1450082562
Iteration 7277: Achieved Loss=> 22.8625584001
Gradients: w=>0.0084004924, b=>-0.1449311653
Iteration 7278: Achieved Loss=> 22.8625373301
Gradients: w=>0.0083960265, b=>-0.1448541155
Iteration 7279: Achieved Loss=> 22.8625162825
Gradients: w=>0.0083915629, b=>-0.1447771066
Iteration 7280: Achieved Loss=> 22.8624952572
Gradients: w=>0.0083871017, b=>-0.1447001386
Iteration 7281: Achieved Loss=> 22.8624742544
Gradients: w=>0.0083826428, b=>-0.1446232116
Iteration 7282: Achieved Loss=> 22.8624532738
Gradients: w=>0.0083781863, b=>-0.1445463254
Iteration 7283: Achieved Loss=> 22.8624323155
Gradients: w=>0.0083737322, b=>-0.1444694802
Iteration 7284: Achieved Loss=> 22.8624113795
Gradients: w=>0.0083692805, b=>-0.1443926758
Iteration 7285: Achieved Loss=> 22.8623904658
Gradients: w=>0.0083648311, b=>-0.1443159122
Iteration 7286: Achieved Loss=> 22.8623695743
Gradients: w=>0.0083603841, b=>-0.1442391894
Iteration 7287: Achieved Loss=> 22.8623487050
Gradients: w=>0.0083559395, b=>-0.1441625074
Iteration 7288: Achieved Loss=> 22.8623278579
Gradients: w=>0.0083514972, b=>-0.1440858662
Iteration 7289: Achieved Loss=> 22.8623070330
Gradients: w=>0.0083470573, b=>-0.1440092658
Iteration 7290: Achieved Loss=> 22.8622862302
Gradients: w=>0.0083426198, b=>-0.1439327060
Iteration 7291: Achieved Loss=> 22.8622654495
Gradients: w=>0.0083381846, b=>-0.1438561870
Iteration 7292: Achieved Loss=> 22.8622446909
Gradients: w=>0.0083337517, b=>-0.1437797086
Iteration 7293: Achieved Loss=> 22.8622239543
Gradients: w=>0.0083293213, b=>-0.1437032709
Iteration 7294: Achieved Loss=> 22.8622032398
Gradients: w=>0.0083248931, b=>-0.1436268738
Iteration 7295: Achieved Loss=> 22.8621825473
Gradients: w=>0.0083204674, b=>-0.1435505174
Iteration 7296: Achieved Loss=> 22.8621618768
Gradients: w=>0.0083160440, b=>-0.1434742015
Iteration 7297: Achieved Loss=> 22.8621412283
Gradients: w=>0.0083116229, b=>-0.1433979262
Iteration 7298: Achieved Loss=> 22.8621206018
Gradients: w=>0.0083072042, b=>-0.1433216915
Iteration 7299: Achieved Loss=> 22.8620999971
Gradients: w=>0.0083027878, b=>-0.1432454973
Iteration 7300: Achieved Loss=> 22.8620794144
Gradients: w=>0.0082983738, b=>-0.1431693436
Iteration 7301: Achieved Loss=> 22.8620588535
Gradients: w=>0.0082939621, b=>-0.1430932304
Iteration 7302: Achieved Loss=> 22.8620383145
Gradients: w=>0.0082895528, b=>-0.1430171576
Iteration 7303: Achieved Loss=> 22.8620177974
Gradients: w=>0.0082851458, b=>-0.1429411253
Iteration 7304: Achieved Loss=> 22.8619973020
Gradients: w=>0.0082807412, b=>-0.1428651334
Iteration 7305: Achieved Loss=> 22.8619768284
Gradients: w=>0.0082763389, b=>-0.1427891819
Iteration 7306: Achieved Loss=> 22.8619563766
Gradients: w=>0.0082719389, b=>-0.1427132708
Iteration 7307: Achieved Loss=> 22.8619359466
Gradients: w=>0.0082675413, b=>-0.1426374000
Iteration 7308: Achieved Loss=> 22.8619155382
Gradients: w=>0.0082631460, b=>-0.1425615696
Iteration 7309: Achieved Loss=> 22.8618951515
Gradients: w=>0.0082587531, b=>-0.1424857795
Iteration 7310: Achieved Loss=> 22.8618747866
Gradients: w=>0.0082543625, b=>-0.1424100297
Iteration 7311: Achieved Loss=> 22.8618544432
Gradients: w=>0.0082499742, b=>-0.1423343202
Iteration 7312: Achieved Loss=> 22.8618341215
Gradients: w=>0.0082455883, b=>-0.1422586509
Iteration 7313: Achieved Loss=> 22.8618138214
Gradients: w=>0.0082412047, b=>-0.1421830218
Iteration 7314: Achieved Loss=> 22.8617935428
Gradients: w=>0.0082368234, b=>-0.1421074329
Iteration 7315: Achieved Loss=> 22.8617732859
Gradients: w=>0.0082324444, b=>-0.1420318843
Iteration 7316: Achieved Loss=> 22.8617530504
Gradients: w=>0.0082280678, b=>-0.1419563758
Iteration 7317: Achieved Loss=> 22.8617328365
Gradients: w=>0.0082236935, b=>-0.1418809074
Iteration 7318: Achieved Loss=> 22.8617126440
Gradients: w=>0.0082193216, b=>-0.1418054791
Iteration 7319: Achieved Loss=> 22.8616924730
Gradients: w=>0.0082149519, b=>-0.1417300910
Iteration 7320: Achieved Loss=> 22.8616723235
Gradients: w=>0.0082105846, b=>-0.1416547429
Iteration 7321: Achieved Loss=> 22.8616521954
Gradients: w=>0.0082062196, b=>-0.1415794349
Iteration 7322: Achieved Loss=> 22.8616320886
Gradients: w=>0.0082018569, b=>-0.1415041669
Iteration 7323: Achieved Loss=> 22.8616120033
Gradients: w=>0.0081974966, b=>-0.1414289390
Iteration 7324: Achieved Loss=> 22.8615919393
Gradients: w=>0.0081931385, b=>-0.1413537510
Iteration 7325: Achieved Loss=> 22.8615718966
Gradients: w=>0.0081887828, b=>-0.1412786030
Iteration 7326: Achieved Loss=> 22.8615518752
Gradients: w=>0.0081844294, b=>-0.1412034950
Iteration 7327: Achieved Loss=> 22.8615318751
Gradients: w=>0.0081800783, b=>-0.1411284269
Iteration 7328: Achieved Loss=> 22.8615118963
Gradients: w=>0.0081757295, b=>-0.1410533987
Iteration 7329: Achieved Loss=> 22.8614919387
Gradients: w=>0.0081713830, b=>-0.1409784104
Iteration 7330: Achieved Loss=> 22.8614720023
Gradients: w=>0.0081670389, b=>-0.1409034619
Iteration 7331: Achieved Loss=> 22.8614520871
Gradients: w=>0.0081626970, b=>-0.1408285533
Iteration 7332: Achieved Loss=> 22.8614321931
Gradients: w=>0.0081583575, b=>-0.1407536845
Iteration 7333: Achieved Loss=> 22.8614123202
Gradients: w=>0.0081540203, b=>-0.1406788555
Iteration 7334: Achieved Loss=> 22.8613924684
Gradients: w=>0.0081496853, b=>-0.1406040663
Iteration 7335: Achieved Loss=> 22.8613726378
Gradients: w=>0.0081453527, b=>-0.1405293169
Iteration 7336: Achieved Loss=> 22.8613528282
Gradients: w=>0.0081410224, b=>-0.1404546072
Iteration 7337: Achieved Loss=> 22.8613330397
Gradients: w=>0.0081366944, b=>-0.1403799372
Iteration 7338: Achieved Loss=> 22.8613132722
Gradients: w=>0.0081323687, b=>-0.1403053069
Iteration 7339: Achieved Loss=> 22.8612935258
Gradients: w=>0.0081280452, b=>-0.1402307163
Iteration 7340: Achieved Loss=> 22.8612738003
Gradients: w=>0.0081237241, b=>-0.1401561654
Iteration 7341: Achieved Loss=> 22.8612540958
Gradients: w=>0.0081194053, b=>-0.1400816541
Iteration 7342: Achieved Loss=> 22.8612344122
Gradients: w=>0.0081150888, b=>-0.1400071824
Iteration 7343: Achieved Loss=> 22.8612147496
Gradients: w=>0.0081107745, b=>-0.1399327502
Iteration 7344: Achieved Loss=> 22.8611951079
Gradients: w=>0.0081064626, b=>-0.1398583577
Iteration 7345: Achieved Loss=> 22.8611754870
Gradients: w=>0.0081021530, b=>-0.1397840047
Iteration 7346: Achieved Loss=> 22.8611558870
Gradients: w=>0.0080978456, b=>-0.1397096912
Iteration 7347: Achieved Loss=> 22.8611363078
Gradients: w=>0.0080935406, b=>-0.1396354173
Iteration 7348: Achieved Loss=> 22.8611167495
Gradients: w=>0.0080892378, b=>-0.1395611828
Iteration 7349: Achieved Loss=> 22.8610972119
Gradients: w=>0.0080849373, b=>-0.1394869878
Iteration 7350: Achieved Loss=> 22.8610776951
Gradients: w=>0.0080806391, b=>-0.1394128322
Iteration 7351: Achieved Loss=> 22.8610581991
Gradients: w=>0.0080763432, b=>-0.1393387161
Iteration 7352: Achieved Loss=> 22.8610387237
Gradients: w=>0.0080720495, b=>-0.1392646393
Iteration 7353: Achieved Loss=> 22.8610192691
Gradients: w=>0.0080677582, b=>-0.1391906020
Iteration 7354: Achieved Loss=> 22.8609998352
Gradients: w=>0.0080634691, b=>-0.1391166040
Iteration 7355: Achieved Loss=> 22.8609804219
Gradients: w=>0.0080591823, b=>-0.1390426453
Iteration 7356: Achieved Loss=> 22.8609610292
Gradients: w=>0.0080548978, b=>-0.1389687260
Iteration 7357: Achieved Loss=> 22.8609416572
Gradients: w=>0.0080506156, b=>-0.1388948460
Iteration 7358: Achieved Loss=> 22.8609223058
Gradients: w=>0.0080463356, b=>-0.1388210052
Iteration 7359: Achieved Loss=> 22.8609029749
Gradients: w=>0.0080420580, b=>-0.1387472037
Iteration 7360: Achieved Loss=> 22.8608836645
Gradients: w=>0.0080377826, b=>-0.1386734414
Iteration 7361: Achieved Loss=> 22.8608643747
Gradients: w=>0.0080335094, b=>-0.1385997184
Iteration 7362: Achieved Loss=> 22.8608451055
Gradients: w=>0.0080292386, b=>-0.1385260345
Iteration 7363: Achieved Loss=> 22.8608258566
Gradients: w=>0.0080249700, b=>-0.1384523898
Iteration 7364: Achieved Loss=> 22.8608066283
Gradients: w=>0.0080207037, b=>-0.1383787843
Iteration 7365: Achieved Loss=> 22.8607874204
Gradients: w=>0.0080164396, b=>-0.1383052179
Iteration 7366: Achieved Loss=> 22.8607682329
Gradients: w=>0.0080121778, b=>-0.1382316906
Iteration 7367: Achieved Loss=> 22.8607490658
Gradients: w=>0.0080079183, b=>-0.1381582023
Iteration 7368: Achieved Loss=> 22.8607299191
Gradients: w=>0.0080036610, b=>-0.1380847532
Iteration 7369: Achieved Loss=> 22.8607107927
Gradients: w=>0.0079994061, b=>-0.1380113431
Iteration 7370: Achieved Loss=> 22.8606916866
Gradients: w=>0.0079951533, b=>-0.1379379720
Iteration 7371: Achieved Loss=> 22.8606726009
Gradients: w=>0.0079909029, b=>-0.1378646400
Iteration 7372: Achieved Loss=> 22.8606535355
Gradients: w=>0.0079866546, b=>-0.1377913469
Iteration 7373: Achieved Loss=> 22.8606344903
Gradients: w=>0.0079824087, b=>-0.1377180928
Iteration 7374: Achieved Loss=> 22.8606154654
Gradients: w=>0.0079781650, b=>-0.1376448776
Iteration 7375: Achieved Loss=> 22.8605964606
Gradients: w=>0.0079739236, b=>-0.1375717014
Iteration 7376: Achieved Loss=> 22.8605774761
Gradients: w=>0.0079696844, b=>-0.1374985641
Iteration 7377: Achieved Loss=> 22.8605585118
Gradients: w=>0.0079654475, b=>-0.1374254656
Iteration 7378: Achieved Loss=> 22.8605395676
Gradients: w=>0.0079612128, b=>-0.1373524060
Iteration 7379: Achieved Loss=> 22.8605206436
Gradients: w=>0.0079569804, b=>-0.1372793852
Iteration 7380: Achieved Loss=> 22.8605017397
Gradients: w=>0.0079527502, b=>-0.1372064033
Iteration 7381: Achieved Loss=> 22.8604828559
Gradients: w=>0.0079485222, b=>-0.1371334602
Iteration 7382: Achieved Loss=> 22.8604639921
Gradients: w=>0.0079442966, b=>-0.1370605558
Iteration 7383: Achieved Loss=> 22.8604451484
Gradients: w=>0.0079400731, b=>-0.1369876902
Iteration 7384: Achieved Loss=> 22.8604263248
Gradients: w=>0.0079358520, b=>-0.1369148634
Iteration 7385: Achieved Loss=> 22.8604075211
Gradients: w=>0.0079316330, b=>-0.1368420752
Iteration 7386: Achieved Loss=> 22.8603887374
Gradients: w=>0.0079274163, b=>-0.1367693258
Iteration 7387: Achieved Loss=> 22.8603699737
Gradients: w=>0.0079232019, b=>-0.1366966150
Iteration 7388: Achieved Loss=> 22.8603512300
Gradients: w=>0.0079189896, b=>-0.1366239429
Iteration 7389: Achieved Loss=> 22.8603325061
Gradients: w=>0.0079147797, b=>-0.1365513094
Iteration 7390: Achieved Loss=> 22.8603138022
Gradients: w=>0.0079105719, b=>-0.1364787145
Iteration 7391: Achieved Loss=> 22.8602951181
Gradients: w=>0.0079063664, b=>-0.1364061583
Iteration 7392: Achieved Loss=> 22.8602764540
Gradients: w=>0.0079021631, b=>-0.1363336406
Iteration 7393: Achieved Loss=> 22.8602578096
Gradients: w=>0.0078979621, b=>-0.1362611614
Iteration 7394: Achieved Loss=> 22.8602391851
Gradients: w=>0.0078937633, b=>-0.1361887208
Iteration 7395: Achieved Loss=> 22.8602205804
Gradients: w=>0.0078895667, b=>-0.1361163187
Iteration 7396: Achieved Loss=> 22.8602019954
Gradients: w=>0.0078853724, b=>-0.1360439551
Iteration 7397: Achieved Loss=> 22.8601834302
Gradients: w=>0.0078811803, b=>-0.1359716299
Iteration 7398: Achieved Loss=> 22.8601648847
Gradients: w=>0.0078769904, b=>-0.1358993432
Iteration 7399: Achieved Loss=> 22.8601463590
Gradients: w=>0.0078728028, b=>-0.1358270950
Iteration 7400: Achieved Loss=> 22.8601278529
Gradients: w=>0.0078686174, b=>-0.1357548851
Iteration 7401: Achieved Loss=> 22.8601093665
Gradients: w=>0.0078644342, b=>-0.1356827137
Iteration 7402: Achieved Loss=> 22.8600908998
Gradients: w=>0.0078602532, b=>-0.1356105806
Iteration 7403: Achieved Loss=> 22.8600724527
Gradients: w=>0.0078560744, b=>-0.1355384858
Iteration 7404: Achieved Loss=> 22.8600540252
Gradients: w=>0.0078518979, b=>-0.1354664294
Iteration 7405: Achieved Loss=> 22.8600356173
Gradients: w=>0.0078477236, b=>-0.1353944113
Iteration 7406: Achieved Loss=> 22.8600172289
Gradients: w=>0.0078435515, b=>-0.1353224315
Iteration 7407: Achieved Loss=> 22.8599988601
Gradients: w=>0.0078393816, b=>-0.1352504899
Iteration 7408: Achieved Loss=> 22.8599805109
Gradients: w=>0.0078352140, b=>-0.1351785866
Iteration 7409: Achieved Loss=> 22.8599621811
Gradients: w=>0.0078310485, b=>-0.1351067215
Iteration 7410: Achieved Loss=> 22.8599438708
Gradients: w=>0.0078268853, b=>-0.1350348946
Iteration 7411: Achieved Loss=> 22.8599255800
Gradients: w=>0.0078227243, b=>-0.1349631059
Iteration 7412: Achieved Loss=> 22.8599073086
Gradients: w=>0.0078185655, b=>-0.1348913554
Iteration 7413: Achieved Loss=> 22.8598890567
Gradients: w=>0.0078144089, b=>-0.1348196430
Iteration 7414: Achieved Loss=> 22.8598708241
Gradients: w=>0.0078102545, b=>-0.1347479688
Iteration 7415: Achieved Loss=> 22.8598526109
Gradients: w=>0.0078061024, b=>-0.1346763326
Iteration 7416: Achieved Loss=> 22.8598344171
Gradients: w=>0.0078019524, b=>-0.1346047345
Iteration 7417: Achieved Loss=> 22.8598162426
Gradients: w=>0.0077978046, b=>-0.1345331745
Iteration 7418: Achieved Loss=> 22.8597980875
Gradients: w=>0.0077936591, b=>-0.1344616526
Iteration 7419: Achieved Loss=> 22.8597799516
Gradients: w=>0.0077895157, b=>-0.1343901686
Iteration 7420: Achieved Loss=> 22.8597618351
Gradients: w=>0.0077853746, b=>-0.1343187227
Iteration 7421: Achieved Loss=> 22.8597437377
Gradients: w=>0.0077812356, b=>-0.1342473147
Iteration 7422: Achieved Loss=> 22.8597256597
Gradients: w=>0.0077770989, b=>-0.1341759447
Iteration 7423: Achieved Loss=> 22.8597076008
Gradients: w=>0.0077729644, b=>-0.1341046127
Iteration 7424: Achieved Loss=> 22.8596895611
Gradients: w=>0.0077688320, b=>-0.1340333185
Iteration 7425: Achieved Loss=> 22.8596715406
Gradients: w=>0.0077647019, b=>-0.1339620623
Iteration 7426: Achieved Loss=> 22.8596535393
Gradients: w=>0.0077605739, b=>-0.1338908440
Iteration 7427: Achieved Loss=> 22.8596355571
Gradients: w=>0.0077564481, b=>-0.1338196635
Iteration 7428: Achieved Loss=> 22.8596175940
Gradients: w=>0.0077523246, b=>-0.1337485208
Iteration 7429: Achieved Loss=> 22.8595996500
Gradients: w=>0.0077482032, b=>-0.1336774160
Iteration 7430: Achieved Loss=> 22.8595817251
Gradients: w=>0.0077440840, b=>-0.1336063490
Iteration 7431: Achieved Loss=> 22.8595638192
Gradients: w=>0.0077399670, b=>-0.1335353198
Iteration 7432: Achieved Loss=> 22.8595459324
Gradients: w=>0.0077358522, b=>-0.1334643283
Iteration 7433: Achieved Loss=> 22.8595280646
Gradients: w=>0.0077317396, b=>-0.1333933746
Iteration 7434: Achieved Loss=> 22.8595102157
Gradients: w=>0.0077276292, b=>-0.1333224586
Iteration 7435: Achieved Loss=> 22.8594923859
Gradients: w=>0.0077235209, b=>-0.1332515802
Iteration 7436: Achieved Loss=> 22.8594745750
Gradients: w=>0.0077194149, b=>-0.1331807396
Iteration 7437: Achieved Loss=> 22.8594567830
Gradients: w=>0.0077153110, b=>-0.1331099366
Iteration 7438: Achieved Loss=> 22.8594390100
Gradients: w=>0.0077112093, b=>-0.1330391713
Iteration 7439: Achieved Loss=> 22.8594212558
Gradients: w=>0.0077071098, b=>-0.1329684436
Iteration 7440: Achieved Loss=> 22.8594035205
Gradients: w=>0.0077030124, b=>-0.1328977535
Iteration 7441: Achieved Loss=> 22.8593858041
Gradients: w=>0.0076989173, b=>-0.1328271010
Iteration 7442: Achieved Loss=> 22.8593681065
Gradients: w=>0.0076948243, b=>-0.1327564860
Iteration 7443: Achieved Loss=> 22.8593504277
Gradients: w=>0.0076907335, b=>-0.1326859086
Iteration 7444: Achieved Loss=> 22.8593327677
Gradients: w=>0.0076866449, b=>-0.1326153687
Iteration 7445: Achieved Loss=> 22.8593151264
Gradients: w=>0.0076825584, b=>-0.1325448663
Iteration 7446: Achieved Loss=> 22.8592975040
Gradients: w=>0.0076784741, b=>-0.1324744014
Iteration 7447: Achieved Loss=> 22.8592799002
Gradients: w=>0.0076743920, b=>-0.1324039739
Iteration 7448: Achieved Loss=> 22.8592623152
Gradients: w=>0.0076703121, b=>-0.1323335839
Iteration 7449: Achieved Loss=> 22.8592447488
Gradients: w=>0.0076662343, b=>-0.1322632313
Iteration 7450: Achieved Loss=> 22.8592272012
Gradients: w=>0.0076621587, b=>-0.1321929161
Iteration 7451: Achieved Loss=> 22.8592096722
Gradients: w=>0.0076580853, b=>-0.1321226383
Iteration 7452: Achieved Loss=> 22.8591921618
Gradients: w=>0.0076540140, b=>-0.1320523978
Iteration 7453: Achieved Loss=> 22.8591746700
Gradients: w=>0.0076499449, b=>-0.1319821947
Iteration 7454: Achieved Loss=> 22.8591571968
Gradients: w=>0.0076458779, b=>-0.1319120290
Iteration 7455: Achieved Loss=> 22.8591397422
Gradients: w=>0.0076418131, b=>-0.1318419005
Iteration 7456: Achieved Loss=> 22.8591223062
Gradients: w=>0.0076377505, b=>-0.1317718093
Iteration 7457: Achieved Loss=> 22.8591048887
Gradients: w=>0.0076336901, b=>-0.1317017553
Iteration 7458: Achieved Loss=> 22.8590874897
Gradients: w=>0.0076296318, b=>-0.1316317386
Iteration 7459: Achieved Loss=> 22.8590701092
Gradients: w=>0.0076255756, b=>-0.1315617592
Iteration 7460: Achieved Loss=> 22.8590527471
Gradients: w=>0.0076215216, b=>-0.1314918169
Iteration 7461: Achieved Loss=> 22.8590354036
Gradients: w=>0.0076174698, b=>-0.1314219118
Iteration 7462: Achieved Loss=> 22.8590180784
Gradients: w=>0.0076134201, b=>-0.1313520439
Iteration 7463: Achieved Loss=> 22.8590007717
Gradients: w=>0.0076093726, b=>-0.1312822131
Iteration 7464: Achieved Loss=> 22.8589834834
Gradients: w=>0.0076053272, b=>-0.1312124194
Iteration 7465: Achieved Loss=> 22.8589662134
Gradients: w=>0.0076012840, b=>-0.1311426629
Iteration 7466: Achieved Loss=> 22.8589489618
Gradients: w=>0.0075972429, b=>-0.1310729434
Iteration 7467: Achieved Loss=> 22.8589317286
Gradients: w=>0.0075932040, b=>-0.1310032610
Iteration 7468: Achieved Loss=> 22.8589145136
Gradients: w=>0.0075891672, b=>-0.1309336157
Iteration 7469: Achieved Loss=> 22.8588973170
Gradients: w=>0.0075851326, b=>-0.1308640074
Iteration 7470: Achieved Loss=> 22.8588801387
Gradients: w=>0.0075811001, b=>-0.1307944360
Iteration 7471: Achieved Loss=> 22.8588629786
Gradients: w=>0.0075770697, b=>-0.1307249017
Iteration 7472: Achieved Loss=> 22.8588458367
Gradients: w=>0.0075730415, b=>-0.1306554043
Iteration 7473: Achieved Loss=> 22.8588287131
Gradients: w=>0.0075690155, b=>-0.1305859439
Iteration 7474: Achieved Loss=> 22.8588116076
Gradients: w=>0.0075649915, b=>-0.1305165204
Iteration 7475: Achieved Loss=> 22.8587945204
Gradients: w=>0.0075609698, b=>-0.1304471338
Iteration 7476: Achieved Loss=> 22.8587774513
Gradients: w=>0.0075569501, b=>-0.1303777841
Iteration 7477: Achieved Loss=> 22.8587604004
Gradients: w=>0.0075529326, b=>-0.1303084713
Iteration 7478: Achieved Loss=> 22.8587433676
Gradients: w=>0.0075489172, b=>-0.1302391953
Iteration 7479: Achieved Loss=> 22.8587263528
Gradients: w=>0.0075449040, b=>-0.1301699561
Iteration 7480: Achieved Loss=> 22.8587093562
Gradients: w=>0.0075408929, b=>-0.1301007538
Iteration 7481: Achieved Loss=> 22.8586923777
Gradients: w=>0.0075368839, b=>-0.1300315882
Iteration 7482: Achieved Loss=> 22.8586754172
Gradients: w=>0.0075328771, b=>-0.1299624595
Iteration 7483: Achieved Loss=> 22.8586584747
Gradients: w=>0.0075288724, b=>-0.1298933674
Iteration 7484: Achieved Loss=> 22.8586415502
Gradients: w=>0.0075248698, b=>-0.1298243121
Iteration 7485: Achieved Loss=> 22.8586246437
Gradients: w=>0.0075208694, b=>-0.1297552935
Iteration 7486: Achieved Loss=> 22.8586077552
Gradients: w=>0.0075168710, b=>-0.1296863116
Iteration 7487: Achieved Loss=> 22.8585908847
Gradients: w=>0.0075128748, b=>-0.1296173664
Iteration 7488: Achieved Loss=> 22.8585740320
Gradients: w=>0.0075088808, b=>-0.1295484578
Iteration 7489: Achieved Loss=> 22.8585571973
Gradients: w=>0.0075048888, b=>-0.1294795859
Iteration 7490: Achieved Loss=> 22.8585403805
Gradients: w=>0.0075008990, b=>-0.1294107506
Iteration 7491: Achieved Loss=> 22.8585235816
Gradients: w=>0.0074969113, b=>-0.1293419519
Iteration 7492: Achieved Loss=> 22.8585068005
Gradients: w=>0.0074929257, b=>-0.1292731897
Iteration 7493: Achieved Loss=> 22.8584900372
Gradients: w=>0.0074889422, b=>-0.1292044641
Iteration 7494: Achieved Loss=> 22.8584732918
Gradients: w=>0.0074849609, b=>-0.1291357751
Iteration 7495: Achieved Loss=> 22.8584565642
Gradients: w=>0.0074809816, b=>-0.1290671225
Iteration 7496: Achieved Loss=> 22.8584398543
Gradients: w=>0.0074770045, b=>-0.1289985065
Iteration 7497: Achieved Loss=> 22.8584231623
Gradients: w=>0.0074730295, b=>-0.1289299269
Iteration 7498: Achieved Loss=> 22.8584064879
Gradients: w=>0.0074690566, b=>-0.1288613838
Iteration 7499: Achieved Loss=> 22.8583898313
Gradients: w=>0.0074650858, b=>-0.1287928771
Iteration 7500: Achieved Loss=> 22.8583731924
Gradients: w=>0.0074611172, b=>-0.1287244069
Iteration 7501: Achieved Loss=> 22.8583565712
Gradients: w=>0.0074571506, b=>-0.1286559730
Iteration 7502: Achieved Loss=> 22.8583399676
Gradients: w=>0.0074531861, b=>-0.1285875756
Iteration 7503: Achieved Loss=> 22.8583233817
Gradients: w=>0.0074492238, b=>-0.1285192145
Iteration 7504: Achieved Loss=> 22.8583068134
Gradients: w=>0.0074452636, b=>-0.1284508897
Iteration 7505: Achieved Loss=> 22.8582902628
Gradients: w=>0.0074413054, b=>-0.1283826013
Iteration 7506: Achieved Loss=> 22.8582737297
Gradients: w=>0.0074373494, b=>-0.1283143491
Iteration 7507: Achieved Loss=> 22.8582572142
Gradients: w=>0.0074333955, b=>-0.1282461333
Iteration 7508: Achieved Loss=> 22.8582407163
Gradients: w=>0.0074294437, b=>-0.1281779537
Iteration 7509: Achieved Loss=> 22.8582242359
Gradients: w=>0.0074254939, b=>-0.1281098104
Iteration 7510: Achieved Loss=> 22.8582077730
Gradients: w=>0.0074215463, b=>-0.1280417033
Iteration 7511: Achieved Loss=> 22.8581913276
Gradients: w=>0.0074176008, b=>-0.1279736324
Iteration 7512: Achieved Loss=> 22.8581748997
Gradients: w=>0.0074136574, b=>-0.1279055977
Iteration 7513: Achieved Loss=> 22.8581584893
Gradients: w=>0.0074097161, b=>-0.1278375991
Iteration 7514: Achieved Loss=> 22.8581420963
Gradients: w=>0.0074057768, b=>-0.1277696367
Iteration 7515: Achieved Loss=> 22.8581257207
Gradients: w=>0.0074018397, b=>-0.1277017105
Iteration 7516: Achieved Loss=> 22.8581093625
Gradients: w=>0.0073979046, b=>-0.1276338203
Iteration 7517: Achieved Loss=> 22.8580930218
Gradients: w=>0.0073939717, b=>-0.1275659663
Iteration 7518: Achieved Loss=> 22.8580766983
Gradients: w=>0.0073900408, b=>-0.1274981483
Iteration 7519: Achieved Loss=> 22.8580603923
Gradients: w=>0.0073861120, b=>-0.1274303664
Iteration 7520: Achieved Loss=> 22.8580441036
Gradients: w=>0.0073821854, b=>-0.1273626205
Iteration 7521: Achieved Loss=> 22.8580278322
Gradients: w=>0.0073782608, b=>-0.1272949106
Iteration 7522: Achieved Loss=> 22.8580115781
Gradients: w=>0.0073743383, b=>-0.1272272367
Iteration 7523: Achieved Loss=> 22.8579953412
Gradients: w=>0.0073704178, b=>-0.1271595988
Iteration 7524: Achieved Loss=> 22.8579791216
Gradients: w=>0.0073664995, b=>-0.1270919968
Iteration 7525: Achieved Loss=> 22.8579629193
Gradients: w=>0.0073625832, b=>-0.1270244308
Iteration 7526: Achieved Loss=> 22.8579467342
Gradients: w=>0.0073586691, b=>-0.1269569008
Iteration 7527: Achieved Loss=> 22.8579305663
Gradients: w=>0.0073547570, b=>-0.1268894066
Iteration 7528: Achieved Loss=> 22.8579144156
Gradients: w=>0.0073508469, b=>-0.1268219483
Iteration 7529: Achieved Loss=> 22.8578982820
Gradients: w=>0.0073469390, b=>-0.1267545258
Iteration 7530: Achieved Loss=> 22.8578821656
Gradients: w=>0.0073430332, b=>-0.1266871392
Iteration 7531: Achieved Loss=> 22.8578660664
Gradients: w=>0.0073391294, b=>-0.1266197885
Iteration 7532: Achieved Loss=> 22.8578499842
Gradients: w=>0.0073352277, b=>-0.1265524735
Iteration 7533: Achieved Loss=> 22.8578339191
Gradients: w=>0.0073313280, b=>-0.1264851943
Iteration 7534: Achieved Loss=> 22.8578178711
Gradients: w=>0.0073274305, b=>-0.1264179509
Iteration 7535: Achieved Loss=> 22.8578018402
Gradients: w=>0.0073235350, b=>-0.1263507432
Iteration 7536: Achieved Loss=> 22.8577858263
Gradients: w=>0.0073196416, b=>-0.1262835713
Iteration 7537: Achieved Loss=> 22.8577698295
Gradients: w=>0.0073157502, b=>-0.1262164351
Iteration 7538: Achieved Loss=> 22.8577538496
Gradients: w=>0.0073118609, b=>-0.1261493345
Iteration 7539: Achieved Loss=> 22.8577378867
Gradients: w=>0.0073079737, b=>-0.1260822697
Iteration 7540: Achieved Loss=> 22.8577219408
Gradients: w=>0.0073040886, b=>-0.1260152405
Iteration 7541: Achieved Loss=> 22.8577060119
Gradients: w=>0.0073002055, b=>-0.1259482469
Iteration 7542: Achieved Loss=> 22.8576900998
Gradients: w=>0.0072963245, b=>-0.1258812890
Iteration 7543: Achieved Loss=> 22.8576742047
Gradients: w=>0.0072924456, b=>-0.1258143666
Iteration 7544: Achieved Loss=> 22.8576583265
Gradients: w=>0.0072885687, b=>-0.1257474798
Iteration 7545: Achieved Loss=> 22.8576424652
Gradients: w=>0.0072846938, b=>-0.1256806286
Iteration 7546: Achieved Loss=> 22.8576266207
Gradients: w=>0.0072808211, b=>-0.1256138129
Iteration 7547: Achieved Loss=> 22.8576107931
Gradients: w=>0.0072769504, b=>-0.1255470328
Iteration 7548: Achieved Loss=> 22.8575949823
Gradients: w=>0.0072730817, b=>-0.1254802881
Iteration 7549: Achieved Loss=> 22.8575791883
Gradients: w=>0.0072692151, b=>-0.1254135789
Iteration 7550: Achieved Loss=> 22.8575634111
Gradients: w=>0.0072653506, b=>-0.1253469052
Iteration 7551: Achieved Loss=> 22.8575476506
Gradients: w=>0.0072614881, b=>-0.1252802670
Iteration 7552: Achieved Loss=> 22.8575319069
Gradients: w=>0.0072576277, b=>-0.1252136641
Iteration 7553: Achieved Loss=> 22.8575161800
Gradients: w=>0.0072537693, b=>-0.1251470967
Iteration 7554: Achieved Loss=> 22.8575004697
Gradients: w=>0.0072499130, b=>-0.1250805647
Iteration 7555: Achieved Loss=> 22.8574847762
Gradients: w=>0.0072460587, b=>-0.1250140680
Iteration 7556: Achieved Loss=> 22.8574690994
Gradients: w=>0.0072422065, b=>-0.1249476067
Iteration 7557: Achieved Loss=> 22.8574534392
Gradients: w=>0.0072383563, b=>-0.1248811807
Iteration 7558: Achieved Loss=> 22.8574377956
Gradients: w=>0.0072345082, b=>-0.1248147900
Iteration 7559: Achieved Loss=> 22.8574221687
Gradients: w=>0.0072306621, b=>-0.1247484347
Iteration 7560: Achieved Loss=> 22.8574065584
Gradients: w=>0.0072268180, b=>-0.1246821146
Iteration 7561: Achieved Loss=> 22.8573909647
Gradients: w=>0.0072229760, b=>-0.1246158297
Iteration 7562: Achieved Loss=> 22.8573753876
Gradients: w=>0.0072191361, b=>-0.1245495801
Iteration 7563: Achieved Loss=> 22.8573598270
Gradients: w=>0.0072152982, b=>-0.1244833657
Iteration 7564: Achieved Loss=> 22.8573442829
Gradients: w=>0.0072114623, b=>-0.1244171866
Iteration 7565: Achieved Loss=> 22.8573287554
Gradients: w=>0.0072076284, b=>-0.1243510426
Iteration 7566: Achieved Loss=> 22.8573132444
Gradients: w=>0.0072037967, b=>-0.1242849337
Iteration 7567: Achieved Loss=> 22.8572977499
Gradients: w=>0.0071999669, b=>-0.1242188601
Iteration 7568: Achieved Loss=> 22.8572822719
Gradients: w=>0.0071961392, b=>-0.1241528215
Iteration 7569: Achieved Loss=> 22.8572668103
Gradients: w=>0.0071923135, b=>-0.1240868180
Iteration 7570: Achieved Loss=> 22.8572513651
Gradients: w=>0.0071884898, b=>-0.1240208497
Iteration 7571: Achieved Loss=> 22.8572359364
Gradients: w=>0.0071846682, b=>-0.1239549164
Iteration 7572: Achieved Loss=> 22.8572205240
Gradients: w=>0.0071808486, b=>-0.1238890182
Iteration 7573: Achieved Loss=> 22.8572051281
Gradients: w=>0.0071770311, b=>-0.1238231549
Iteration 7574: Achieved Loss=> 22.8571897485
Gradients: w=>0.0071732155, b=>-0.1237573268
Iteration 7575: Achieved Loss=> 22.8571743852
Gradients: w=>0.0071694020, b=>-0.1236915336
Iteration 7576: Achieved Loss=> 22.8571590383
Gradients: w=>0.0071655905, b=>-0.1236257753
Iteration 7577: Achieved Loss=> 22.8571437077
Gradients: w=>0.0071617811, b=>-0.1235600521
Iteration 7578: Achieved Loss=> 22.8571283934
Gradients: w=>0.0071579737, b=>-0.1234943638
Iteration 7579: Achieved Loss=> 22.8571130954
Gradients: w=>0.0071541683, b=>-0.1234287104
Iteration 7580: Achieved Loss=> 22.8570978136
Gradients: w=>0.0071503649, b=>-0.1233630919
Iteration 7581: Achieved Loss=> 22.8570825481
Gradients: w=>0.0071465635, b=>-0.1232975083
Iteration 7582: Achieved Loss=> 22.8570672988
Gradients: w=>0.0071427642, b=>-0.1232319595
Iteration 7583: Achieved Loss=> 22.8570520657
Gradients: w=>0.0071389669, b=>-0.1231664457
Iteration 7584: Achieved Loss=> 22.8570368488
Gradients: w=>0.0071351716, b=>-0.1231009666
Iteration 7585: Achieved Loss=> 22.8570216481
Gradients: w=>0.0071313783, b=>-0.1230355223
Iteration 7586: Achieved Loss=> 22.8570064635
Gradients: w=>0.0071275871, b=>-0.1229701129
Iteration 7587: Achieved Loss=> 22.8569912951
Gradients: w=>0.0071237978, b=>-0.1229047382
Iteration 7588: Achieved Loss=> 22.8569761428
Gradients: w=>0.0071200106, b=>-0.1228393983
Iteration 7589: Achieved Loss=> 22.8569610066
Gradients: w=>0.0071162254, b=>-0.1227740931
Iteration 7590: Achieved Loss=> 22.8569458865
Gradients: w=>0.0071124422, b=>-0.1227088226
Iteration 7591: Achieved Loss=> 22.8569307825
Gradients: w=>0.0071086610, b=>-0.1226435868
Iteration 7592: Achieved Loss=> 22.8569156945
Gradients: w=>0.0071048818, b=>-0.1225783857
Iteration 7593: Achieved Loss=> 22.8569006226
Gradients: w=>0.0071011046, b=>-0.1225132193
Iteration 7594: Achieved Loss=> 22.8568855667
Gradients: w=>0.0070973295, b=>-0.1224480875
Iteration 7595: Achieved Loss=> 22.8568705268
Gradients: w=>0.0070935563, b=>-0.1223829903
Iteration 7596: Achieved Loss=> 22.8568555029
Gradients: w=>0.0070897852, b=>-0.1223179278
Iteration 7597: Achieved Loss=> 22.8568404949
Gradients: w=>0.0070860160, b=>-0.1222528998
Iteration 7598: Achieved Loss=> 22.8568255029
Gradients: w=>0.0070822489, b=>-0.1221879064
Iteration 7599: Achieved Loss=> 22.8568105269
Gradients: w=>0.0070784837, b=>-0.1221229476
Iteration 7600: Achieved Loss=> 22.8567955667
Gradients: w=>0.0070747206, b=>-0.1220580233
Iteration 7601: Achieved Loss=> 22.8567806225
Gradients: w=>0.0070709594, b=>-0.1219931335
Iteration 7602: Achieved Loss=> 22.8567656941
Gradients: w=>0.0070672003, b=>-0.1219282782
Iteration 7603: Achieved Loss=> 22.8567507816
Gradients: w=>0.0070634432, b=>-0.1218634574
Iteration 7604: Achieved Loss=> 22.8567358850
Gradients: w=>0.0070596880, b=>-0.1217986710
Iteration 7605: Achieved Loss=> 22.8567210042
Gradients: w=>0.0070559349, b=>-0.1217339191
Iteration 7606: Achieved Loss=> 22.8567061392
Gradients: w=>0.0070521837, b=>-0.1216692016
Iteration 7607: Achieved Loss=> 22.8566912900
Gradients: w=>0.0070484346, b=>-0.1216045185
Iteration 7608: Achieved Loss=> 22.8566764566
Gradients: w=>0.0070446874, b=>-0.1215398698
Iteration 7609: Achieved Loss=> 22.8566616390
Gradients: w=>0.0070409422, b=>-0.1214752555
Iteration 7610: Achieved Loss=> 22.8566468371
Gradients: w=>0.0070371991, b=>-0.1214106755
Iteration 7611: Achieved Loss=> 22.8566320510
Gradients: w=>0.0070334579, b=>-0.1213461299
Iteration 7612: Achieved Loss=> 22.8566172806
Gradients: w=>0.0070297187, b=>-0.1212816186
Iteration 7613: Achieved Loss=> 22.8566025258
Gradients: w=>0.0070259814, b=>-0.1212171415
Iteration 7614: Achieved Loss=> 22.8565877868
Gradients: w=>0.0070222462, b=>-0.1211526988
Iteration 7615: Achieved Loss=> 22.8565730634
Gradients: w=>0.0070185130, b=>-0.1210882903
Iteration 7616: Achieved Loss=> 22.8565583557
Gradients: w=>0.0070147817, b=>-0.1210239160
Iteration 7617: Achieved Loss=> 22.8565436636
Gradients: w=>0.0070110524, b=>-0.1209595760
Iteration 7618: Achieved Loss=> 22.8565289871
Gradients: w=>0.0070073252, b=>-0.1208952702
Iteration 7619: Achieved Loss=> 22.8565143263
Gradients: w=>0.0070035998, b=>-0.1208309985
Iteration 7620: Achieved Loss=> 22.8564996810
Gradients: w=>0.0069998765, b=>-0.1207667611
Iteration 7621: Achieved Loss=> 22.8564850513
Gradients: w=>0.0069961552, b=>-0.1207025578
Iteration 7622: Achieved Loss=> 22.8564704371
Gradients: w=>0.0069924358, b=>-0.1206383886
Iteration 7623: Achieved Loss=> 22.8564558385
Gradients: w=>0.0069887184, b=>-0.1205742535
Iteration 7624: Achieved Loss=> 22.8564412554
Gradients: w=>0.0069850030, b=>-0.1205101525
Iteration 7625: Achieved Loss=> 22.8564266877
Gradients: w=>0.0069812895, b=>-0.1204460856
Iteration 7626: Achieved Loss=> 22.8564121356
Gradients: w=>0.0069775781, b=>-0.1203820528
Iteration 7627: Achieved Loss=> 22.8563975990
Gradients: w=>0.0069738686, b=>-0.1203180540
Iteration 7628: Achieved Loss=> 22.8563830777
Gradients: w=>0.0069701611, b=>-0.1202540892
Iteration 7629: Achieved Loss=> 22.8563685720
Gradients: w=>0.0069664555, b=>-0.1201901585
Iteration 7630: Achieved Loss=> 22.8563540816
Gradients: w=>0.0069627519, b=>-0.1201262617
Iteration 7631: Achieved Loss=> 22.8563396067
Gradients: w=>0.0069590503, b=>-0.1200623989
Iteration 7632: Achieved Loss=> 22.8563251471
Gradients: w=>0.0069553507, b=>-0.1199985700
Iteration 7633: Achieved Loss=> 22.8563107029
Gradients: w=>0.0069516530, b=>-0.1199347751
Iteration 7634: Achieved Loss=> 22.8562962741
Gradients: w=>0.0069479573, b=>-0.1198710141
Iteration 7635: Achieved Loss=> 22.8562818606
Gradients: w=>0.0069442635, b=>-0.1198072870
Iteration 7636: Achieved Loss=> 22.8562674624
Gradients: w=>0.0069405717, b=>-0.1197435938
Iteration 7637: Achieved Loss=> 22.8562530795
Gradients: w=>0.0069368819, b=>-0.1196799344
Iteration 7638: Achieved Loss=> 22.8562387119
Gradients: w=>0.0069331941, b=>-0.1196163089
Iteration 7639: Achieved Loss=> 22.8562243596
Gradients: w=>0.0069295082, b=>-0.1195527172
Iteration 7640: Achieved Loss=> 22.8562100226
Gradients: w=>0.0069258242, b=>-0.1194891593
Iteration 7641: Achieved Loss=> 22.8561957007
Gradients: w=>0.0069221422, b=>-0.1194256352
Iteration 7642: Achieved Loss=> 22.8561813941
Gradients: w=>0.0069184622, b=>-0.1193621448
Iteration 7643: Achieved Loss=> 22.8561671028
Gradients: w=>0.0069147842, b=>-0.1192986882
Iteration 7644: Achieved Loss=> 22.8561528266
Gradients: w=>0.0069111080, b=>-0.1192352654
Iteration 7645: Achieved Loss=> 22.8561385655
Gradients: w=>0.0069074339, b=>-0.1191718763
Iteration 7646: Achieved Loss=> 22.8561243197
Gradients: w=>0.0069037617, b=>-0.1191085208
Iteration 7647: Achieved Loss=> 22.8561100890
Gradients: w=>0.0069000914, b=>-0.1190451991
Iteration 7648: Achieved Loss=> 22.8560958734
Gradients: w=>0.0068964231, b=>-0.1189819110
Iteration 7649: Achieved Loss=> 22.8560816729
Gradients: w=>0.0068927568, b=>-0.1189186566
Iteration 7650: Achieved Loss=> 22.8560674875
Gradients: w=>0.0068890924, b=>-0.1188554358
Iteration 7651: Achieved Loss=> 22.8560533172
Gradients: w=>0.0068854299, b=>-0.1187922486
Iteration 7652: Achieved Loss=> 22.8560391620
Gradients: w=>0.0068817694, b=>-0.1187290950
Iteration 7653: Achieved Loss=> 22.8560250218
Gradients: w=>0.0068781109, b=>-0.1186659749
Iteration 7654: Achieved Loss=> 22.8560108966
Gradients: w=>0.0068744542, b=>-0.1186028885
Iteration 7655: Achieved Loss=> 22.8559967864
Gradients: w=>0.0068707996, b=>-0.1185398355
Iteration 7656: Achieved Loss=> 22.8559826913
Gradients: w=>0.0068671468, b=>-0.1184768161
Iteration 7657: Achieved Loss=> 22.8559686111
Gradients: w=>0.0068634961, b=>-0.1184138302
Iteration 7658: Achieved Loss=> 22.8559545459
Gradients: w=>0.0068598472, b=>-0.1183508778
Iteration 7659: Achieved Loss=> 22.8559404957
Gradients: w=>0.0068562003, b=>-0.1182879588
Iteration 7660: Achieved Loss=> 22.8559264603
Gradients: w=>0.0068525553, b=>-0.1182250733
Iteration 7661: Achieved Loss=> 22.8559124399
Gradients: w=>0.0068489123, b=>-0.1181622212
Iteration 7662: Achieved Loss=> 22.8558984345
Gradients: w=>0.0068452712, b=>-0.1180994026
Iteration 7663: Achieved Loss=> 22.8558844438
Gradients: w=>0.0068416321, b=>-0.1180366173
Iteration 7664: Achieved Loss=> 22.8558704681
Gradients: w=>0.0068379948, b=>-0.1179738654
Iteration 7665: Achieved Loss=> 22.8558565072
Gradients: w=>0.0068343596, b=>-0.1179111469
Iteration 7666: Achieved Loss=> 22.8558425612
Gradients: w=>0.0068307262, b=>-0.1178484617
Iteration 7667: Achieved Loss=> 22.8558286300
Gradients: w=>0.0068270948, b=>-0.1177858098
Iteration 7668: Achieved Loss=> 22.8558147136
Gradients: w=>0.0068234653, b=>-0.1177231913
Iteration 7669: Achieved Loss=> 22.8558008120
Gradients: w=>0.0068198377, b=>-0.1176606060
Iteration 7670: Achieved Loss=> 22.8557869251
Gradients: w=>0.0068162121, b=>-0.1175980540
Iteration 7671: Achieved Loss=> 22.8557730531
Gradients: w=>0.0068125884, b=>-0.1175355353
Iteration 7672: Achieved Loss=> 22.8557591957
Gradients: w=>0.0068089666, b=>-0.1174730498
Iteration 7673: Achieved Loss=> 22.8557453531
Gradients: w=>0.0068053467, b=>-0.1174105975
Iteration 7674: Achieved Loss=> 22.8557315252
Gradients: w=>0.0068017288, b=>-0.1173481784
Iteration 7675: Achieved Loss=> 22.8557177121
Gradients: w=>0.0067981128, b=>-0.1172857926
Iteration 7676: Achieved Loss=> 22.8557039135
Gradients: w=>0.0067944987, b=>-0.1172234398
Iteration 7677: Achieved Loss=> 22.8556901297
Gradients: w=>0.0067908866, b=>-0.1171611202
Iteration 7678: Achieved Loss=> 22.8556763605
Gradients: w=>0.0067872763, b=>-0.1170988338
Iteration 7679: Achieved Loss=> 22.8556626060
Gradients: w=>0.0067836680, b=>-0.1170365805
Iteration 7680: Achieved Loss=> 22.8556488661
Gradients: w=>0.0067800616, b=>-0.1169743602
Iteration 7681: Achieved Loss=> 22.8556351407
Gradients: w=>0.0067764571, b=>-0.1169121731
Iteration 7682: Achieved Loss=> 22.8556214300
Gradients: w=>0.0067728545, b=>-0.1168500190
Iteration 7683: Achieved Loss=> 22.8556077338
Gradients: w=>0.0067692539, b=>-0.1167878979
Iteration 7684: Achieved Loss=> 22.8555940523
Gradients: w=>0.0067656551, b=>-0.1167258099
Iteration 7685: Achieved Loss=> 22.8555803852
Gradients: w=>0.0067620583, b=>-0.1166637549
Iteration 7686: Achieved Loss=> 22.8555667327
Gradients: w=>0.0067584634, b=>-0.1166017328
Iteration 7687: Achieved Loss=> 22.8555530947
Gradients: w=>0.0067548704, b=>-0.1165397438
Iteration 7688: Achieved Loss=> 22.8555394711
Gradients: w=>0.0067512793, b=>-0.1164777877
Iteration 7689: Achieved Loss=> 22.8555258621
Gradients: w=>0.0067476901, b=>-0.1164158645
Iteration 7690: Achieved Loss=> 22.8555122675
Gradients: w=>0.0067441028, b=>-0.1163539742
Iteration 7691: Achieved Loss=> 22.8554986874
Gradients: w=>0.0067405174, b=>-0.1162921169
Iteration 7692: Achieved Loss=> 22.8554851217
Gradients: w=>0.0067369340, b=>-0.1162302924
Iteration 7693: Achieved Loss=> 22.8554715705
Gradients: w=>0.0067333524, b=>-0.1161685008
Iteration 7694: Achieved Loss=> 22.8554580336
Gradients: w=>0.0067297727, b=>-0.1161067421
Iteration 7695: Achieved Loss=> 22.8554445111
Gradients: w=>0.0067261950, b=>-0.1160450162
Iteration 7696: Achieved Loss=> 22.8554310030
Gradients: w=>0.0067226191, b=>-0.1159833231
Iteration 7697: Achieved Loss=> 22.8554175093
Gradients: w=>0.0067190452, b=>-0.1159216628
Iteration 7698: Achieved Loss=> 22.8554040299
Gradients: w=>0.0067154731, b=>-0.1158600353
Iteration 7699: Achieved Loss=> 22.8553905648
Gradients: w=>0.0067119030, b=>-0.1157984405
Iteration 7700: Achieved Loss=> 22.8553771141
Gradients: w=>0.0067083347, b=>-0.1157368785
Iteration 7701: Achieved Loss=> 22.8553636776
Gradients: w=>0.0067047684, b=>-0.1156753493
Iteration 7702: Achieved Loss=> 22.8553502555
Gradients: w=>0.0067012039, b=>-0.1156138527
Iteration 7703: Achieved Loss=> 22.8553368476
Gradients: w=>0.0066976414, b=>-0.1155523888
Iteration 7704: Achieved Loss=> 22.8553234539
Gradients: w=>0.0066940807, b=>-0.1154909576
Iteration 7705: Achieved Loss=> 22.8553100745
Gradients: w=>0.0066905219, b=>-0.1154295591
Iteration 7706: Achieved Loss=> 22.8552967093
Gradients: w=>0.0066869650, b=>-0.1153681932
Iteration 7707: Achieved Loss=> 22.8552833583
Gradients: w=>0.0066834100, b=>-0.1153068599
Iteration 7708: Achieved Loss=> 22.8552700215
Gradients: w=>0.0066798569, b=>-0.1152455593
Iteration 7709: Achieved Loss=> 22.8552566989
Gradients: w=>0.0066763057, b=>-0.1151842912
Iteration 7710: Achieved Loss=> 22.8552433904
Gradients: w=>0.0066727564, b=>-0.1151230557
Iteration 7711: Achieved Loss=> 22.8552300961
Gradients: w=>0.0066692089, b=>-0.1150618527
Iteration 7712: Achieved Loss=> 22.8552168160
Gradients: w=>0.0066656634, b=>-0.1150006823
Iteration 7713: Achieved Loss=> 22.8552035499
Gradients: w=>0.0066621197, b=>-0.1149395444
Iteration 7714: Achieved Loss=> 22.8551902979
Gradients: w=>0.0066585779, b=>-0.1148784390
Iteration 7715: Achieved Loss=> 22.8551770601
Gradients: w=>0.0066550380, b=>-0.1148173661
Iteration 7716: Achieved Loss=> 22.8551638363
Gradients: w=>0.0066515000, b=>-0.1147563257
Iteration 7717: Achieved Loss=> 22.8551506265
Gradients: w=>0.0066479638, b=>-0.1146953177
Iteration 7718: Achieved Loss=> 22.8551374308
Gradients: w=>0.0066444296, b=>-0.1146343421
Iteration 7719: Achieved Loss=> 22.8551242491
Gradients: w=>0.0066408972, b=>-0.1145733990
Iteration 7720: Achieved Loss=> 22.8551110815
Gradients: w=>0.0066373667, b=>-0.1145124883
Iteration 7721: Achieved Loss=> 22.8550979278
Gradients: w=>0.0066338381, b=>-0.1144516099
Iteration 7722: Achieved Loss=> 22.8550847881
Gradients: w=>0.0066303113, b=>-0.1143907639
Iteration 7723: Achieved Loss=> 22.8550716624
Gradients: w=>0.0066267864, b=>-0.1143299503
Iteration 7724: Achieved Loss=> 22.8550585506
Gradients: w=>0.0066232634, b=>-0.1142691690
Iteration 7725: Achieved Loss=> 22.8550454528
Gradients: w=>0.0066197423, b=>-0.1142084200
Iteration 7726: Achieved Loss=> 22.8550323689
Gradients: w=>0.0066162230, b=>-0.1141477033
Iteration 7727: Achieved Loss=> 22.8550192989
Gradients: w=>0.0066127057, b=>-0.1140870189
Iteration 7728: Achieved Loss=> 22.8550062428
Gradients: w=>0.0066091901, b=>-0.1140263667
Iteration 7729: Achieved Loss=> 22.8549932006
Gradients: w=>0.0066056765, b=>-0.1139657468
Iteration 7730: Achieved Loss=> 22.8549801722
Gradients: w=>0.0066021647, b=>-0.1139051591
Iteration 7731: Achieved Loss=> 22.8549671577
Gradients: w=>0.0065986548, b=>-0.1138446036
Iteration 7732: Achieved Loss=> 22.8549541570
Gradients: w=>0.0065951467, b=>-0.1137840803
Iteration 7733: Achieved Loss=> 22.8549411702
Gradients: w=>0.0065916406, b=>-0.1137235892
Iteration 7734: Achieved Loss=> 22.8549281971
Gradients: w=>0.0065881362, b=>-0.1136631303
Iteration 7735: Achieved Loss=> 22.8549152378
Gradients: w=>0.0065846338, b=>-0.1136027035
Iteration 7736: Achieved Loss=> 22.8549022924
Gradients: w=>0.0065811332, b=>-0.1135423088
Iteration 7737: Achieved Loss=> 22.8548893606
Gradients: w=>0.0065776345, b=>-0.1134819462
Iteration 7738: Achieved Loss=> 22.8548764426
Gradients: w=>0.0065741376, b=>-0.1134216157
Iteration 7739: Achieved Loss=> 22.8548635384
Gradients: w=>0.0065706426, b=>-0.1133613173
Iteration 7740: Achieved Loss=> 22.8548506479
Gradients: w=>0.0065671494, b=>-0.1133010509
Iteration 7741: Achieved Loss=> 22.8548377710
Gradients: w=>0.0065636581, b=>-0.1132408166
Iteration 7742: Achieved Loss=> 22.8548249079
Gradients: w=>0.0065601687, b=>-0.1131806143
Iteration 7743: Achieved Loss=> 22.8548120584
Gradients: w=>0.0065566811, b=>-0.1131204440
Iteration 7744: Achieved Loss=> 22.8547992226
Gradients: w=>0.0065531954, b=>-0.1130603057
Iteration 7745: Achieved Loss=> 22.8547864004
Gradients: w=>0.0065497115, b=>-0.1130001994
Iteration 7746: Achieved Loss=> 22.8547735919
Gradients: w=>0.0065462295, b=>-0.1129401250
Iteration 7747: Achieved Loss=> 22.8547607970
Gradients: w=>0.0065427493, b=>-0.1128800826
Iteration 7748: Achieved Loss=> 22.8547480157
Gradients: w=>0.0065392710, b=>-0.1128200721
Iteration 7749: Achieved Loss=> 22.8547352479
Gradients: w=>0.0065357945, b=>-0.1127600935
Iteration 7750: Achieved Loss=> 22.8547224938
Gradients: w=>0.0065323198, b=>-0.1127001467
Iteration 7751: Achieved Loss=> 22.8547097531
Gradients: w=>0.0065288471, b=>-0.1126402319
Iteration 7752: Achieved Loss=> 22.8546970261
Gradients: w=>0.0065253761, b=>-0.1125803489
Iteration 7753: Achieved Loss=> 22.8546843125
Gradients: w=>0.0065219070, b=>-0.1125204977
Iteration 7754: Achieved Loss=> 22.8546716125
Gradients: w=>0.0065184398, b=>-0.1124606784
Iteration 7755: Achieved Loss=> 22.8546589260
Gradients: w=>0.0065149744, b=>-0.1124008908
Iteration 7756: Achieved Loss=> 22.8546462530
Gradients: w=>0.0065115108, b=>-0.1123411351
Iteration 7757: Achieved Loss=> 22.8546335934
Gradients: w=>0.0065080491, b=>-0.1122814111
Iteration 7758: Achieved Loss=> 22.8546209473
Gradients: w=>0.0065045892, b=>-0.1122217188
Iteration 7759: Achieved Loss=> 22.8546083146
Gradients: w=>0.0065011312, b=>-0.1121620583
Iteration 7760: Achieved Loss=> 22.8545956954
Gradients: w=>0.0064976750, b=>-0.1121024295
Iteration 7761: Achieved Loss=> 22.8545830896
Gradients: w=>0.0064942206, b=>-0.1120428324
Iteration 7762: Achieved Loss=> 22.8545704971
Gradients: w=>0.0064907681, b=>-0.1119832670
Iteration 7763: Achieved Loss=> 22.8545579181
Gradients: w=>0.0064873174, b=>-0.1119237333
Iteration 7764: Achieved Loss=> 22.8545453524
Gradients: w=>0.0064838686, b=>-0.1118642312
Iteration 7765: Achieved Loss=> 22.8545328001
Gradients: w=>0.0064804215, b=>-0.1118047607
Iteration 7766: Achieved Loss=> 22.8545202612
Gradients: w=>0.0064769763, b=>-0.1117453219
Iteration 7767: Achieved Loss=> 22.8545077355
Gradients: w=>0.0064735330, b=>-0.1116859146
Iteration 7768: Achieved Loss=> 22.8544952232
Gradients: w=>0.0064700915, b=>-0.1116265390
Iteration 7769: Achieved Loss=> 22.8544827242
Gradients: w=>0.0064666518, b=>-0.1115671949
Iteration 7770: Achieved Loss=> 22.8544702384
Gradients: w=>0.0064632139, b=>-0.1115078823
Iteration 7771: Achieved Loss=> 22.8544577660
Gradients: w=>0.0064597778, b=>-0.1114486013
Iteration 7772: Achieved Loss=> 22.8544453068
Gradients: w=>0.0064563436, b=>-0.1113893518
Iteration 7773: Achieved Loss=> 22.8544328608
Gradients: w=>0.0064529112, b=>-0.1113301338
Iteration 7774: Achieved Loss=> 22.8544204281
Gradients: w=>0.0064494807, b=>-0.1112709473
Iteration 7775: Achieved Loss=> 22.8544080086
Gradients: w=>0.0064460519, b=>-0.1112117923
Iteration 7776: Achieved Loss=> 22.8543956022
Gradients: w=>0.0064426250, b=>-0.1111526687
Iteration 7777: Achieved Loss=> 22.8543832091
Gradients: w=>0.0064391999, b=>-0.1110935765
Iteration 7778: Achieved Loss=> 22.8543708292
Gradients: w=>0.0064357766, b=>-0.1110345157
Iteration 7779: Achieved Loss=> 22.8543584624
Gradients: w=>0.0064323552, b=>-0.1109754864
Iteration 7780: Achieved Loss=> 22.8543461087
Gradients: w=>0.0064289355, b=>-0.1109164884
Iteration 7781: Achieved Loss=> 22.8543337682
Gradients: w=>0.0064255177, b=>-0.1108575218
Iteration 7782: Achieved Loss=> 22.8543214408
Gradients: w=>0.0064221017, b=>-0.1107985865
Iteration 7783: Achieved Loss=> 22.8543091265
Gradients: w=>0.0064186875, b=>-0.1107396826
Iteration 7784: Achieved Loss=> 22.8542968253
Gradients: w=>0.0064152752, b=>-0.1106808100
Iteration 7785: Achieved Loss=> 22.8542845372
Gradients: w=>0.0064118646, b=>-0.1106219687
Iteration 7786: Achieved Loss=> 22.8542722621
Gradients: w=>0.0064084559, b=>-0.1105631586
Iteration 7787: Achieved Loss=> 22.8542600001
Gradients: w=>0.0064050489, b=>-0.1105043799
Iteration 7788: Achieved Loss=> 22.8542477511
Gradients: w=>0.0064016438, b=>-0.1104456323
Iteration 7789: Achieved Loss=> 22.8542355151
Gradients: w=>0.0063982405, b=>-0.1103869161
Iteration 7790: Achieved Loss=> 22.8542232922
Gradients: w=>0.0063948390, b=>-0.1103282310
Iteration 7791: Achieved Loss=> 22.8542110822
Gradients: w=>0.0063914393, b=>-0.1102695771
Iteration 7792: Achieved Loss=> 22.8541988852
Gradients: w=>0.0063880414, b=>-0.1102109544
Iteration 7793: Achieved Loss=> 22.8541867012
Gradients: w=>0.0063846453, b=>-0.1101523629
Iteration 7794: Achieved Loss=> 22.8541745301
Gradients: w=>0.0063812511, b=>-0.1100938025
Iteration 7795: Achieved Loss=> 22.8541623720
Gradients: w=>0.0063778586, b=>-0.1100352733
Iteration 7796: Achieved Loss=> 22.8541502268
Gradients: w=>0.0063744679, b=>-0.1099767751
Iteration 7797: Achieved Loss=> 22.8541380945
Gradients: w=>0.0063710791, b=>-0.1099183081
Iteration 7798: Achieved Loss=> 22.8541259751
Gradients: w=>0.0063676920, b=>-0.1098598722
Iteration 7799: Achieved Loss=> 22.8541138686
Gradients: w=>0.0063643067, b=>-0.1098014673
Iteration 7800: Achieved Loss=> 22.8541017749
Gradients: w=>0.0063609233, b=>-0.1097430935
Iteration 7801: Achieved Loss=> 22.8540896941
Gradients: w=>0.0063575416, b=>-0.1096847507
Iteration 7802: Achieved Loss=> 22.8540776262
Gradients: w=>0.0063541618, b=>-0.1096264389
Iteration 7803: Achieved Loss=> 22.8540655710
Gradients: w=>0.0063507837, b=>-0.1095681581
Iteration 7804: Achieved Loss=> 22.8540535287
Gradients: w=>0.0063474074, b=>-0.1095099083
Iteration 7805: Achieved Loss=> 22.8540414992
Gradients: w=>0.0063440329, b=>-0.1094516895
Iteration 7806: Achieved Loss=> 22.8540294825
Gradients: w=>0.0063406603, b=>-0.1093935016
Iteration 7807: Achieved Loss=> 22.8540174785
Gradients: w=>0.0063372894, b=>-0.1093353446
Iteration 7808: Achieved Loss=> 22.8540054873
Gradients: w=>0.0063339203, b=>-0.1092772186
Iteration 7809: Achieved Loss=> 22.8539935089
Gradients: w=>0.0063305530, b=>-0.1092191235
Iteration 7810: Achieved Loss=> 22.8539815432
Gradients: w=>0.0063271875, b=>-0.1091610593
Iteration 7811: Achieved Loss=> 22.8539695902
Gradients: w=>0.0063238237, b=>-0.1091030259
Iteration 7812: Achieved Loss=> 22.8539576499
Gradients: w=>0.0063204618, b=>-0.1090450234
Iteration 7813: Achieved Loss=> 22.8539457223
Gradients: w=>0.0063171016, b=>-0.1089870517
Iteration 7814: Achieved Loss=> 22.8539338074
Gradients: w=>0.0063137433, b=>-0.1089291108
Iteration 7815: Achieved Loss=> 22.8539219052
Gradients: w=>0.0063103867, b=>-0.1088712008
Iteration 7816: Achieved Loss=> 22.8539100156
Gradients: w=>0.0063070319, b=>-0.1088133215
Iteration 7817: Achieved Loss=> 22.8538981386
Gradients: w=>0.0063036789, b=>-0.1087554730
Iteration 7818: Achieved Loss=> 22.8538862743
Gradients: w=>0.0063003277, b=>-0.1086976553
Iteration 7819: Achieved Loss=> 22.8538744225
Gradients: w=>0.0062969782, b=>-0.1086398682
Iteration 7820: Achieved Loss=> 22.8538625834
Gradients: w=>0.0062936305, b=>-0.1085821120
Iteration 7821: Achieved Loss=> 22.8538507569
Gradients: w=>0.0062902847, b=>-0.1085243864
Iteration 7822: Achieved Loss=> 22.8538389429
Gradients: w=>0.0062869405, b=>-0.1084666915
Iteration 7823: Achieved Loss=> 22.8538271415
Gradients: w=>0.0062835982, b=>-0.1084090273
Iteration 7824: Achieved Loss=> 22.8538153526
Gradients: w=>0.0062802577, b=>-0.1083513937
Iteration 7825: Achieved Loss=> 22.8538035763
Gradients: w=>0.0062769189, b=>-0.1082937908
Iteration 7826: Achieved Loss=> 22.8537918125
Gradients: w=>0.0062735819, b=>-0.1082362185
Iteration 7827: Achieved Loss=> 22.8537800612
Gradients: w=>0.0062702466, b=>-0.1081786768
Iteration 7828: Achieved Loss=> 22.8537683223
Gradients: w=>0.0062669132, b=>-0.1081211657
Iteration 7829: Achieved Loss=> 22.8537565960
Gradients: w=>0.0062635815, b=>-0.1080636851
Iteration 7830: Achieved Loss=> 22.8537448821
Gradients: w=>0.0062602516, b=>-0.1080062352
Iteration 7831: Achieved Loss=> 22.8537331807
Gradients: w=>0.0062569235, b=>-0.1079488157
Iteration 7832: Achieved Loss=> 22.8537214917
Gradients: w=>0.0062535971, b=>-0.1078914268
Iteration 7833: Achieved Loss=> 22.8537098151
Gradients: w=>0.0062502725, b=>-0.1078340684
Iteration 7834: Achieved Loss=> 22.8536981510
Gradients: w=>0.0062469496, b=>-0.1077767405
Iteration 7835: Achieved Loss=> 22.8536864992
Gradients: w=>0.0062436286, b=>-0.1077194431
Iteration 7836: Achieved Loss=> 22.8536748599
Gradients: w=>0.0062403093, b=>-0.1076621761
Iteration 7837: Achieved Loss=> 22.8536632329
Gradients: w=>0.0062369917, b=>-0.1076049396
Iteration 7838: Achieved Loss=> 22.8536516182
Gradients: w=>0.0062336760, b=>-0.1075477335
Iteration 7839: Achieved Loss=> 22.8536400160
Gradients: w=>0.0062303619, b=>-0.1074905579
Iteration 7840: Achieved Loss=> 22.8536284260
Gradients: w=>0.0062270497, b=>-0.1074334126
Iteration 7841: Achieved Loss=> 22.8536168484
Gradients: w=>0.0062237392, b=>-0.1073762977
Iteration 7842: Achieved Loss=> 22.8536052830
Gradients: w=>0.0062204305, b=>-0.1073192131
Iteration 7843: Achieved Loss=> 22.8535937300
Gradients: w=>0.0062171235, b=>-0.1072621590
Iteration 7844: Achieved Loss=> 22.8535821892
Gradients: w=>0.0062138183, b=>-0.1072051351
Iteration 7845: Achieved Loss=> 22.8535706608
Gradients: w=>0.0062105148, b=>-0.1071481416
Iteration 7846: Achieved Loss=> 22.8535591445
Gradients: w=>0.0062072131, b=>-0.1070911783
Iteration 7847: Achieved Loss=> 22.8535476405
Gradients: w=>0.0062039132, b=>-0.1070342454
Iteration 7848: Achieved Loss=> 22.8535361488
Gradients: w=>0.0062006150, b=>-0.1069773427
Iteration 7849: Achieved Loss=> 22.8535246692
Gradients: w=>0.0061973185, b=>-0.1069204702
Iteration 7850: Achieved Loss=> 22.8535132019
Gradients: w=>0.0061940239, b=>-0.1068636280
Iteration 7851: Achieved Loss=> 22.8535017467
Gradients: w=>0.0061907309, b=>-0.1068068160
Iteration 7852: Achieved Loss=> 22.8534903037
Gradients: w=>0.0061874397, b=>-0.1067500343
Iteration 7853: Achieved Loss=> 22.8534788729
Gradients: w=>0.0061841503, b=>-0.1066932827
Iteration 7854: Achieved Loss=> 22.8534674543
Gradients: w=>0.0061808626, b=>-0.1066365612
Iteration 7855: Achieved Loss=> 22.8534560477
Gradients: w=>0.0061775767, b=>-0.1065798700
Iteration 7856: Achieved Loss=> 22.8534446533
Gradients: w=>0.0061742925, b=>-0.1065232088
Iteration 7857: Achieved Loss=> 22.8534332711
Gradients: w=>0.0061710101, b=>-0.1064665778
Iteration 7858: Achieved Loss=> 22.8534219009
Gradients: w=>0.0061677294, b=>-0.1064099769
Iteration 7859: Achieved Loss=> 22.8534105428
Gradients: w=>0.0061644504, b=>-0.1063534061
Iteration 7860: Achieved Loss=> 22.8533991967
Gradients: w=>0.0061611732, b=>-0.1062968654
Iteration 7861: Achieved Loss=> 22.8533878628
Gradients: w=>0.0061578977, b=>-0.1062403547
Iteration 7862: Achieved Loss=> 22.8533765408
Gradients: w=>0.0061546240, b=>-0.1061838741
Iteration 7863: Achieved Loss=> 22.8533652309
Gradients: w=>0.0061513520, b=>-0.1061274235
Iteration 7864: Achieved Loss=> 22.8533539331
Gradients: w=>0.0061480818, b=>-0.1060710029
Iteration 7865: Achieved Loss=> 22.8533426472
Gradients: w=>0.0061448133, b=>-0.1060146123
Iteration 7866: Achieved Loss=> 22.8533313734
Gradients: w=>0.0061415465, b=>-0.1059582516
Iteration 7867: Achieved Loss=> 22.8533201115
Gradients: w=>0.0061382815, b=>-0.1059019210
Iteration 7868: Achieved Loss=> 22.8533088616
Gradients: w=>0.0061350182, b=>-0.1058456203
Iteration 7869: Achieved Loss=> 22.8532976236
Gradients: w=>0.0061317566, b=>-0.1057893495
Iteration 7870: Achieved Loss=> 22.8532863976
Gradients: w=>0.0061284968, b=>-0.1057331086
Iteration 7871: Achieved Loss=> 22.8532751836
Gradients: w=>0.0061252387, b=>-0.1056768977
Iteration 7872: Achieved Loss=> 22.8532639814
Gradients: w=>0.0061219823, b=>-0.1056207166
Iteration 7873: Achieved Loss=> 22.8532527912
Gradients: w=>0.0061187277, b=>-0.1055645654
Iteration 7874: Achieved Loss=> 22.8532416128
Gradients: w=>0.0061154748, b=>-0.1055084440
Iteration 7875: Achieved Loss=> 22.8532304464
Gradients: w=>0.0061122236, b=>-0.1054523525
Iteration 7876: Achieved Loss=> 22.8532192918
Gradients: w=>0.0061089741, b=>-0.1053962908
Iteration 7877: Achieved Loss=> 22.8532081491
Gradients: w=>0.0061057264, b=>-0.1053402589
Iteration 7878: Achieved Loss=> 22.8531970182
Gradients: w=>0.0061024804, b=>-0.1052842568
Iteration 7879: Achieved Loss=> 22.8531858991
Gradients: w=>0.0060992362, b=>-0.1052282844
Iteration 7880: Achieved Loss=> 22.8531747919
Gradients: w=>0.0060959936, b=>-0.1051723418
Iteration 7881: Achieved Loss=> 22.8531636964
Gradients: w=>0.0060927528, b=>-0.1051164290
Iteration 7882: Achieved Loss=> 22.8531526128
Gradients: w=>0.0060895137, b=>-0.1050605459
Iteration 7883: Achieved Loss=> 22.8531415409
Gradients: w=>0.0060862763, b=>-0.1050046924
Iteration 7884: Achieved Loss=> 22.8531304809
Gradients: w=>0.0060830407, b=>-0.1049488687
Iteration 7885: Achieved Loss=> 22.8531194325
Gradients: w=>0.0060798068, b=>-0.1048930747
Iteration 7886: Achieved Loss=> 22.8531083959
Gradients: w=>0.0060765745, b=>-0.1048373103
Iteration 7887: Achieved Loss=> 22.8530973711
Gradients: w=>0.0060733441, b=>-0.1047815756
Iteration 7888: Achieved Loss=> 22.8530863579
Gradients: w=>0.0060701153, b=>-0.1047258705
Iteration 7889: Achieved Loss=> 22.8530753565
Gradients: w=>0.0060668882, b=>-0.1046701950
Iteration 7890: Achieved Loss=> 22.8530643668
Gradients: w=>0.0060636629, b=>-0.1046145491
Iteration 7891: Achieved Loss=> 22.8530533887
Gradients: w=>0.0060604392, b=>-0.1045589328
Iteration 7892: Achieved Loss=> 22.8530424223
Gradients: w=>0.0060572173, b=>-0.1045033461
Iteration 7893: Achieved Loss=> 22.8530314676
Gradients: w=>0.0060539971, b=>-0.1044477889
Iteration 7894: Achieved Loss=> 22.8530205245
Gradients: w=>0.0060507786, b=>-0.1043922612
Iteration 7895: Achieved Loss=> 22.8530095931
Gradients: w=>0.0060475619, b=>-0.1043367631
Iteration 7896: Achieved Loss=> 22.8529986733
Gradients: w=>0.0060443468, b=>-0.1042812945
Iteration 7897: Achieved Loss=> 22.8529877650
Gradients: w=>0.0060411334, b=>-0.1042258553
Iteration 7898: Achieved Loss=> 22.8529768684
Gradients: w=>0.0060379218, b=>-0.1041704457
Iteration 7899: Achieved Loss=> 22.8529659834
Gradients: w=>0.0060347118, b=>-0.1041150655
Iteration 7900: Achieved Loss=> 22.8529551099
Gradients: w=>0.0060315036, b=>-0.1040597147
Iteration 7901: Achieved Loss=> 22.8529442480
Gradients: w=>0.0060282971, b=>-0.1040043934
Iteration 7902: Achieved Loss=> 22.8529333976
Gradients: w=>0.0060250922, b=>-0.1039491014
Iteration 7903: Achieved Loss=> 22.8529225588
Gradients: w=>0.0060218891, b=>-0.1038938389
Iteration 7904: Achieved Loss=> 22.8529117315
Gradients: w=>0.0060186877, b=>-0.1038386057
Iteration 7905: Achieved Loss=> 22.8529009156
Gradients: w=>0.0060154880, b=>-0.1037834019
Iteration 7906: Achieved Loss=> 22.8528901113
Gradients: w=>0.0060122899, b=>-0.1037282275
Iteration 7907: Achieved Loss=> 22.8528793185
Gradients: w=>0.0060090936, b=>-0.1036730824
Iteration 7908: Achieved Loss=> 22.8528685372
Gradients: w=>0.0060058990, b=>-0.1036179666
Iteration 7909: Achieved Loss=> 22.8528577673
Gradients: w=>0.0060027061, b=>-0.1035628801
Iteration 7910: Achieved Loss=> 22.8528470088
Gradients: w=>0.0059995149, b=>-0.1035078229
Iteration 7911: Achieved Loss=> 22.8528362618
Gradients: w=>0.0059963253, b=>-0.1034527950
Iteration 7912: Achieved Loss=> 22.8528255262
Gradients: w=>0.0059931375, b=>-0.1033977963
Iteration 7913: Achieved Loss=> 22.8528148021
Gradients: w=>0.0059899514, b=>-0.1033428268
Iteration 7914: Achieved Loss=> 22.8528040893
Gradients: w=>0.0059867669, b=>-0.1032878866
Iteration 7915: Achieved Loss=> 22.8527933879
Gradients: w=>0.0059835842, b=>-0.1032329756
Iteration 7916: Achieved Loss=> 22.8527826979
Gradients: w=>0.0059804031, b=>-0.1031780938
Iteration 7917: Achieved Loss=> 22.8527720193
Gradients: w=>0.0059772238, b=>-0.1031232412
Iteration 7918: Achieved Loss=> 22.8527613520
Gradients: w=>0.0059740461, b=>-0.1030684177
Iteration 7919: Achieved Loss=> 22.8527506960
Gradients: w=>0.0059708701, b=>-0.1030136233
Iteration 7920: Achieved Loss=> 22.8527400514
Gradients: w=>0.0059676958, b=>-0.1029588581
Iteration 7921: Achieved Loss=> 22.8527294181
Gradients: w=>0.0059645232, b=>-0.1029041221
Iteration 7922: Achieved Loss=> 22.8527187961
Gradients: w=>0.0059613523, b=>-0.1028494151
Iteration 7923: Achieved Loss=> 22.8527081854
Gradients: w=>0.0059581830, b=>-0.1027947372
Iteration 7924: Achieved Loss=> 22.8526975859
Gradients: w=>0.0059550155, b=>-0.1027400883
Iteration 7925: Achieved Loss=> 22.8526869977
Gradients: w=>0.0059518496, b=>-0.1026854685
Iteration 7926: Achieved Loss=> 22.8526764208
Gradients: w=>0.0059486854, b=>-0.1026308778
Iteration 7927: Achieved Loss=> 22.8526658551
Gradients: w=>0.0059455229, b=>-0.1025763161
Iteration 7928: Achieved Loss=> 22.8526553007
Gradients: w=>0.0059423621, b=>-0.1025217833
Iteration 7929: Achieved Loss=> 22.8526447575
Gradients: w=>0.0059392030, b=>-0.1024672796
Iteration 7930: Achieved Loss=> 22.8526342255
Gradients: w=>0.0059360455, b=>-0.1024128049
Iteration 7931: Achieved Loss=> 22.8526237046
Gradients: w=>0.0059328897, b=>-0.1023583591
Iteration 7932: Achieved Loss=> 22.8526131950
Gradients: w=>0.0059297356, b=>-0.1023039422
Iteration 7933: Achieved Loss=> 22.8526026965
Gradients: w=>0.0059265832, b=>-0.1022495543
Iteration 7934: Achieved Loss=> 22.8525922092
Gradients: w=>0.0059234324, b=>-0.1021951953
Iteration 7935: Achieved Loss=> 22.8525817331
Gradients: w=>0.0059202833, b=>-0.1021408652
Iteration 7936: Achieved Loss=> 22.8525712680
Gradients: w=>0.0059171359, b=>-0.1020865640
Iteration 7937: Achieved Loss=> 22.8525608141
Gradients: w=>0.0059139902, b=>-0.1020322916
Iteration 7938: Achieved Loss=> 22.8525503714
Gradients: w=>0.0059108461, b=>-0.1019780481
Iteration 7939: Achieved Loss=> 22.8525399397
Gradients: w=>0.0059077038, b=>-0.1019238335
Iteration 7940: Achieved Loss=> 22.8525295191
Gradients: w=>0.0059045630, b=>-0.1018696476
Iteration 7941: Achieved Loss=> 22.8525191096
Gradients: w=>0.0059014240, b=>-0.1018154906
Iteration 7942: Achieved Loss=> 22.8525087111
Gradients: w=>0.0058982866, b=>-0.1017613624
Iteration 7943: Achieved Loss=> 22.8524983237
Gradients: w=>0.0058951509, b=>-0.1017072629
Iteration 7944: Achieved Loss=> 22.8524879473
Gradients: w=>0.0058920169, b=>-0.1016531922
Iteration 7945: Achieved Loss=> 22.8524775820
Gradients: w=>0.0058888845, b=>-0.1015991502
Iteration 7946: Achieved Loss=> 22.8524672277
Gradients: w=>0.0058857538, b=>-0.1015451370
Iteration 7947: Achieved Loss=> 22.8524568844
Gradients: w=>0.0058826247, b=>-0.1014911525
Iteration 7948: Achieved Loss=> 22.8524465521
Gradients: w=>0.0058794973, b=>-0.1014371967
Iteration 7949: Achieved Loss=> 22.8524362307
Gradients: w=>0.0058763716, b=>-0.1013832696
Iteration 7950: Achieved Loss=> 22.8524259204
Gradients: w=>0.0058732476, b=>-0.1013293711
Iteration 7951: Achieved Loss=> 22.8524156210
Gradients: w=>0.0058701252, b=>-0.1012755013
Iteration 7952: Achieved Loss=> 22.8524053325
Gradients: w=>0.0058670044, b=>-0.1012216601
Iteration 7953: Achieved Loss=> 22.8523950550
Gradients: w=>0.0058638853, b=>-0.1011678476
Iteration 7954: Achieved Loss=> 22.8523847884
Gradients: w=>0.0058607679, b=>-0.1011140637
Iteration 7955: Achieved Loss=> 22.8523745328
Gradients: w=>0.0058576522, b=>-0.1010603083
Iteration 7956: Achieved Loss=> 22.8523642880
Gradients: w=>0.0058545381, b=>-0.1010065816
Iteration 7957: Achieved Loss=> 22.8523540541
Gradients: w=>0.0058514256, b=>-0.1009528834
Iteration 7958: Achieved Loss=> 22.8523438311
Gradients: w=>0.0058483148, b=>-0.1008992137
Iteration 7959: Achieved Loss=> 22.8523336190
Gradients: w=>0.0058452057, b=>-0.1008455726
Iteration 7960: Achieved Loss=> 22.8523234177
Gradients: w=>0.0058420982, b=>-0.1007919600
Iteration 7961: Achieved Loss=> 22.8523132272
Gradients: w=>0.0058389923, b=>-0.1007383759
Iteration 7962: Achieved Loss=> 22.8523030476
Gradients: w=>0.0058358881, b=>-0.1006848203
Iteration 7963: Achieved Loss=> 22.8522928788
Gradients: w=>0.0058327856, b=>-0.1006312931
Iteration 7964: Achieved Loss=> 22.8522827209
Gradients: w=>0.0058296847, b=>-0.1005777945
Iteration 7965: Achieved Loss=> 22.8522725737
Gradients: w=>0.0058265855, b=>-0.1005243242
Iteration 7966: Achieved Loss=> 22.8522624373
Gradients: w=>0.0058234879, b=>-0.1004708824
Iteration 7967: Achieved Loss=> 22.8522523117
Gradients: w=>0.0058203919, b=>-0.1004174690
Iteration 7968: Achieved Loss=> 22.8522421968
Gradients: w=>0.0058172976, b=>-0.1003640840
Iteration 7969: Achieved Loss=> 22.8522320927
Gradients: w=>0.0058142050, b=>-0.1003107274
Iteration 7970: Achieved Loss=> 22.8522219993
Gradients: w=>0.0058111140, b=>-0.1002573991
Iteration 7971: Achieved Loss=> 22.8522119167
Gradients: w=>0.0058080246, b=>-0.1002040992
Iteration 7972: Achieved Loss=> 22.8522018448
Gradients: w=>0.0058049369, b=>-0.1001508276
Iteration 7973: Achieved Loss=> 22.8521917836
Gradients: w=>0.0058018508, b=>-0.1000975844
Iteration 7974: Achieved Loss=> 22.8521817331
Gradients: w=>0.0057987664, b=>-0.1000443694
Iteration 7975: Achieved Loss=> 22.8521716932
Gradients: w=>0.0057956836, b=>-0.0999911827
Iteration 7976: Achieved Loss=> 22.8521616641
Gradients: w=>0.0057926024, b=>-0.0999380244
Iteration 7977: Achieved Loss=> 22.8521516456
Gradients: w=>0.0057895229, b=>-0.0998848942
Iteration 7978: Achieved Loss=> 22.8521416377
Gradients: w=>0.0057864450, b=>-0.0998317924
Iteration 7979: Achieved Loss=> 22.8521316405
Gradients: w=>0.0057833687, b=>-0.0997787187
Iteration 7980: Achieved Loss=> 22.8521216539
Gradients: w=>0.0057802941, b=>-0.0997256733
Iteration 7981: Achieved Loss=> 22.8521116780
Gradients: w=>0.0057772211, b=>-0.0996726561
Iteration 7982: Achieved Loss=> 22.8521017126
Gradients: w=>0.0057741498, b=>-0.0996196670
Iteration 7983: Achieved Loss=> 22.8520917578
Gradients: w=>0.0057710801, b=>-0.0995667061
Iteration 7984: Achieved Loss=> 22.8520818136
Gradients: w=>0.0057680120, b=>-0.0995137734
Iteration 7985: Achieved Loss=> 22.8520718800
Gradients: w=>0.0057649455, b=>-0.0994608689
Iteration 7986: Achieved Loss=> 22.8520619570
Gradients: w=>0.0057618807, b=>-0.0994079924
Iteration 7987: Achieved Loss=> 22.8520520444
Gradients: w=>0.0057588175, b=>-0.0993551441
Iteration 7988: Achieved Loss=> 22.8520421425
Gradients: w=>0.0057557560, b=>-0.0993023238
Iteration 7989: Achieved Loss=> 22.8520322510
Gradients: w=>0.0057526960, b=>-0.0992495317
Iteration 7990: Achieved Loss=> 22.8520223701
Gradients: w=>0.0057496377, b=>-0.0991967676
Iteration 7991: Achieved Loss=> 22.8520124996
Gradients: w=>0.0057465810, b=>-0.0991440315
Iteration 7992: Achieved Loss=> 22.8520026397
Gradients: w=>0.0057435260, b=>-0.0990913235
Iteration 7993: Achieved Loss=> 22.8519927902
Gradients: w=>0.0057404725, b=>-0.0990386435
Iteration 7994: Achieved Loss=> 22.8519829513
Gradients: w=>0.0057374207, b=>-0.0989859915
Iteration 7995: Achieved Loss=> 22.8519731227
Gradients: w=>0.0057343705, b=>-0.0989333675
Iteration 7996: Achieved Loss=> 22.8519633046
Gradients: w=>0.0057313220, b=>-0.0988807715
Iteration 7997: Achieved Loss=> 22.8519534970
Gradients: w=>0.0057282750, b=>-0.0988282035
Iteration 7998: Achieved Loss=> 22.8519436998
Gradients: w=>0.0057252297, b=>-0.0987756634
Iteration 7999: Achieved Loss=> 22.8519339130
Gradients: w=>0.0057221860, b=>-0.0987231512
Iteration 8000: Achieved Loss=> 22.8519241366
Gradients: w=>0.0057191439, b=>-0.0986706669
Iteration 8001: Achieved Loss=> 22.8519143705
Gradients: w=>0.0057161034, b=>-0.0986182106
Iteration 8002: Achieved Loss=> 22.8519046149
Gradients: w=>0.0057130646, b=>-0.0985657821
Iteration 8003: Achieved Loss=> 22.8518948697
Gradients: w=>0.0057100273, b=>-0.0985133815
Iteration 8004: Achieved Loss=> 22.8518851347
Gradients: w=>0.0057069917, b=>-0.0984610088
Iteration 8005: Achieved Loss=> 22.8518754102
Gradients: w=>0.0057039577, b=>-0.0984086639
Iteration 8006: Achieved Loss=> 22.8518656960
Gradients: w=>0.0057009253, b=>-0.0983563468
Iteration 8007: Achieved Loss=> 22.8518559921
Gradients: w=>0.0056978945, b=>-0.0983040576
Iteration 8008: Achieved Loss=> 22.8518462985
Gradients: w=>0.0056948653, b=>-0.0982517961
Iteration 8009: Achieved Loss=> 22.8518366152
Gradients: w=>0.0056918378, b=>-0.0981995624
Iteration 8010: Achieved Loss=> 22.8518269423
Gradients: w=>0.0056888118, b=>-0.0981473565
Iteration 8011: Achieved Loss=> 22.8518172796
Gradients: w=>0.0056857875, b=>-0.0980951784
Iteration 8012: Achieved Loss=> 22.8518076271
Gradients: w=>0.0056827647, b=>-0.0980430280
Iteration 8013: Achieved Loss=> 22.8517979850
Gradients: w=>0.0056797436, b=>-0.0979909053
Iteration 8014: Achieved Loss=> 22.8517883531
Gradients: w=>0.0056767241, b=>-0.0979388103
Iteration 8015: Achieved Loss=> 22.8517787314
Gradients: w=>0.0056737061, b=>-0.0978867430
Iteration 8016: Achieved Loss=> 22.8517691199
Gradients: w=>0.0056706898, b=>-0.0978347034
Iteration 8017: Achieved Loss=> 22.8517595187
Gradients: w=>0.0056676751, b=>-0.0977826915
Iteration 8018: Achieved Loss=> 22.8517499277
Gradients: w=>0.0056646620, b=>-0.0977307072
Iteration 8019: Achieved Loss=> 22.8517403468
Gradients: w=>0.0056616505, b=>-0.0976787506
Iteration 8020: Achieved Loss=> 22.8517307762
Gradients: w=>0.0056586406, b=>-0.0976268216
Iteration 8021: Achieved Loss=> 22.8517212157
Gradients: w=>0.0056556323, b=>-0.0975749202
Iteration 8022: Achieved Loss=> 22.8517116654
Gradients: w=>0.0056526256, b=>-0.0975230463
Iteration 8023: Achieved Loss=> 22.8517021252
Gradients: w=>0.0056496205, b=>-0.0974712001
Iteration 8024: Achieved Loss=> 22.8516925952
Gradients: w=>0.0056466169, b=>-0.0974193814
Iteration 8025: Achieved Loss=> 22.8516830753
Gradients: w=>0.0056436150, b=>-0.0973675903
Iteration 8026: Achieved Loss=> 22.8516735656
Gradients: w=>0.0056406147, b=>-0.0973158267
Iteration 8027: Achieved Loss=> 22.8516640659
Gradients: w=>0.0056376160, b=>-0.0972640906
Iteration 8028: Achieved Loss=> 22.8516545763
Gradients: w=>0.0056346189, b=>-0.0972123820
Iteration 8029: Achieved Loss=> 22.8516450969
Gradients: w=>0.0056316233, b=>-0.0971607009
Iteration 8030: Achieved Loss=> 22.8516356275
Gradients: w=>0.0056286294, b=>-0.0971090473
Iteration 8031: Achieved Loss=> 22.8516261681
Gradients: w=>0.0056256370, b=>-0.0970574212
Iteration 8032: Achieved Loss=> 22.8516167189
Gradients: w=>0.0056226463, b=>-0.0970058225
Iteration 8033: Achieved Loss=> 22.8516072796
Gradients: w=>0.0056196571, b=>-0.0969542512
Iteration 8034: Achieved Loss=> 22.8515978504
Gradients: w=>0.0056166695, b=>-0.0969027073
Iteration 8035: Achieved Loss=> 22.8515884312
Gradients: w=>0.0056136835, b=>-0.0968511909
Iteration 8036: Achieved Loss=> 22.8515790221
Gradients: w=>0.0056106991, b=>-0.0967997018
Iteration 8037: Achieved Loss=> 22.8515696229
Gradients: w=>0.0056077163, b=>-0.0967482401
Iteration 8038: Achieved Loss=> 22.8515602337
Gradients: w=>0.0056047351, b=>-0.0966968058
Iteration 8039: Achieved Loss=> 22.8515508545
Gradients: w=>0.0056017554, b=>-0.0966453988
Iteration 8040: Achieved Loss=> 22.8515414853
Gradients: w=>0.0055987773, b=>-0.0965940191
Iteration 8041: Achieved Loss=> 22.8515321261
Gradients: w=>0.0055958009, b=>-0.0965426668
Iteration 8042: Achieved Loss=> 22.8515227768
Gradients: w=>0.0055928260, b=>-0.0964913417
Iteration 8043: Achieved Loss=> 22.8515134374
Gradients: w=>0.0055898526, b=>-0.0964400440
Iteration 8044: Achieved Loss=> 22.8515041079
Gradients: w=>0.0055868809, b=>-0.0963887735
Iteration 8045: Achieved Loss=> 22.8514947884
Gradients: w=>0.0055839108, b=>-0.0963375303
Iteration 8046: Achieved Loss=> 22.8514854788
Gradients: w=>0.0055809422, b=>-0.0962863143
Iteration 8047: Achieved Loss=> 22.8514761790
Gradients: w=>0.0055779752, b=>-0.0962351255
Iteration 8048: Achieved Loss=> 22.8514668892
Gradients: w=>0.0055750098, b=>-0.0961839640
Iteration 8049: Achieved Loss=> 22.8514576092
Gradients: w=>0.0055720459, b=>-0.0961328296
Iteration 8050: Achieved Loss=> 22.8514483391
Gradients: w=>0.0055690836, b=>-0.0960817225
Iteration 8051: Achieved Loss=> 22.8514390789
Gradients: w=>0.0055661229, b=>-0.0960306425
Iteration 8052: Achieved Loss=> 22.8514298285
Gradients: w=>0.0055631638, b=>-0.0959795896
Iteration 8053: Achieved Loss=> 22.8514205879
Gradients: w=>0.0055602063, b=>-0.0959285639
Iteration 8054: Achieved Loss=> 22.8514113571
Gradients: w=>0.0055572503, b=>-0.0958775654
Iteration 8055: Achieved Loss=> 22.8514021362
Gradients: w=>0.0055542959, b=>-0.0958265939
Iteration 8056: Achieved Loss=> 22.8513929251
Gradients: w=>0.0055513431, b=>-0.0957756496
Iteration 8057: Achieved Loss=> 22.8513837237
Gradients: w=>0.0055483918, b=>-0.0957247323
Iteration 8058: Achieved Loss=> 22.8513745322
Gradients: w=>0.0055454421, b=>-0.0956738421
Iteration 8059: Achieved Loss=> 22.8513653504
Gradients: w=>0.0055424940, b=>-0.0956229789
Iteration 8060: Achieved Loss=> 22.8513561783
Gradients: w=>0.0055395474, b=>-0.0955721428
Iteration 8061: Achieved Loss=> 22.8513470160
Gradients: w=>0.0055366024, b=>-0.0955213337
Iteration 8062: Achieved Loss=> 22.8513378635
Gradients: w=>0.0055336590, b=>-0.0954705517
Iteration 8063: Achieved Loss=> 22.8513287207
Gradients: w=>0.0055307171, b=>-0.0954197966
Iteration 8064: Achieved Loss=> 22.8513195876
Gradients: w=>0.0055277768, b=>-0.0953690685
Iteration 8065: Achieved Loss=> 22.8513104642
Gradients: w=>0.0055248381, b=>-0.0953183674
Iteration 8066: Achieved Loss=> 22.8513013505
Gradients: w=>0.0055219009, b=>-0.0952676932
Iteration 8067: Achieved Loss=> 22.8512922465
Gradients: w=>0.0055189653, b=>-0.0952170460
Iteration 8068: Achieved Loss=> 22.8512831522
Gradients: w=>0.0055160312, b=>-0.0951664257
Iteration 8069: Achieved Loss=> 22.8512740675
Gradients: w=>0.0055130988, b=>-0.0951158323
Iteration 8070: Achieved Loss=> 22.8512649925
Gradients: w=>0.0055101678, b=>-0.0950652658
Iteration 8071: Achieved Loss=> 22.8512559272
Gradients: w=>0.0055072385, b=>-0.0950147262
Iteration 8072: Achieved Loss=> 22.8512468714
Gradients: w=>0.0055043106, b=>-0.0949642134
Iteration 8073: Achieved Loss=> 22.8512378253
Gradients: w=>0.0055013844, b=>-0.0949137275
Iteration 8074: Achieved Loss=> 22.8512287889
Gradients: w=>0.0054984597, b=>-0.0948632685
Iteration 8075: Achieved Loss=> 22.8512197620
Gradients: w=>0.0054955365, b=>-0.0948128362
Iteration 8076: Achieved Loss=> 22.8512107447
Gradients: w=>0.0054926149, b=>-0.0947624308
Iteration 8077: Achieved Loss=> 22.8512017370
Gradients: w=>0.0054896949, b=>-0.0947120522
Iteration 8078: Achieved Loss=> 22.8511927389
Gradients: w=>0.0054867764, b=>-0.0946617004
Iteration 8079: Achieved Loss=> 22.8511837504
Gradients: w=>0.0054838595, b=>-0.0946113753
Iteration 8080: Achieved Loss=> 22.8511747714
Gradients: w=>0.0054809441, b=>-0.0945610770
Iteration 8081: Achieved Loss=> 22.8511658019
Gradients: w=>0.0054780302, b=>-0.0945108054
Iteration 8082: Achieved Loss=> 22.8511568420
Gradients: w=>0.0054751179, b=>-0.0944605606
Iteration 8083: Achieved Loss=> 22.8511478916
Gradients: w=>0.0054722072, b=>-0.0944103425
Iteration 8084: Achieved Loss=> 22.8511389507
Gradients: w=>0.0054692980, b=>-0.0943601510
Iteration 8085: Achieved Loss=> 22.8511300193
Gradients: w=>0.0054663904, b=>-0.0943099863
Iteration 8086: Achieved Loss=> 22.8511210974
Gradients: w=>0.0054634843, b=>-0.0942598482
Iteration 8087: Achieved Loss=> 22.8511121850
Gradients: w=>0.0054605797, b=>-0.0942097368
Iteration 8088: Achieved Loss=> 22.8511032821
Gradients: w=>0.0054576767, b=>-0.0941596520
Iteration 8089: Achieved Loss=> 22.8510943887
Gradients: w=>0.0054547752, b=>-0.0941095938
Iteration 8090: Achieved Loss=> 22.8510855047
Gradients: w=>0.0054518753, b=>-0.0940595623
Iteration 8091: Achieved Loss=> 22.8510766301
Gradients: w=>0.0054489769, b=>-0.0940095573
Iteration 8092: Achieved Loss=> 22.8510677650
Gradients: w=>0.0054460801, b=>-0.0939595789
Iteration 8093: Achieved Loss=> 22.8510589092
Gradients: w=>0.0054431848, b=>-0.0939096271
Iteration 8094: Achieved Loss=> 22.8510500630
Gradients: w=>0.0054402910, b=>-0.0938597019
Iteration 8095: Achieved Loss=> 22.8510412261
Gradients: w=>0.0054373988, b=>-0.0938098032
Iteration 8096: Achieved Loss=> 22.8510323986
Gradients: w=>0.0054345081, b=>-0.0937599310
Iteration 8097: Achieved Loss=> 22.8510235805
Gradients: w=>0.0054316189, b=>-0.0937100854
Iteration 8098: Achieved Loss=> 22.8510147717
Gradients: w=>0.0054287313, b=>-0.0936602662
Iteration 8099: Achieved Loss=> 22.8510059723
Gradients: w=>0.0054258452, b=>-0.0936104735
Iteration 8100: Achieved Loss=> 22.8509971823
Gradients: w=>0.0054229607, b=>-0.0935607073
Iteration 8101: Achieved Loss=> 22.8509884016
Gradients: w=>0.0054200777, b=>-0.0935109676
Iteration 8102: Achieved Loss=> 22.8509796303
Gradients: w=>0.0054171962, b=>-0.0934612543
Iteration 8103: Achieved Loss=> 22.8509708683
Gradients: w=>0.0054143163, b=>-0.0934115674
Iteration 8104: Achieved Loss=> 22.8509621156
Gradients: w=>0.0054114378, b=>-0.0933619070
Iteration 8105: Achieved Loss=> 22.8509533722
Gradients: w=>0.0054085609, b=>-0.0933122729
Iteration 8106: Achieved Loss=> 22.8509446380
Gradients: w=>0.0054056856, b=>-0.0932626652
Iteration 8107: Achieved Loss=> 22.8509359132
Gradients: w=>0.0054028118, b=>-0.0932130840
Iteration 8108: Achieved Loss=> 22.8509271977
Gradients: w=>0.0053999395, b=>-0.0931635290
Iteration 8109: Achieved Loss=> 22.8509184914
Gradients: w=>0.0053970687, b=>-0.0931140004
Iteration 8110: Achieved Loss=> 22.8509097943
Gradients: w=>0.0053941994, b=>-0.0930644982
Iteration 8111: Achieved Loss=> 22.8509011066
Gradients: w=>0.0053913317, b=>-0.0930150222
Iteration 8112: Achieved Loss=> 22.8508924280
Gradients: w=>0.0053884655, b=>-0.0929655726
Iteration 8113: Achieved Loss=> 22.8508837587
Gradients: w=>0.0053856008, b=>-0.0929161492
Iteration 8114: Achieved Loss=> 22.8508750986
Gradients: w=>0.0053827377, b=>-0.0928667521
Iteration 8115: Achieved Loss=> 22.8508664477
Gradients: w=>0.0053798761, b=>-0.0928173813
Iteration 8116: Achieved Loss=> 22.8508578059
Gradients: w=>0.0053770160, b=>-0.0927680368
Iteration 8117: Achieved Loss=> 22.8508491734
Gradients: w=>0.0053741574, b=>-0.0927187184
Iteration 8118: Achieved Loss=> 22.8508405501
Gradients: w=>0.0053713003, b=>-0.0926694263
Iteration 8119: Achieved Loss=> 22.8508319359
Gradients: w=>0.0053684448, b=>-0.0926201604
Iteration 8120: Achieved Loss=> 22.8508233309
Gradients: w=>0.0053655907, b=>-0.0925709207
Iteration 8121: Achieved Loss=> 22.8508147350
Gradients: w=>0.0053627382, b=>-0.0925217071
Iteration 8122: Achieved Loss=> 22.8508061482
Gradients: w=>0.0053598872, b=>-0.0924725198
Iteration 8123: Achieved Loss=> 22.8507975706
Gradients: w=>0.0053570377, b=>-0.0924233585
Iteration 8124: Achieved Loss=> 22.8507890021
Gradients: w=>0.0053541898, b=>-0.0923742234
Iteration 8125: Achieved Loss=> 22.8507804427
Gradients: w=>0.0053513433, b=>-0.0923251145
Iteration 8126: Achieved Loss=> 22.8507718924
Gradients: w=>0.0053484984, b=>-0.0922760316
Iteration 8127: Achieved Loss=> 22.8507633512
Gradients: w=>0.0053456550, b=>-0.0922269748
Iteration 8128: Achieved Loss=> 22.8507548191
Gradients: w=>0.0053428130, b=>-0.0921779441
Iteration 8129: Achieved Loss=> 22.8507462961
Gradients: w=>0.0053399726, b=>-0.0921289395
Iteration 8130: Achieved Loss=> 22.8507377821
Gradients: w=>0.0053371337, b=>-0.0920799609
Iteration 8131: Achieved Loss=> 22.8507292771
Gradients: w=>0.0053342964, b=>-0.0920310084
Iteration 8132: Achieved Loss=> 22.8507207812
Gradients: w=>0.0053314605, b=>-0.0919820819
Iteration 8133: Achieved Loss=> 22.8507122944
Gradients: w=>0.0053286261, b=>-0.0919331814
Iteration 8134: Achieved Loss=> 22.8507038165
Gradients: w=>0.0053257933, b=>-0.0918843069
Iteration 8135: Achieved Loss=> 22.8506953477
Gradients: w=>0.0053229619, b=>-0.0918354584
Iteration 8136: Achieved Loss=> 22.8506868878
Gradients: w=>0.0053201321, b=>-0.0917866358
Iteration 8137: Achieved Loss=> 22.8506784370
Gradients: w=>0.0053173037, b=>-0.0917378392
Iteration 8138: Achieved Loss=> 22.8506699951
Gradients: w=>0.0053144769, b=>-0.0916890686
Iteration 8139: Achieved Loss=> 22.8506615622
Gradients: w=>0.0053116515, b=>-0.0916403239
Iteration 8140: Achieved Loss=> 22.8506531383
Gradients: w=>0.0053088277, b=>-0.0915916051
Iteration 8141: Achieved Loss=> 22.8506447234
Gradients: w=>0.0053060054, b=>-0.0915429121
Iteration 8142: Achieved Loss=> 22.8506363173
Gradients: w=>0.0053031845, b=>-0.0914942451
Iteration 8143: Achieved Loss=> 22.8506279202
Gradients: w=>0.0053003652, b=>-0.0914456040
Iteration 8144: Achieved Loss=> 22.8506195321
Gradients: w=>0.0052975473, b=>-0.0913969887
Iteration 8145: Achieved Loss=> 22.8506111528
Gradients: w=>0.0052947310, b=>-0.0913483992
Iteration 8146: Achieved Loss=> 22.8506027825
Gradients: w=>0.0052919162, b=>-0.0912998356
Iteration 8147: Achieved Loss=> 22.8505944211
Gradients: w=>0.0052891028, b=>-0.0912512978
Iteration 8148: Achieved Loss=> 22.8505860685
Gradients: w=>0.0052862910, b=>-0.0912027858
Iteration 8149: Achieved Loss=> 22.8505777248
Gradients: w=>0.0052834806, b=>-0.0911542996
Iteration 8150: Achieved Loss=> 22.8505693900
Gradients: w=>0.0052806718, b=>-0.0911058392
Iteration 8151: Achieved Loss=> 22.8505610641
Gradients: w=>0.0052778644, b=>-0.0910574046
Iteration 8152: Achieved Loss=> 22.8505527470
Gradients: w=>0.0052750585, b=>-0.0910089956
Iteration 8153: Achieved Loss=> 22.8505444387
Gradients: w=>0.0052722541, b=>-0.0909606125
Iteration 8154: Achieved Loss=> 22.8505361393
Gradients: w=>0.0052694512, b=>-0.0909122550
Iteration 8155: Achieved Loss=> 22.8505278487
Gradients: w=>0.0052666498, b=>-0.0908639233
Iteration 8156: Achieved Loss=> 22.8505195669
Gradients: w=>0.0052638499, b=>-0.0908156172
Iteration 8157: Achieved Loss=> 22.8505112939
Gradients: w=>0.0052610515, b=>-0.0907673368
Iteration 8158: Achieved Loss=> 22.8505030297
Gradients: w=>0.0052582546, b=>-0.0907190821
Iteration 8159: Achieved Loss=> 22.8504947743
Gradients: w=>0.0052554591, b=>-0.0906708531
Iteration 8160: Achieved Loss=> 22.8504865277
Gradients: w=>0.0052526652, b=>-0.0906226497
Iteration 8161: Achieved Loss=> 22.8504782898
Gradients: w=>0.0052498727, b=>-0.0905744719
Iteration 8162: Achieved Loss=> 22.8504700607
Gradients: w=>0.0052470817, b=>-0.0905263197
Iteration 8163: Achieved Loss=> 22.8504618404
Gradients: w=>0.0052442922, b=>-0.0904781932
Iteration 8164: Achieved Loss=> 22.8504536287
Gradients: w=>0.0052415041, b=>-0.0904300922
Iteration 8165: Achieved Loss=> 22.8504454258
Gradients: w=>0.0052387176, b=>-0.0903820168
Iteration 8166: Achieved Loss=> 22.8504372317
Gradients: w=>0.0052359325, b=>-0.0903339669
Iteration 8167: Achieved Loss=> 22.8504290462
Gradients: w=>0.0052331490, b=>-0.0902859426
Iteration 8168: Achieved Loss=> 22.8504208694
Gradients: w=>0.0052303669, b=>-0.0902379438
Iteration 8169: Achieved Loss=> 22.8504127014
Gradients: w=>0.0052275862, b=>-0.0901899706
Iteration 8170: Achieved Loss=> 22.8504045420
Gradients: w=>0.0052248071, b=>-0.0901420228
Iteration 8171: Achieved Loss=> 22.8503963913
Gradients: w=>0.0052220294, b=>-0.0900941005
Iteration 8172: Achieved Loss=> 22.8503882492
Gradients: w=>0.0052192532, b=>-0.0900462038
Iteration 8173: Achieved Loss=> 22.8503801158
Gradients: w=>0.0052164785, b=>-0.0899983324
Iteration 8174: Achieved Loss=> 22.8503719911
Gradients: w=>0.0052137053, b=>-0.0899504866
Iteration 8175: Achieved Loss=> 22.8503638750
Gradients: w=>0.0052109335, b=>-0.0899026661
Iteration 8176: Achieved Loss=> 22.8503557675
Gradients: w=>0.0052081632, b=>-0.0898548711
Iteration 8177: Achieved Loss=> 22.8503476686
Gradients: w=>0.0052053944, b=>-0.0898071015
Iteration 8178: Achieved Loss=> 22.8503395783
Gradients: w=>0.0052026270, b=>-0.0897593573
Iteration 8179: Achieved Loss=> 22.8503314967
Gradients: w=>0.0051998612, b=>-0.0897116384
Iteration 8180: Achieved Loss=> 22.8503234236
Gradients: w=>0.0051970968, b=>-0.0896639450
Iteration 8181: Achieved Loss=> 22.8503153591
Gradients: w=>0.0051943338, b=>-0.0896162769
Iteration 8182: Achieved Loss=> 22.8503073032
Gradients: w=>0.0051915724, b=>-0.0895686341
Iteration 8183: Achieved Loss=> 22.8502992558
Gradients: w=>0.0051888124, b=>-0.0895210167
Iteration 8184: Achieved Loss=> 22.8502912171
Gradients: w=>0.0051860538, b=>-0.0894734246
Iteration 8185: Achieved Loss=> 22.8502831868
Gradients: w=>0.0051832968, b=>-0.0894258577
Iteration 8186: Achieved Loss=> 22.8502751651
Gradients: w=>0.0051805412, b=>-0.0893783162
Iteration 8187: Achieved Loss=> 22.8502671519
Gradients: w=>0.0051777870, b=>-0.0893308000
Iteration 8188: Achieved Loss=> 22.8502591472
Gradients: w=>0.0051750344, b=>-0.0892833090
Iteration 8189: Achieved Loss=> 22.8502511511
Gradients: w=>0.0051722831, b=>-0.0892358432
Iteration 8190: Achieved Loss=> 22.8502431634
Gradients: w=>0.0051695334, b=>-0.0891884027
Iteration 8191: Achieved Loss=> 22.8502351842
Gradients: w=>0.0051667851, b=>-0.0891409874
Iteration 8192: Achieved Loss=> 22.8502272135
Gradients: w=>0.0051640383, b=>-0.0890935973
Iteration 8193: Achieved Loss=> 22.8502192513
Gradients: w=>0.0051612929, b=>-0.0890462324
Iteration 8194: Achieved Loss=> 22.8502112975
Gradients: w=>0.0051585490, b=>-0.0889988927
Iteration 8195: Achieved Loss=> 22.8502033522
Gradients: w=>0.0051558066, b=>-0.0889515782
Iteration 8196: Achieved Loss=> 22.8501954154
Gradients: w=>0.0051530656, b=>-0.0889042888
Iteration 8197: Achieved Loss=> 22.8501874870
Gradients: w=>0.0051503261, b=>-0.0888570245
Iteration 8198: Achieved Loss=> 22.8501795670
Gradients: w=>0.0051475880, b=>-0.0888097854
Iteration 8199: Achieved Loss=> 22.8501716554
Gradients: w=>0.0051448514, b=>-0.0887625714
Iteration 8200: Achieved Loss=> 22.8501637522
Gradients: w=>0.0051421162, b=>-0.0887153825
Iteration 8201: Achieved Loss=> 22.8501558575
Gradients: w=>0.0051393825, b=>-0.0886682187
Iteration 8202: Achieved Loss=> 22.8501479711
Gradients: w=>0.0051366503, b=>-0.0886210799
Iteration 8203: Achieved Loss=> 22.8501400931
Gradients: w=>0.0051339195, b=>-0.0885739662
Iteration 8204: Achieved Loss=> 22.8501322235
Gradients: w=>0.0051311901, b=>-0.0885268776
Iteration 8205: Achieved Loss=> 22.8501243623
Gradients: w=>0.0051284622, b=>-0.0884798140
Iteration 8206: Achieved Loss=> 22.8501165094
Gradients: w=>0.0051257358, b=>-0.0884327754
Iteration 8207: Achieved Loss=> 22.8501086648
Gradients: w=>0.0051230108, b=>-0.0883857618
Iteration 8208: Achieved Loss=> 22.8501008286
Gradients: w=>0.0051202872, b=>-0.0883387733
Iteration 8209: Achieved Loss=> 22.8500930008
Gradients: w=>0.0051175651, b=>-0.0882918097
Iteration 8210: Achieved Loss=> 22.8500851812
Gradients: w=>0.0051148445, b=>-0.0882448710
Iteration 8211: Achieved Loss=> 22.8500773700
Gradients: w=>0.0051121253, b=>-0.0881979573
Iteration 8212: Achieved Loss=> 22.8500695670
Gradients: w=>0.0051094075, b=>-0.0881510686
Iteration 8213: Achieved Loss=> 22.8500617724
Gradients: w=>0.0051066912, b=>-0.0881042048
Iteration 8214: Achieved Loss=> 22.8500539860
Gradients: w=>0.0051039763, b=>-0.0880573659
Iteration 8215: Achieved Loss=> 22.8500462079
Gradients: w=>0.0051012629, b=>-0.0880105519
Iteration 8216: Achieved Loss=> 22.8500384381
Gradients: w=>0.0050985509, b=>-0.0879637628
Iteration 8217: Achieved Loss=> 22.8500306766
Gradients: w=>0.0050958403, b=>-0.0879169985
Iteration 8218: Achieved Loss=> 22.8500229233
Gradients: w=>0.0050931312, b=>-0.0878702592
Iteration 8219: Achieved Loss=> 22.8500151782
Gradients: w=>0.0050904236, b=>-0.0878235446
Iteration 8220: Achieved Loss=> 22.8500074414
Gradients: w=>0.0050877173, b=>-0.0877768549
Iteration 8221: Achieved Loss=> 22.8499997128
Gradients: w=>0.0050850126, b=>-0.0877301901
Iteration 8222: Achieved Loss=> 22.8499919924
Gradients: w=>0.0050823092, b=>-0.0876835500
Iteration 8223: Achieved Loss=> 22.8499842802
Gradients: w=>0.0050796073, b=>-0.0876369347
Iteration 8224: Achieved Loss=> 22.8499765762
Gradients: w=>0.0050769068, b=>-0.0875903443
Iteration 8225: Achieved Loss=> 22.8499688804
Gradients: w=>0.0050742078, b=>-0.0875437785
Iteration 8226: Achieved Loss=> 22.8499611928
Gradients: w=>0.0050715102, b=>-0.0874972376
Iteration 8227: Achieved Loss=> 22.8499535133
Gradients: w=>0.0050688140, b=>-0.0874507214
Iteration 8228: Achieved Loss=> 22.8499458421
Gradients: w=>0.0050661193, b=>-0.0874042299
Iteration 8229: Achieved Loss=> 22.8499381789
Gradients: w=>0.0050634260, b=>-0.0873577631
Iteration 8230: Achieved Loss=> 22.8499305240
Gradients: w=>0.0050607341, b=>-0.0873113210
Iteration 8231: Achieved Loss=> 22.8499228771
Gradients: w=>0.0050580437, b=>-0.0872649037
Iteration 8232: Achieved Loss=> 22.8499152384
Gradients: w=>0.0050553546, b=>-0.0872185110
Iteration 8233: Achieved Loss=> 22.8499076078
Gradients: w=>0.0050526671, b=>-0.0871721429
Iteration 8234: Achieved Loss=> 22.8498999853
Gradients: w=>0.0050499809, b=>-0.0871257995
Iteration 8235: Achieved Loss=> 22.8498923709
Gradients: w=>0.0050472962, b=>-0.0870794808
Iteration 8236: Achieved Loss=> 22.8498847646
Gradients: w=>0.0050446129, b=>-0.0870331867
Iteration 8237: Achieved Loss=> 22.8498771664
Gradients: w=>0.0050419310, b=>-0.0869869171
Iteration 8238: Achieved Loss=> 22.8498695763
Gradients: w=>0.0050392506, b=>-0.0869406722
Iteration 8239: Achieved Loss=> 22.8498619942
Gradients: w=>0.0050365716, b=>-0.0868944519
Iteration 8240: Achieved Loss=> 22.8498544202
Gradients: w=>0.0050338940, b=>-0.0868482561
Iteration 8241: Achieved Loss=> 22.8498468543
Gradients: w=>0.0050312178, b=>-0.0868020849
Iteration 8242: Achieved Loss=> 22.8498392964
Gradients: w=>0.0050285430, b=>-0.0867559383
Iteration 8243: Achieved Loss=> 22.8498317465
Gradients: w=>0.0050258697, b=>-0.0867098162
Iteration 8244: Achieved Loss=> 22.8498242047
Gradients: w=>0.0050231978, b=>-0.0866637186
Iteration 8245: Achieved Loss=> 22.8498166708
Gradients: w=>0.0050205273, b=>-0.0866176455
Iteration 8246: Achieved Loss=> 22.8498091450
Gradients: w=>0.0050178582, b=>-0.0865715969
Iteration 8247: Achieved Loss=> 22.8498016272
Gradients: w=>0.0050151906, b=>-0.0865255728
Iteration 8248: Achieved Loss=> 22.8497941174
Gradients: w=>0.0050125244, b=>-0.0864795731
Iteration 8249: Achieved Loss=> 22.8497866155
Gradients: w=>0.0050098596, b=>-0.0864335979
Iteration 8250: Achieved Loss=> 22.8497791217
Gradients: w=>0.0050071962, b=>-0.0863876472
Iteration 8251: Achieved Loss=> 22.8497716357
Gradients: w=>0.0050045342, b=>-0.0863417208
Iteration 8252: Achieved Loss=> 22.8497641578
Gradients: w=>0.0050018736, b=>-0.0862958189
Iteration 8253: Achieved Loss=> 22.8497566878
Gradients: w=>0.0049992145, b=>-0.0862499414
Iteration 8254: Achieved Loss=> 22.8497492257
Gradients: w=>0.0049965567, b=>-0.0862040883
Iteration 8255: Achieved Loss=> 22.8497417716
Gradients: w=>0.0049939004, b=>-0.0861582596
Iteration 8256: Achieved Loss=> 22.8497343254
Gradients: w=>0.0049912455, b=>-0.0861124552
Iteration 8257: Achieved Loss=> 22.8497268871
Gradients: w=>0.0049885920, b=>-0.0860666752
Iteration 8258: Achieved Loss=> 22.8497194567
Gradients: w=>0.0049859399, b=>-0.0860209195
Iteration 8259: Achieved Loss=> 22.8497120342
Gradients: w=>0.0049832892, b=>-0.0859751881
Iteration 8260: Achieved Loss=> 22.8497046197
Gradients: w=>0.0049806400, b=>-0.0859294811
Iteration 8261: Achieved Loss=> 22.8496972129
Gradients: w=>0.0049779921, b=>-0.0858837983
Iteration 8262: Achieved Loss=> 22.8496898141
Gradients: w=>0.0049753456, b=>-0.0858381399
Iteration 8263: Achieved Loss=> 22.8496824231
Gradients: w=>0.0049727006, b=>-0.0857925057
Iteration 8264: Achieved Loss=> 22.8496750400
Gradients: w=>0.0049700570, b=>-0.0857468957
Iteration 8265: Achieved Loss=> 22.8496676647
Gradients: w=>0.0049674147, b=>-0.0857013101
Iteration 8266: Achieved Loss=> 22.8496602973
Gradients: w=>0.0049647739, b=>-0.0856557486
Iteration 8267: Achieved Loss=> 22.8496529377
Gradients: w=>0.0049621345, b=>-0.0856102114
Iteration 8268: Achieved Loss=> 22.8496455859
Gradients: w=>0.0049594964, b=>-0.0855646984
Iteration 8269: Achieved Loss=> 22.8496382420
Gradients: w=>0.0049568598, b=>-0.0855192095
Iteration 8270: Achieved Loss=> 22.8496309058
Gradients: w=>0.0049542246, b=>-0.0854737449
Iteration 8271: Achieved Loss=> 22.8496235775
Gradients: w=>0.0049515908, b=>-0.0854283044
Iteration 8272: Achieved Loss=> 22.8496162569
Gradients: w=>0.0049489584, b=>-0.0853828881
Iteration 8273: Achieved Loss=> 22.8496089441
Gradients: w=>0.0049463274, b=>-0.0853374960
Iteration 8274: Achieved Loss=> 22.8496016391
Gradients: w=>0.0049436977, b=>-0.0852921279
Iteration 8275: Achieved Loss=> 22.8495943418
Gradients: w=>0.0049410695, b=>-0.0852467840
Iteration 8276: Achieved Loss=> 22.8495870524
Gradients: w=>0.0049384427, b=>-0.0852014642
Iteration 8277: Achieved Loss=> 22.8495797706
Gradients: w=>0.0049358173, b=>-0.0851561685
Iteration 8278: Achieved Loss=> 22.8495724966
Gradients: w=>0.0049331932, b=>-0.0851108969
Iteration 8279: Achieved Loss=> 22.8495652303
Gradients: w=>0.0049305706, b=>-0.0850656493
Iteration 8280: Achieved Loss=> 22.8495579718
Gradients: w=>0.0049279494, b=>-0.0850204258
Iteration 8281: Achieved Loss=> 22.8495507210
Gradients: w=>0.0049253295, b=>-0.0849752263
Iteration 8282: Achieved Loss=> 22.8495434778
Gradients: w=>0.0049227110, b=>-0.0849300509
Iteration 8283: Achieved Loss=> 22.8495362424
Gradients: w=>0.0049200940, b=>-0.0848848995
Iteration 8284: Achieved Loss=> 22.8495290147
Gradients: w=>0.0049174783, b=>-0.0848397721
Iteration 8285: Achieved Loss=> 22.8495217946
Gradients: w=>0.0049148640, b=>-0.0847946686
Iteration 8286: Achieved Loss=> 22.8495145823
Gradients: w=>0.0049122511, b=>-0.0847495892
Iteration 8287: Achieved Loss=> 22.8495073776
Gradients: w=>0.0049096396, b=>-0.0847045337
Iteration 8288: Achieved Loss=> 22.8495001805
Gradients: w=>0.0049070295, b=>-0.0846595022
Iteration 8289: Achieved Loss=> 22.8494929911
Gradients: w=>0.0049044208, b=>-0.0846144946
Iteration 8290: Achieved Loss=> 22.8494858094
Gradients: w=>0.0049018134, b=>-0.0845695109
Iteration 8291: Achieved Loss=> 22.8494786352
Gradients: w=>0.0048992075, b=>-0.0845245512
Iteration 8292: Achieved Loss=> 22.8494714687
Gradients: w=>0.0048966029, b=>-0.0844796153
Iteration 8293: Achieved Loss=> 22.8494643099
Gradients: w=>0.0048939997, b=>-0.0844347034
Iteration 8294: Achieved Loss=> 22.8494571586
Gradients: w=>0.0048913979, b=>-0.0843898153
Iteration 8295: Achieved Loss=> 22.8494500149
Gradients: w=>0.0048887975, b=>-0.0843449511
Iteration 8296: Achieved Loss=> 22.8494428788
Gradients: w=>0.0048861985, b=>-0.0843001107
Iteration 8297: Achieved Loss=> 22.8494357504
Gradients: w=>0.0048836008, b=>-0.0842552942
Iteration 8298: Achieved Loss=> 22.8494286294
Gradients: w=>0.0048810046, b=>-0.0842105015
Iteration 8299: Achieved Loss=> 22.8494215161
Gradients: w=>0.0048784097, b=>-0.0841657326
Iteration 8300: Achieved Loss=> 22.8494144103
Gradients: w=>0.0048758162, b=>-0.0841209875
Iteration 8301: Achieved Loss=> 22.8494073121
Gradients: w=>0.0048732240, b=>-0.0840762662
Iteration 8302: Achieved Loss=> 22.8494002214
Gradients: w=>0.0048706333, b=>-0.0840315687
Iteration 8303: Achieved Loss=> 22.8493931383
Gradients: w=>0.0048680439, b=>-0.0839868949
Iteration 8304: Achieved Loss=> 22.8493860627
Gradients: w=>0.0048654559, b=>-0.0839422449
Iteration 8305: Achieved Loss=> 22.8493789946
Gradients: w=>0.0048628693, b=>-0.0838976186
Iteration 8306: Achieved Loss=> 22.8493719340
Gradients: w=>0.0048602840, b=>-0.0838530161
Iteration 8307: Achieved Loss=> 22.8493648809
Gradients: w=>0.0048577001, b=>-0.0838084372
Iteration 8308: Achieved Loss=> 22.8493578353
Gradients: w=>0.0048551176, b=>-0.0837638821
Iteration 8309: Achieved Loss=> 22.8493507972
Gradients: w=>0.0048525365, b=>-0.0837193507
Iteration 8310: Achieved Loss=> 22.8493437666
Gradients: w=>0.0048499567, b=>-0.0836748429
Iteration 8311: Achieved Loss=> 22.8493367435
Gradients: w=>0.0048473784, b=>-0.0836303588
Iteration 8312: Achieved Loss=> 22.8493297278
Gradients: w=>0.0048448013, b=>-0.0835858983
Iteration 8313: Achieved Loss=> 22.8493227196
Gradients: w=>0.0048422257, b=>-0.0835414615
Iteration 8314: Achieved Loss=> 22.8493157189
Gradients: w=>0.0048396514, b=>-0.0834970483
Iteration 8315: Achieved Loss=> 22.8493087255
Gradients: w=>0.0048370785, b=>-0.0834526587
Iteration 8316: Achieved Loss=> 22.8493017397
Gradients: w=>0.0048345070, b=>-0.0834082927
Iteration 8317: Achieved Loss=> 22.8492947612
Gradients: w=>0.0048319368, b=>-0.0833639503
Iteration 8318: Achieved Loss=> 22.8492877902
Gradients: w=>0.0048293680, b=>-0.0833196314
Iteration 8319: Achieved Loss=> 22.8492808265
Gradients: w=>0.0048268006, b=>-0.0832753362
Iteration 8320: Achieved Loss=> 22.8492738703
Gradients: w=>0.0048242345, b=>-0.0832310644
Iteration 8321: Achieved Loss=> 22.8492669215
Gradients: w=>0.0048216698, b=>-0.0831868162
Iteration 8322: Achieved Loss=> 22.8492599800
Gradients: w=>0.0048191064, b=>-0.0831425916
Iteration 8323: Achieved Loss=> 22.8492530459
Gradients: w=>0.0048165444, b=>-0.0830983904
Iteration 8324: Achieved Loss=> 22.8492461192
Gradients: w=>0.0048139838, b=>-0.0830542128
Iteration 8325: Achieved Loss=> 22.8492391999
Gradients: w=>0.0048114246, b=>-0.0830100586
Iteration 8326: Achieved Loss=> 22.8492322879
Gradients: w=>0.0048088667, b=>-0.0829659279
Iteration 8327: Achieved Loss=> 22.8492253833
Gradients: w=>0.0048063101, b=>-0.0829218207
Iteration 8328: Achieved Loss=> 22.8492184860
Gradients: w=>0.0048037549, b=>-0.0828777369
Iteration 8329: Achieved Loss=> 22.8492115960
Gradients: w=>0.0048012011, b=>-0.0828336765
Iteration 8330: Achieved Loss=> 22.8492047134
Gradients: w=>0.0047986486, b=>-0.0827896396
Iteration 8331: Achieved Loss=> 22.8491978381
Gradients: w=>0.0047960975, b=>-0.0827456261
Iteration 8332: Achieved Loss=> 22.8491909700
Gradients: w=>0.0047935478, b=>-0.0827016360
Iteration 8333: Achieved Loss=> 22.8491841093
Gradients: w=>0.0047909994, b=>-0.0826576693
Iteration 8334: Achieved Loss=> 22.8491772559
Gradients: w=>0.0047884523, b=>-0.0826137259
Iteration 8335: Achieved Loss=> 22.8491704098
Gradients: w=>0.0047859067, b=>-0.0825698059
Iteration 8336: Achieved Loss=> 22.8491635709
Gradients: w=>0.0047833623, b=>-0.0825259093
Iteration 8337: Achieved Loss=> 22.8491567393
Gradients: w=>0.0047808193, b=>-0.0824820360
Iteration 8338: Achieved Loss=> 22.8491499150
Gradients: w=>0.0047782777, b=>-0.0824381860
Iteration 8339: Achieved Loss=> 22.8491430979
Gradients: w=>0.0047757374, b=>-0.0823943593
Iteration 8340: Achieved Loss=> 22.8491362881
Gradients: w=>0.0047731985, b=>-0.0823505559
Iteration 8341: Achieved Loss=> 22.8491294855
Gradients: w=>0.0047706609, b=>-0.0823067759
Iteration 8342: Achieved Loss=> 22.8491226901
Gradients: w=>0.0047681247, b=>-0.0822630190
Iteration 8343: Achieved Loss=> 22.8491159020
Gradients: w=>0.0047655898, b=>-0.0822192855
Iteration 8344: Achieved Loss=> 22.8491091211
Gradients: w=>0.0047630563, b=>-0.0821755752
Iteration 8345: Achieved Loss=> 22.8491023474
Gradients: w=>0.0047605241, b=>-0.0821318881
Iteration 8346: Achieved Loss=> 22.8490955809
Gradients: w=>0.0047579932, b=>-0.0820882243
Iteration 8347: Achieved Loss=> 22.8490888216
Gradients: w=>0.0047554638, b=>-0.0820445837
Iteration 8348: Achieved Loss=> 22.8490820694
Gradients: w=>0.0047529356, b=>-0.0820009663
Iteration 8349: Achieved Loss=> 22.8490753245
Gradients: w=>0.0047504088, b=>-0.0819573720
Iteration 8350: Achieved Loss=> 22.8490685867
Gradients: w=>0.0047478833, b=>-0.0819138010
Iteration 8351: Achieved Loss=> 22.8490618561
Gradients: w=>0.0047453592, b=>-0.0818702531
Iteration 8352: Achieved Loss=> 22.8490551326
Gradients: w=>0.0047428364, b=>-0.0818267284
Iteration 8353: Achieved Loss=> 22.8490484163
Gradients: w=>0.0047403150, b=>-0.0817832268
Iteration 8354: Achieved Loss=> 22.8490417071
Gradients: w=>0.0047377949, b=>-0.0817397483
Iteration 8355: Achieved Loss=> 22.8490350050
Gradients: w=>0.0047352761, b=>-0.0816962929
Iteration 8356: Achieved Loss=> 22.8490283101
Gradients: w=>0.0047327587, b=>-0.0816528607
Iteration 8357: Achieved Loss=> 22.8490216223
Gradients: w=>0.0047302426, b=>-0.0816094515
Iteration 8358: Achieved Loss=> 22.8490149416
Gradients: w=>0.0047277279, b=>-0.0815660654
Iteration 8359: Achieved Loss=> 22.8490082680
Gradients: w=>0.0047252145, b=>-0.0815227024
Iteration 8360: Achieved Loss=> 22.8490016015
Gradients: w=>0.0047227024, b=>-0.0814793624
Iteration 8361: Achieved Loss=> 22.8489949421
Gradients: w=>0.0047201917, b=>-0.0814360455
Iteration 8362: Achieved Loss=> 22.8489882897
Gradients: w=>0.0047176823, b=>-0.0813927516
Iteration 8363: Achieved Loss=> 22.8489816445
Gradients: w=>0.0047151742, b=>-0.0813494807
Iteration 8364: Achieved Loss=> 22.8489750062
Gradients: w=>0.0047126675, b=>-0.0813062328
Iteration 8365: Achieved Loss=> 22.8489683751
Gradients: w=>0.0047101621, b=>-0.0812630079
Iteration 8366: Achieved Loss=> 22.8489617510
Gradients: w=>0.0047076580, b=>-0.0812198060
Iteration 8367: Achieved Loss=> 22.8489551339
Gradients: w=>0.0047051553, b=>-0.0811766271
Iteration 8368: Achieved Loss=> 22.8489485239
Gradients: w=>0.0047026539, b=>-0.0811334711
Iteration 8369: Achieved Loss=> 22.8489419209
Gradients: w=>0.0047001538, b=>-0.0810903380
Iteration 8370: Achieved Loss=> 22.8489353249
Gradients: w=>0.0046976551, b=>-0.0810472279
Iteration 8371: Achieved Loss=> 22.8489287360
Gradients: w=>0.0046951577, b=>-0.0810041407
Iteration 8372: Achieved Loss=> 22.8489221540
Gradients: w=>0.0046926616, b=>-0.0809610765
Iteration 8373: Achieved Loss=> 22.8489155790
Gradients: w=>0.0046901668, b=>-0.0809180351
Iteration 8374: Achieved Loss=> 22.8489090110
Gradients: w=>0.0046876734, b=>-0.0808750165
Iteration 8375: Achieved Loss=> 22.8489024501
Gradients: w=>0.0046851813, b=>-0.0808320209
Iteration 8376: Achieved Loss=> 22.8488958960
Gradients: w=>0.0046826905, b=>-0.0807890481
Iteration 8377: Achieved Loss=> 22.8488893490
Gradients: w=>0.0046802010, b=>-0.0807460982
Iteration 8378: Achieved Loss=> 22.8488828089
Gradients: w=>0.0046777129, b=>-0.0807031711
Iteration 8379: Achieved Loss=> 22.8488762757
Gradients: w=>0.0046752260, b=>-0.0806602668
Iteration 8380: Achieved Loss=> 22.8488697495
Gradients: w=>0.0046727406, b=>-0.0806173853
Iteration 8381: Achieved Loss=> 22.8488632303
Gradients: w=>0.0046702564, b=>-0.0805745266
Iteration 8382: Achieved Loss=> 22.8488567179
Gradients: w=>0.0046677735, b=>-0.0805316908
Iteration 8383: Achieved Loss=> 22.8488502125
Gradients: w=>0.0046652920, b=>-0.0804888776
Iteration 8384: Achieved Loss=> 22.8488437140
Gradients: w=>0.0046628118, b=>-0.0804460873
Iteration 8385: Achieved Loss=> 22.8488372224
Gradients: w=>0.0046603329, b=>-0.0804033197
Iteration 8386: Achieved Loss=> 22.8488307377
Gradients: w=>0.0046578553, b=>-0.0803605748
Iteration 8387: Achieved Loss=> 22.8488242599
Gradients: w=>0.0046553791, b=>-0.0803178527
Iteration 8388: Achieved Loss=> 22.8488177890
Gradients: w=>0.0046529041, b=>-0.0802751532
Iteration 8389: Achieved Loss=> 22.8488113250
Gradients: w=>0.0046504305, b=>-0.0802324765
Iteration 8390: Achieved Loss=> 22.8488048678
Gradients: w=>0.0046479582, b=>-0.0801898224
Iteration 8391: Achieved Loss=> 22.8487984176
Gradients: w=>0.0046454872, b=>-0.0801471911
Iteration 8392: Achieved Loss=> 22.8487919741
Gradients: w=>0.0046430175, b=>-0.0801045824
Iteration 8393: Achieved Loss=> 22.8487855375
Gradients: w=>0.0046405491, b=>-0.0800619963
Iteration 8394: Achieved Loss=> 22.8487791078
Gradients: w=>0.0046380821, b=>-0.0800194329
Iteration 8395: Achieved Loss=> 22.8487726849
Gradients: w=>0.0046356163, b=>-0.0799768921
Iteration 8396: Achieved Loss=> 22.8487662688
Gradients: w=>0.0046331519, b=>-0.0799343739
Iteration 8397: Achieved Loss=> 22.8487598595
Gradients: w=>0.0046306888, b=>-0.0798918784
Iteration 8398: Achieved Loss=> 22.8487534571
Gradients: w=>0.0046282269, b=>-0.0798494054
Iteration 8399: Achieved Loss=> 22.8487470614
Gradients: w=>0.0046257664, b=>-0.0798069550
Iteration 8400: Achieved Loss=> 22.8487406726
Gradients: w=>0.0046233072, b=>-0.0797645272
Iteration 8401: Achieved Loss=> 22.8487342905
Gradients: w=>0.0046208493, b=>-0.0797221219
Iteration 8402: Achieved Loss=> 22.8487279152
Gradients: w=>0.0046183928, b=>-0.0796797392
Iteration 8403: Achieved Loss=> 22.8487215467
Gradients: w=>0.0046159375, b=>-0.0796373790
Iteration 8404: Achieved Loss=> 22.8487151850
Gradients: w=>0.0046134835, b=>-0.0795950413
Iteration 8405: Achieved Loss=> 22.8487088300
Gradients: w=>0.0046110308, b=>-0.0795527261
Iteration 8406: Achieved Loss=> 22.8487024818
Gradients: w=>0.0046085795, b=>-0.0795104335
Iteration 8407: Achieved Loss=> 22.8486961404
Gradients: w=>0.0046061294, b=>-0.0794681633
Iteration 8408: Achieved Loss=> 22.8486898056
Gradients: w=>0.0046036806, b=>-0.0794259156
Iteration 8409: Achieved Loss=> 22.8486834777
Gradients: w=>0.0046012332, b=>-0.0793836903
Iteration 8410: Achieved Loss=> 22.8486771564
Gradients: w=>0.0045987870, b=>-0.0793414875
Iteration 8411: Achieved Loss=> 22.8486708419
Gradients: w=>0.0045963422, b=>-0.0792993071
Iteration 8412: Achieved Loss=> 22.8486645340
Gradients: w=>0.0045938986, b=>-0.0792571492
Iteration 8413: Achieved Loss=> 22.8486582329
Gradients: w=>0.0045914564, b=>-0.0792150136
Iteration 8414: Achieved Loss=> 22.8486519385
Gradients: w=>0.0045890154, b=>-0.0791729005
Iteration 8415: Achieved Loss=> 22.8486456507
Gradients: w=>0.0045865757, b=>-0.0791308098
Iteration 8416: Achieved Loss=> 22.8486393697
Gradients: w=>0.0045841374, b=>-0.0790887414
Iteration 8417: Achieved Loss=> 22.8486330953
Gradients: w=>0.0045817003, b=>-0.0790466954
Iteration 8418: Achieved Loss=> 22.8486268276
Gradients: w=>0.0045792645, b=>-0.0790046717
Iteration 8419: Achieved Loss=> 22.8486205666
Gradients: w=>0.0045768301, b=>-0.0789626704
Iteration 8420: Achieved Loss=> 22.8486143122
Gradients: w=>0.0045743969, b=>-0.0789206915
Iteration 8421: Achieved Loss=> 22.8486080644
Gradients: w=>0.0045719650, b=>-0.0788787348
Iteration 8422: Achieved Loss=> 22.8486018233
Gradients: w=>0.0045695344, b=>-0.0788368004
Iteration 8423: Achieved Loss=> 22.8485955889
Gradients: w=>0.0045671051, b=>-0.0787948884
Iteration 8424: Achieved Loss=> 22.8485893610
Gradients: w=>0.0045646771, b=>-0.0787529986
Iteration 8425: Achieved Loss=> 22.8485831398
Gradients: w=>0.0045622504, b=>-0.0787111311
Iteration 8426: Achieved Loss=> 22.8485769252
Gradients: w=>0.0045598249, b=>-0.0786692858
Iteration 8427: Achieved Loss=> 22.8485707172
Gradients: w=>0.0045574008, b=>-0.0786274628
Iteration 8428: Achieved Loss=> 22.8485645158
Gradients: w=>0.0045549779, b=>-0.0785856621
Iteration 8429: Achieved Loss=> 22.8485583210
Gradients: w=>0.0045525564, b=>-0.0785438835
Iteration 8430: Achieved Loss=> 22.8485521328
Gradients: w=>0.0045501361, b=>-0.0785021272
Iteration 8431: Achieved Loss=> 22.8485459511
Gradients: w=>0.0045477171, b=>-0.0784603930
Iteration 8432: Achieved Loss=> 22.8485397761
Gradients: w=>0.0045452994, b=>-0.0784186811
Iteration 8433: Achieved Loss=> 22.8485336076
Gradients: w=>0.0045428830, b=>-0.0783769913
Iteration 8434: Achieved Loss=> 22.8485274456
Gradients: w=>0.0045404678, b=>-0.0783353237
Iteration 8435: Achieved Loss=> 22.8485212902
Gradients: w=>0.0045380540, b=>-0.0782936782
Iteration 8436: Achieved Loss=> 22.8485151413
Gradients: w=>0.0045356414, b=>-0.0782520549
Iteration 8437: Achieved Loss=> 22.8485089990
Gradients: w=>0.0045332301, b=>-0.0782104537
Iteration 8438: Achieved Loss=> 22.8485028632
Gradients: w=>0.0045308201, b=>-0.0781688746
Iteration 8439: Achieved Loss=> 22.8484967340
Gradients: w=>0.0045284114, b=>-0.0781273176
Iteration 8440: Achieved Loss=> 22.8484906112
Gradients: w=>0.0045260039, b=>-0.0780857828
Iteration 8441: Achieved Loss=> 22.8484844950
Gradients: w=>0.0045235978, b=>-0.0780442700
Iteration 8442: Achieved Loss=> 22.8484783852
Gradients: w=>0.0045211929, b=>-0.0780027792
Iteration 8443: Achieved Loss=> 22.8484722820
Gradients: w=>0.0045187893, b=>-0.0779613106
Iteration 8444: Achieved Loss=> 22.8484661852
Gradients: w=>0.0045163870, b=>-0.0779198639
Iteration 8445: Achieved Loss=> 22.8484600949
Gradients: w=>0.0045139859, b=>-0.0778784394
Iteration 8446: Achieved Loss=> 22.8484540111
Gradients: w=>0.0045115861, b=>-0.0778370368
Iteration 8447: Achieved Loss=> 22.8484479338
Gradients: w=>0.0045091876, b=>-0.0777956562
Iteration 8448: Achieved Loss=> 22.8484418629
Gradients: w=>0.0045067904, b=>-0.0777542977
Iteration 8449: Achieved Loss=> 22.8484357984
Gradients: w=>0.0045043945, b=>-0.0777129611
Iteration 8450: Achieved Loss=> 22.8484297405
Gradients: w=>0.0045019998, b=>-0.0776716465
Iteration 8451: Achieved Loss=> 22.8484236889
Gradients: w=>0.0044996064, b=>-0.0776303539
Iteration 8452: Achieved Loss=> 22.8484176438
Gradients: w=>0.0044972143, b=>-0.0775890832
Iteration 8453: Achieved Loss=> 22.8484116051
Gradients: w=>0.0044948234, b=>-0.0775478344
Iteration 8454: Achieved Loss=> 22.8484055729
Gradients: w=>0.0044924338, b=>-0.0775066076
Iteration 8455: Achieved Loss=> 22.8483995470
Gradients: w=>0.0044900455, b=>-0.0774654027
Iteration 8456: Achieved Loss=> 22.8483935276
Gradients: w=>0.0044876585, b=>-0.0774242198
Iteration 8457: Achieved Loss=> 22.8483875145
Gradients: w=>0.0044852727, b=>-0.0773830587
Iteration 8458: Achieved Loss=> 22.8483815078
Gradients: w=>0.0044828882, b=>-0.0773419195
Iteration 8459: Achieved Loss=> 22.8483755076
Gradients: w=>0.0044805049, b=>-0.0773008021
Iteration 8460: Achieved Loss=> 22.8483695137
Gradients: w=>0.0044781230, b=>-0.0772597066
Iteration 8461: Achieved Loss=> 22.8483635262
Gradients: w=>0.0044757423, b=>-0.0772186330
Iteration 8462: Achieved Loss=> 22.8483575450
Gradients: w=>0.0044733628, b=>-0.0771775812
Iteration 8463: Achieved Loss=> 22.8483515702
Gradients: w=>0.0044709846, b=>-0.0771365512
Iteration 8464: Achieved Loss=> 22.8483456017
Gradients: w=>0.0044686077, b=>-0.0770955431
Iteration 8465: Achieved Loss=> 22.8483396396
Gradients: w=>0.0044662321, b=>-0.0770545567
Iteration 8466: Achieved Loss=> 22.8483336839
Gradients: w=>0.0044638577, b=>-0.0770135922
Iteration 8467: Achieved Loss=> 22.8483277344
Gradients: w=>0.0044614846, b=>-0.0769726494
Iteration 8468: Achieved Loss=> 22.8483217913
Gradients: w=>0.0044591127, b=>-0.0769317284
Iteration 8469: Achieved Loss=> 22.8483158545
Gradients: w=>0.0044567421, b=>-0.0768908291
Iteration 8470: Achieved Loss=> 22.8483099240
Gradients: w=>0.0044543728, b=>-0.0768499516
Iteration 8471: Achieved Loss=> 22.8483039999
Gradients: w=>0.0044520047, b=>-0.0768090958
Iteration 8472: Achieved Loss=> 22.8482980820
Gradients: w=>0.0044496379, b=>-0.0767682617
Iteration 8473: Achieved Loss=> 22.8482921704
Gradients: w=>0.0044472723, b=>-0.0767274493
Iteration 8474: Achieved Loss=> 22.8482862651
Gradients: w=>0.0044449080, b=>-0.0766866587
Iteration 8475: Achieved Loss=> 22.8482803660
Gradients: w=>0.0044425449, b=>-0.0766458897
Iteration 8476: Achieved Loss=> 22.8482744733
Gradients: w=>0.0044401831, b=>-0.0766051424
Iteration 8477: Achieved Loss=> 22.8482685868
Gradients: w=>0.0044378226, b=>-0.0765644167
Iteration 8478: Achieved Loss=> 22.8482627065
Gradients: w=>0.0044354633, b=>-0.0765237127
Iteration 8479: Achieved Loss=> 22.8482568325
Gradients: w=>0.0044331053, b=>-0.0764830304
Iteration 8480: Achieved Loss=> 22.8482509648
Gradients: w=>0.0044307485, b=>-0.0764423697
Iteration 8481: Achieved Loss=> 22.8482451033
Gradients: w=>0.0044283930, b=>-0.0764017305
Iteration 8482: Achieved Loss=> 22.8482392480
Gradients: w=>0.0044260387, b=>-0.0763611130
Iteration 8483: Achieved Loss=> 22.8482333990
Gradients: w=>0.0044236857, b=>-0.0763205171
Iteration 8484: Achieved Loss=> 22.8482275561
Gradients: w=>0.0044213339, b=>-0.0762799428
Iteration 8485: Achieved Loss=> 22.8482217195
Gradients: w=>0.0044189834, b=>-0.0762393900
Iteration 8486: Achieved Loss=> 22.8482158891
Gradients: w=>0.0044166342, b=>-0.0761988588
Iteration 8487: Achieved Loss=> 22.8482100648
Gradients: w=>0.0044142861, b=>-0.0761583492
Iteration 8488: Achieved Loss=> 22.8482042468
Gradients: w=>0.0044119394, b=>-0.0761178611
Iteration 8489: Achieved Loss=> 22.8481984350
Gradients: w=>0.0044095939, b=>-0.0760773945
Iteration 8490: Achieved Loss=> 22.8481926293
Gradients: w=>0.0044072496, b=>-0.0760369494
Iteration 8491: Achieved Loss=> 22.8481868298
Gradients: w=>0.0044049065, b=>-0.0759965258
Iteration 8492: Achieved Loss=> 22.8481810365
Gradients: w=>0.0044025648, b=>-0.0759561237
Iteration 8493: Achieved Loss=> 22.8481752493
Gradients: w=>0.0044002242, b=>-0.0759157431
Iteration 8494: Achieved Loss=> 22.8481694683
Gradients: w=>0.0043978849, b=>-0.0758753840
Iteration 8495: Achieved Loss=> 22.8481636934
Gradients: w=>0.0043955469, b=>-0.0758350463
Iteration 8496: Achieved Loss=> 22.8481579246
Gradients: w=>0.0043932101, b=>-0.0757947301
Iteration 8497: Achieved Loss=> 22.8481521620
Gradients: w=>0.0043908745, b=>-0.0757544353
Iteration 8498: Achieved Loss=> 22.8481464055
Gradients: w=>0.0043885402, b=>-0.0757141619
Iteration 8499: Achieved Loss=> 22.8481406552
Gradients: w=>0.0043862071, b=>-0.0756739099
Iteration 8500: Achieved Loss=> 22.8481349109
Gradients: w=>0.0043838753, b=>-0.0756336793
Iteration 8501: Achieved Loss=> 22.8481291728
Gradients: w=>0.0043815447, b=>-0.0755934701
Iteration 8502: Achieved Loss=> 22.8481234407
Gradients: w=>0.0043792153, b=>-0.0755532823
Iteration 8503: Achieved Loss=> 22.8481177148
Gradients: w=>0.0043768872, b=>-0.0755131159
Iteration 8504: Achieved Loss=> 22.8481119949
Gradients: w=>0.0043745603, b=>-0.0754729708
Iteration 8505: Achieved Loss=> 22.8481062811
Gradients: w=>0.0043722346, b=>-0.0754328471
Iteration 8506: Achieved Loss=> 22.8481005734
Gradients: w=>0.0043699102, b=>-0.0753927446
Iteration 8507: Achieved Loss=> 22.8480948718
Gradients: w=>0.0043675870, b=>-0.0753526635
Iteration 8508: Achieved Loss=> 22.8480891762
Gradients: w=>0.0043652651, b=>-0.0753126038
Iteration 8509: Achieved Loss=> 22.8480834867
Gradients: w=>0.0043629444, b=>-0.0752725653
Iteration 8510: Achieved Loss=> 22.8480778032
Gradients: w=>0.0043606249, b=>-0.0752325481
Iteration 8511: Achieved Loss=> 22.8480721257
Gradients: w=>0.0043583067, b=>-0.0751925521
Iteration 8512: Achieved Loss=> 22.8480664543
Gradients: w=>0.0043559897, b=>-0.0751525775
Iteration 8513: Achieved Loss=> 22.8480607889
Gradients: w=>0.0043536739, b=>-0.0751126240
Iteration 8514: Achieved Loss=> 22.8480551296
Gradients: w=>0.0043513593, b=>-0.0750726919
Iteration 8515: Achieved Loss=> 22.8480494762
Gradients: w=>0.0043490460, b=>-0.0750327809
Iteration 8516: Achieved Loss=> 22.8480438289
Gradients: w=>0.0043467339, b=>-0.0749928912
Iteration 8517: Achieved Loss=> 22.8480381876
Gradients: w=>0.0043444231, b=>-0.0749530227
Iteration 8518: Achieved Loss=> 22.8480325523
Gradients: w=>0.0043421134, b=>-0.0749131753
Iteration 8519: Achieved Loss=> 22.8480269229
Gradients: w=>0.0043398050, b=>-0.0748733492
Iteration 8520: Achieved Loss=> 22.8480212996
Gradients: w=>0.0043374979, b=>-0.0748335442
Iteration 8521: Achieved Loss=> 22.8480156822
Gradients: w=>0.0043351919, b=>-0.0747937604
Iteration 8522: Achieved Loss=> 22.8480100708
Gradients: w=>0.0043328872, b=>-0.0747539978
Iteration 8523: Achieved Loss=> 22.8480044653
Gradients: w=>0.0043305837, b=>-0.0747142562
Iteration 8524: Achieved Loss=> 22.8479988658
Gradients: w=>0.0043282814, b=>-0.0746745358
Iteration 8525: Achieved Loss=> 22.8479932723
Gradients: w=>0.0043259804, b=>-0.0746348366
Iteration 8526: Achieved Loss=> 22.8479876847
Gradients: w=>0.0043236806, b=>-0.0745951584
Iteration 8527: Achieved Loss=> 22.8479821031
Gradients: w=>0.0043213820, b=>-0.0745555013
Iteration 8528: Achieved Loss=> 22.8479765274
Gradients: w=>0.0043190846, b=>-0.0745158653
Iteration 8529: Achieved Loss=> 22.8479709576
Gradients: w=>0.0043167884, b=>-0.0744762504
Iteration 8530: Achieved Loss=> 22.8479653937
Gradients: w=>0.0043144935, b=>-0.0744366565
Iteration 8531: Achieved Loss=> 22.8479598357
Gradients: w=>0.0043121998, b=>-0.0743970837
Iteration 8532: Achieved Loss=> 22.8479542837
Gradients: w=>0.0043099073, b=>-0.0743575320
Iteration 8533: Achieved Loss=> 22.8479487376
Gradients: w=>0.0043076160, b=>-0.0743180012
Iteration 8534: Achieved Loss=> 22.8479431973
Gradients: w=>0.0043053259, b=>-0.0742784915
Iteration 8535: Achieved Loss=> 22.8479376630
Gradients: w=>0.0043030371, b=>-0.0742390028
Iteration 8536: Achieved Loss=> 22.8479321345
Gradients: w=>0.0043007495, b=>-0.0741995350
Iteration 8537: Achieved Loss=> 22.8479266119
Gradients: w=>0.0042984631, b=>-0.0741600883
Iteration 8538: Achieved Loss=> 22.8479210952
Gradients: w=>0.0042961779, b=>-0.0741206625
Iteration 8539: Achieved Loss=> 22.8479155843
Gradients: w=>0.0042938939, b=>-0.0740812577
Iteration 8540: Achieved Loss=> 22.8479100793
Gradients: w=>0.0042916111, b=>-0.0740418738
Iteration 8541: Achieved Loss=> 22.8479045801
Gradients: w=>0.0042893296, b=>-0.0740025109
Iteration 8542: Achieved Loss=> 22.8478990868
Gradients: w=>0.0042870492, b=>-0.0739631689
Iteration 8543: Achieved Loss=> 22.8478935993
Gradients: w=>0.0042847701, b=>-0.0739238478
Iteration 8544: Achieved Loss=> 22.8478881177
Gradients: w=>0.0042824922, b=>-0.0738845476
Iteration 8545: Achieved Loss=> 22.8478826419
Gradients: w=>0.0042802155, b=>-0.0738452683
Iteration 8546: Achieved Loss=> 22.8478771719
Gradients: w=>0.0042779400, b=>-0.0738060099
Iteration 8547: Achieved Loss=> 22.8478717077
Gradients: w=>0.0042756657, b=>-0.0737667723
Iteration 8548: Achieved Loss=> 22.8478662494
Gradients: w=>0.0042733926, b=>-0.0737275557
Iteration 8549: Achieved Loss=> 22.8478607968
Gradients: w=>0.0042711208, b=>-0.0736883598
Iteration 8550: Achieved Loss=> 22.8478553500
Gradients: w=>0.0042688501, b=>-0.0736491848
Iteration 8551: Achieved Loss=> 22.8478499091
Gradients: w=>0.0042665806, b=>-0.0736100307
Iteration 8552: Achieved Loss=> 22.8478444739
Gradients: w=>0.0042643124, b=>-0.0735708973
Iteration 8553: Achieved Loss=> 22.8478390444
Gradients: w=>0.0042620454, b=>-0.0735317848
Iteration 8554: Achieved Loss=> 22.8478336208
Gradients: w=>0.0042597795, b=>-0.0734926930
Iteration 8555: Achieved Loss=> 22.8478282029
Gradients: w=>0.0042575149, b=>-0.0734536220
Iteration 8556: Achieved Loss=> 22.8478227908
Gradients: w=>0.0042552515, b=>-0.0734145718
Iteration 8557: Achieved Loss=> 22.8478173844
Gradients: w=>0.0042529892, b=>-0.0733755424
Iteration 8558: Achieved Loss=> 22.8478119838
Gradients: w=>0.0042507282, b=>-0.0733365337
Iteration 8559: Achieved Loss=> 22.8478065889
Gradients: w=>0.0042484684, b=>-0.0732975458
Iteration 8560: Achieved Loss=> 22.8478011998
Gradients: w=>0.0042462098, b=>-0.0732585785
Iteration 8561: Achieved Loss=> 22.8477958164
Gradients: w=>0.0042439524, b=>-0.0732196320
Iteration 8562: Achieved Loss=> 22.8477904387
Gradients: w=>0.0042416962, b=>-0.0731807062
Iteration 8563: Achieved Loss=> 22.8477850667
Gradients: w=>0.0042394411, b=>-0.0731418011
Iteration 8564: Achieved Loss=> 22.8477797004
Gradients: w=>0.0042371873, b=>-0.0731029167
Iteration 8565: Achieved Loss=> 22.8477743398
Gradients: w=>0.0042349347, b=>-0.0730640529
Iteration 8566: Achieved Loss=> 22.8477689850
Gradients: w=>0.0042326833, b=>-0.0730252098
Iteration 8567: Achieved Loss=> 22.8477636358
Gradients: w=>0.0042304331, b=>-0.0729863874
Iteration 8568: Achieved Loss=> 22.8477582923
Gradients: w=>0.0042281840, b=>-0.0729475856
Iteration 8569: Achieved Loss=> 22.8477529545
Gradients: w=>0.0042259362, b=>-0.0729088044
Iteration 8570: Achieved Loss=> 22.8477476224
Gradients: w=>0.0042236896, b=>-0.0728700439
Iteration 8571: Achieved Loss=> 22.8477422959
Gradients: w=>0.0042214441, b=>-0.0728313039
Iteration 8572: Achieved Loss=> 22.8477369751
Gradients: w=>0.0042191999, b=>-0.0727925846
Iteration 8573: Achieved Loss=> 22.8477316600
Gradients: w=>0.0042169568, b=>-0.0727538858
Iteration 8574: Achieved Loss=> 22.8477263505
Gradients: w=>0.0042147150, b=>-0.0727152076
Iteration 8575: Achieved Loss=> 22.8477210466
Gradients: w=>0.0042124743, b=>-0.0726765499
Iteration 8576: Achieved Loss=> 22.8477157484
Gradients: w=>0.0042102348, b=>-0.0726379129
Iteration 8577: Achieved Loss=> 22.8477104558
Gradients: w=>0.0042079965, b=>-0.0725992963
Iteration 8578: Achieved Loss=> 22.8477051688
Gradients: w=>0.0042057594, b=>-0.0725607003
Iteration 8579: Achieved Loss=> 22.8476998875
Gradients: w=>0.0042035235, b=>-0.0725221248
Iteration 8580: Achieved Loss=> 22.8476946118
Gradients: w=>0.0042012888, b=>-0.0724835698
Iteration 8581: Achieved Loss=> 22.8476893417
Gradients: w=>0.0041990552, b=>-0.0724450353
Iteration 8582: Achieved Loss=> 22.8476840771
Gradients: w=>0.0041968229, b=>-0.0724065213
Iteration 8583: Achieved Loss=> 22.8476788182
Gradients: w=>0.0041945917, b=>-0.0723680278
Iteration 8584: Achieved Loss=> 22.8476735649
Gradients: w=>0.0041923618, b=>-0.0723295547
Iteration 8585: Achieved Loss=> 22.8476683171
Gradients: w=>0.0041901330, b=>-0.0722911021
Iteration 8586: Achieved Loss=> 22.8476630750
Gradients: w=>0.0041879054, b=>-0.0722526699
Iteration 8587: Achieved Loss=> 22.8476578384
Gradients: w=>0.0041856790, b=>-0.0722142582
Iteration 8588: Achieved Loss=> 22.8476526074
Gradients: w=>0.0041834537, b=>-0.0721758669
Iteration 8589: Achieved Loss=> 22.8476473819
Gradients: w=>0.0041812297, b=>-0.0721374960
Iteration 8590: Achieved Loss=> 22.8476421620
Gradients: w=>0.0041790068, b=>-0.0720991455
Iteration 8591: Achieved Loss=> 22.8476369476
Gradients: w=>0.0041767851, b=>-0.0720608154
Iteration 8592: Achieved Loss=> 22.8476317388
Gradients: w=>0.0041745646, b=>-0.0720225056
Iteration 8593: Achieved Loss=> 22.8476265355
Gradients: w=>0.0041723453, b=>-0.0719842162
Iteration 8594: Achieved Loss=> 22.8476213378
Gradients: w=>0.0041701271, b=>-0.0719459472
Iteration 8595: Achieved Loss=> 22.8476161455
Gradients: w=>0.0041679102, b=>-0.0719076986
Iteration 8596: Achieved Loss=> 22.8476109588
Gradients: w=>0.0041656944, b=>-0.0718694702
Iteration 8597: Achieved Loss=> 22.8476057776
Gradients: w=>0.0041634798, b=>-0.0718312622
Iteration 8598: Achieved Loss=> 22.8476006019
Gradients: w=>0.0041612663, b=>-0.0717930745
Iteration 8599: Achieved Loss=> 22.8475954317
Gradients: w=>0.0041590541, b=>-0.0717549071
Iteration 8600: Achieved Loss=> 22.8475902670
Gradients: w=>0.0041568430, b=>-0.0717167600
Iteration 8601: Achieved Loss=> 22.8475851078
Gradients: w=>0.0041546331, b=>-0.0716786332
Iteration 8602: Achieved Loss=> 22.8475799541
Gradients: w=>0.0041524244, b=>-0.0716405266
Iteration 8603: Achieved Loss=> 22.8475748059
Gradients: w=>0.0041502168, b=>-0.0716024403
Iteration 8604: Achieved Loss=> 22.8475696631
Gradients: w=>0.0041480104, b=>-0.0715643742
Iteration 8605: Achieved Loss=> 22.8475645258
Gradients: w=>0.0041458052, b=>-0.0715263284
Iteration 8606: Achieved Loss=> 22.8475593940
Gradients: w=>0.0041436012, b=>-0.0714883028
Iteration 8607: Achieved Loss=> 22.8475542676
Gradients: w=>0.0041413983, b=>-0.0714502974
Iteration 8608: Achieved Loss=> 22.8475491467
Gradients: w=>0.0041391966, b=>-0.0714123123
Iteration 8609: Achieved Loss=> 22.8475440312
Gradients: w=>0.0041369961, b=>-0.0713743473
Iteration 8610: Achieved Loss=> 22.8475389211
Gradients: w=>0.0041347967, b=>-0.0713364025
Iteration 8611: Achieved Loss=> 22.8475338165
Gradients: w=>0.0041325985, b=>-0.0712984779
Iteration 8612: Achieved Loss=> 22.8475287173
Gradients: w=>0.0041304015, b=>-0.0712605734
Iteration 8613: Achieved Loss=> 22.8475236235
Gradients: w=>0.0041282057, b=>-0.0712226891
Iteration 8614: Achieved Loss=> 22.8475185352
Gradients: w=>0.0041260110, b=>-0.0711848249
Iteration 8615: Achieved Loss=> 22.8475134522
Gradients: w=>0.0041238175, b=>-0.0711469809
Iteration 8616: Achieved Loss=> 22.8475083747
Gradients: w=>0.0041216251, b=>-0.0711091570
Iteration 8617: Achieved Loss=> 22.8475033025
Gradients: w=>0.0041194340, b=>-0.0710713532
Iteration 8618: Achieved Loss=> 22.8474982358
Gradients: w=>0.0041172439, b=>-0.0710335695
Iteration 8619: Achieved Loss=> 22.8474931744
Gradients: w=>0.0041150551, b=>-0.0709958058
Iteration 8620: Achieved Loss=> 22.8474881184
Gradients: w=>0.0041128674, b=>-0.0709580623
Iteration 8621: Achieved Loss=> 22.8474830678
Gradients: w=>0.0041106809, b=>-0.0709203388
Iteration 8622: Achieved Loss=> 22.8474780225
Gradients: w=>0.0041084955, b=>-0.0708826354
Iteration 8623: Achieved Loss=> 22.8474729826
Gradients: w=>0.0041063113, b=>-0.0708449520
Iteration 8624: Achieved Loss=> 22.8474679481
Gradients: w=>0.0041041283, b=>-0.0708072886
Iteration 8625: Achieved Loss=> 22.8474629189
Gradients: w=>0.0041019464, b=>-0.0707696453
Iteration 8626: Achieved Loss=> 22.8474578951
Gradients: w=>0.0040997657, b=>-0.0707320220
Iteration 8627: Achieved Loss=> 22.8474528766
Gradients: w=>0.0040975861, b=>-0.0706944187
Iteration 8628: Achieved Loss=> 22.8474478634
Gradients: w=>0.0040954077, b=>-0.0706568354
Iteration 8629: Achieved Loss=> 22.8474428556
Gradients: w=>0.0040932304, b=>-0.0706192720
Iteration 8630: Achieved Loss=> 22.8474378531
Gradients: w=>0.0040910544, b=>-0.0705817286
Iteration 8631: Achieved Loss=> 22.8474328559
Gradients: w=>0.0040888794, b=>-0.0705442052
Iteration 8632: Achieved Loss=> 22.8474278640
Gradients: w=>0.0040867057, b=>-0.0705067018
Iteration 8633: Achieved Loss=> 22.8474228775
Gradients: w=>0.0040845330, b=>-0.0704692182
Iteration 8634: Achieved Loss=> 22.8474178962
Gradients: w=>0.0040823616, b=>-0.0704317546
Iteration 8635: Achieved Loss=> 22.8474129202
Gradients: w=>0.0040801913, b=>-0.0703943110
Iteration 8636: Achieved Loss=> 22.8474079495
Gradients: w=>0.0040780221, b=>-0.0703568872
Iteration 8637: Achieved Loss=> 22.8474029841
Gradients: w=>0.0040758541, b=>-0.0703194833
Iteration 8638: Achieved Loss=> 22.8473980240
Gradients: w=>0.0040736873, b=>-0.0702820993
Iteration 8639: Achieved Loss=> 22.8473930692
Gradients: w=>0.0040715216, b=>-0.0702447352
Iteration 8640: Achieved Loss=> 22.8473881196
Gradients: w=>0.0040693570, b=>-0.0702073909
Iteration 8641: Achieved Loss=> 22.8473831753
Gradients: w=>0.0040671936, b=>-0.0701700665
Iteration 8642: Achieved Loss=> 22.8473782362
Gradients: w=>0.0040650314, b=>-0.0701327620
Iteration 8643: Achieved Loss=> 22.8473733024
Gradients: w=>0.0040628703, b=>-0.0700954772
Iteration 8644: Achieved Loss=> 22.8473683738
Gradients: w=>0.0040607103, b=>-0.0700582123
Iteration 8645: Achieved Loss=> 22.8473634505
Gradients: w=>0.0040585515, b=>-0.0700209672
Iteration 8646: Achieved Loss=> 22.8473585324
Gradients: w=>0.0040563939, b=>-0.0699837419
Iteration 8647: Achieved Loss=> 22.8473536195
Gradients: w=>0.0040542374, b=>-0.0699465364
Iteration 8648: Achieved Loss=> 22.8473487118
Gradients: w=>0.0040520820, b=>-0.0699093507
Iteration 8649: Achieved Loss=> 22.8473438094
Gradients: w=>0.0040499278, b=>-0.0698721847
Iteration 8650: Achieved Loss=> 22.8473389122
Gradients: w=>0.0040477748, b=>-0.0698350385
Iteration 8651: Achieved Loss=> 22.8473340202
Gradients: w=>0.0040456228, b=>-0.0697979121
Iteration 8652: Achieved Loss=> 22.8473291334
Gradients: w=>0.0040434721, b=>-0.0697608054
Iteration 8653: Achieved Loss=> 22.8473242517
Gradients: w=>0.0040413224, b=>-0.0697237184
Iteration 8654: Achieved Loss=> 22.8473193753
Gradients: w=>0.0040391739, b=>-0.0696866511
Iteration 8655: Achieved Loss=> 22.8473145041
Gradients: w=>0.0040370266, b=>-0.0696496036
Iteration 8656: Achieved Loss=> 22.8473096380
Gradients: w=>0.0040348804, b=>-0.0696125757
Iteration 8657: Achieved Loss=> 22.8473047771
Gradients: w=>0.0040327353, b=>-0.0695755675
Iteration 8658: Achieved Loss=> 22.8472999214
Gradients: w=>0.0040305914, b=>-0.0695385790
Iteration 8659: Achieved Loss=> 22.8472950708
Gradients: w=>0.0040284486, b=>-0.0695016102
Iteration 8660: Achieved Loss=> 22.8472902254
Gradients: w=>0.0040263069, b=>-0.0694646610
Iteration 8661: Achieved Loss=> 22.8472853851
Gradients: w=>0.0040241664, b=>-0.0694277314
Iteration 8662: Achieved Loss=> 22.8472805500
Gradients: w=>0.0040220271, b=>-0.0693908215
Iteration 8663: Achieved Loss=> 22.8472757200
Gradients: w=>0.0040198888, b=>-0.0693539312
Iteration 8664: Achieved Loss=> 22.8472708952
Gradients: w=>0.0040177517, b=>-0.0693170606
Iteration 8665: Achieved Loss=> 22.8472660754
Gradients: w=>0.0040156158, b=>-0.0692802095
Iteration 8666: Achieved Loss=> 22.8472612609
Gradients: w=>0.0040134810, b=>-0.0692433780
Iteration 8667: Achieved Loss=> 22.8472564514
Gradients: w=>0.0040113473, b=>-0.0692065661
Iteration 8668: Achieved Loss=> 22.8472516470
Gradients: w=>0.0040092147, b=>-0.0691697737
Iteration 8669: Achieved Loss=> 22.8472468478
Gradients: w=>0.0040070833, b=>-0.0691330010
Iteration 8670: Achieved Loss=> 22.8472420536
Gradients: w=>0.0040049530, b=>-0.0690962477
Iteration 8671: Achieved Loss=> 22.8472372646
Gradients: w=>0.0040028238, b=>-0.0690595141
Iteration 8672: Achieved Loss=> 22.8472324806
Gradients: w=>0.0040006958, b=>-0.0690227999
Iteration 8673: Achieved Loss=> 22.8472277017
Gradients: w=>0.0039985689, b=>-0.0689861053
Iteration 8674: Achieved Loss=> 22.8472229279
Gradients: w=>0.0039964432, b=>-0.0689494301
Iteration 8675: Achieved Loss=> 22.8472181592
Gradients: w=>0.0039943185, b=>-0.0689127745
Iteration 8676: Achieved Loss=> 22.8472133955
Gradients: w=>0.0039921950, b=>-0.0688761384
Iteration 8677: Achieved Loss=> 22.8472086369
Gradients: w=>0.0039900727, b=>-0.0688395217
Iteration 8678: Achieved Loss=> 22.8472038834
Gradients: w=>0.0039879514, b=>-0.0688029245
Iteration 8679: Achieved Loss=> 22.8471991349
Gradients: w=>0.0039858313, b=>-0.0687663467
Iteration 8680: Achieved Loss=> 22.8471943915
Gradients: w=>0.0039837123, b=>-0.0687297884
Iteration 8681: Achieved Loss=> 22.8471896531
Gradients: w=>0.0039815944, b=>-0.0686932496
Iteration 8682: Achieved Loss=> 22.8471849197
Gradients: w=>0.0039794777, b=>-0.0686567301
Iteration 8683: Achieved Loss=> 22.8471801914
Gradients: w=>0.0039773621, b=>-0.0686202301
Iteration 8684: Achieved Loss=> 22.8471754681
Gradients: w=>0.0039752476, b=>-0.0685837495
Iteration 8685: Achieved Loss=> 22.8471707498
Gradients: w=>0.0039731342, b=>-0.0685472883
Iteration 8686: Achieved Loss=> 22.8471660365
Gradients: w=>0.0039710220, b=>-0.0685108464
Iteration 8687: Achieved Loss=> 22.8471613283
Gradients: w=>0.0039689109, b=>-0.0684744239
Iteration 8688: Achieved Loss=> 22.8471566250
Gradients: w=>0.0039668009, b=>-0.0684380208
Iteration 8689: Achieved Loss=> 22.8471519268
Gradients: w=>0.0039646920, b=>-0.0684016371
Iteration 8690: Achieved Loss=> 22.8471472335
Gradients: w=>0.0039625843, b=>-0.0683652727
Iteration 8691: Achieved Loss=> 22.8471425453
Gradients: w=>0.0039604776, b=>-0.0683289276
Iteration 8692: Achieved Loss=> 22.8471378620
Gradients: w=>0.0039583721, b=>-0.0682926018
Iteration 8693: Achieved Loss=> 22.8471331837
Gradients: w=>0.0039562677, b=>-0.0682562954
Iteration 8694: Achieved Loss=> 22.8471285104
Gradients: w=>0.0039541645, b=>-0.0682200083
Iteration 8695: Achieved Loss=> 22.8471238420
Gradients: w=>0.0039520623, b=>-0.0681837404
Iteration 8696: Achieved Loss=> 22.8471191786
Gradients: w=>0.0039499613, b=>-0.0681474918
Iteration 8697: Achieved Loss=> 22.8471145201
Gradients: w=>0.0039478613, b=>-0.0681112625
Iteration 8698: Achieved Loss=> 22.8471098667
Gradients: w=>0.0039457625, b=>-0.0680750525
Iteration 8699: Achieved Loss=> 22.8471052181
Gradients: w=>0.0039436648, b=>-0.0680388617
Iteration 8700: Achieved Loss=> 22.8471005745
Gradients: w=>0.0039415683, b=>-0.0680026902
Iteration 8701: Achieved Loss=> 22.8470959358
Gradients: w=>0.0039394728, b=>-0.0679665379
Iteration 8702: Achieved Loss=> 22.8470913021
Gradients: w=>0.0039373785, b=>-0.0679304048
Iteration 8703: Achieved Loss=> 22.8470866733
Gradients: w=>0.0039352852, b=>-0.0678942909
Iteration 8704: Achieved Loss=> 22.8470820494
Gradients: w=>0.0039331931, b=>-0.0678581962
Iteration 8705: Achieved Loss=> 22.8470774304
Gradients: w=>0.0039311021, b=>-0.0678221207
Iteration 8706: Achieved Loss=> 22.8470728164
Gradients: w=>0.0039290122, b=>-0.0677860644
Iteration 8707: Achieved Loss=> 22.8470682072
Gradients: w=>0.0039269234, b=>-0.0677500272
Iteration 8708: Achieved Loss=> 22.8470636029
Gradients: w=>0.0039248358, b=>-0.0677140092
Iteration 8709: Achieved Loss=> 22.8470590036
Gradients: w=>0.0039227492, b=>-0.0676780104
Iteration 8710: Achieved Loss=> 22.8470544091
Gradients: w=>0.0039206637, b=>-0.0676420307
Iteration 8711: Achieved Loss=> 22.8470498195
Gradients: w=>0.0039185794, b=>-0.0676060701
Iteration 8712: Achieved Loss=> 22.8470452348
Gradients: w=>0.0039164962, b=>-0.0675701286
Iteration 8713: Achieved Loss=> 22.8470406549
Gradients: w=>0.0039144140, b=>-0.0675342063
Iteration 8714: Achieved Loss=> 22.8470360799
Gradients: w=>0.0039123330, b=>-0.0674983030
Iteration 8715: Achieved Loss=> 22.8470315098
Gradients: w=>0.0039102531, b=>-0.0674624188
Iteration 8716: Achieved Loss=> 22.8470269446
Gradients: w=>0.0039081743, b=>-0.0674265537
Iteration 8717: Achieved Loss=> 22.8470223842
Gradients: w=>0.0039060966, b=>-0.0673907077
Iteration 8718: Achieved Loss=> 22.8470178286
Gradients: w=>0.0039040200, b=>-0.0673548807
Iteration 8719: Achieved Loss=> 22.8470132779
Gradients: w=>0.0039019445, b=>-0.0673190728
Iteration 8720: Achieved Loss=> 22.8470087320
Gradients: w=>0.0038998701, b=>-0.0672832839
Iteration 8721: Achieved Loss=> 22.8470041910
Gradients: w=>0.0038977968, b=>-0.0672475141
Iteration 8722: Achieved Loss=> 22.8469996548
Gradients: w=>0.0038957246, b=>-0.0672117632
Iteration 8723: Achieved Loss=> 22.8469951234
Gradients: w=>0.0038936535, b=>-0.0671760314
Iteration 8724: Achieved Loss=> 22.8469905968
Gradients: w=>0.0038915835, b=>-0.0671403186
Iteration 8725: Achieved Loss=> 22.8469860751
Gradients: w=>0.0038895147, b=>-0.0671046247
Iteration 8726: Achieved Loss=> 22.8469815581
Gradients: w=>0.0038874469, b=>-0.0670689498
Iteration 8727: Achieved Loss=> 22.8469770459
Gradients: w=>0.0038853802, b=>-0.0670332939
Iteration 8728: Achieved Loss=> 22.8469725386
Gradients: w=>0.0038833146, b=>-0.0669976570
Iteration 8729: Achieved Loss=> 22.8469680360
Gradients: w=>0.0038812501, b=>-0.0669620389
Iteration 8730: Achieved Loss=> 22.8469635382
Gradients: w=>0.0038791867, b=>-0.0669264399
Iteration 8731: Achieved Loss=> 22.8469590452
Gradients: w=>0.0038771244, b=>-0.0668908597
Iteration 8732: Achieved Loss=> 22.8469545570
Gradients: w=>0.0038750632, b=>-0.0668552985
Iteration 8733: Achieved Loss=> 22.8469500735
Gradients: w=>0.0038730031, b=>-0.0668197562
Iteration 8734: Achieved Loss=> 22.8469455949
Gradients: w=>0.0038709441, b=>-0.0667842327
Iteration 8735: Achieved Loss=> 22.8469411209
Gradients: w=>0.0038688862, b=>-0.0667487282
Iteration 8736: Achieved Loss=> 22.8469366518
Gradients: w=>0.0038668294, b=>-0.0667132425
Iteration 8737: Achieved Loss=> 22.8469321873
Gradients: w=>0.0038647737, b=>-0.0666777757
Iteration 8738: Achieved Loss=> 22.8469277277
Gradients: w=>0.0038627190, b=>-0.0666423277
Iteration 8739: Achieved Loss=> 22.8469232727
Gradients: w=>0.0038606655, b=>-0.0666068986
Iteration 8740: Achieved Loss=> 22.8469188225
Gradients: w=>0.0038586130, b=>-0.0665714884
Iteration 8741: Achieved Loss=> 22.8469143770
Gradients: w=>0.0038565617, b=>-0.0665360969
Iteration 8742: Achieved Loss=> 22.8469099363
Gradients: w=>0.0038545114, b=>-0.0665007243
Iteration 8743: Achieved Loss=> 22.8469055003
Gradients: w=>0.0038524622, b=>-0.0664653705
Iteration 8744: Achieved Loss=> 22.8469010690
Gradients: w=>0.0038504141, b=>-0.0664300354
Iteration 8745: Achieved Loss=> 22.8468966424
Gradients: w=>0.0038483671, b=>-0.0663947192
Iteration 8746: Achieved Loss=> 22.8468922205
Gradients: w=>0.0038463212, b=>-0.0663594217
Iteration 8747: Achieved Loss=> 22.8468878033
Gradients: w=>0.0038442764, b=>-0.0663241430
Iteration 8748: Achieved Loss=> 22.8468833908
Gradients: w=>0.0038422327, b=>-0.0662888831
Iteration 8749: Achieved Loss=> 22.8468789830
Gradients: w=>0.0038401900, b=>-0.0662536419
Iteration 8750: Achieved Loss=> 22.8468745799
Gradients: w=>0.0038381485, b=>-0.0662184194
Iteration 8751: Achieved Loss=> 22.8468701814
Gradients: w=>0.0038361080, b=>-0.0661832156
Iteration 8752: Achieved Loss=> 22.8468657877
Gradients: w=>0.0038340686, b=>-0.0661480306
Iteration 8753: Achieved Loss=> 22.8468613986
Gradients: w=>0.0038320303, b=>-0.0661128643
Iteration 8754: Achieved Loss=> 22.8468570141
Gradients: w=>0.0038299931, b=>-0.0660777167
Iteration 8755: Achieved Loss=> 22.8468526344
Gradients: w=>0.0038279569, b=>-0.0660425877
Iteration 8756: Achieved Loss=> 22.8468482592
Gradients: w=>0.0038259219, b=>-0.0660074775
Iteration 8757: Achieved Loss=> 22.8468438888
Gradients: w=>0.0038238879, b=>-0.0659723859
Iteration 8758: Achieved Loss=> 22.8468395230
Gradients: w=>0.0038218550, b=>-0.0659373129
Iteration 8759: Achieved Loss=> 22.8468351618
Gradients: w=>0.0038198232, b=>-0.0659022586
Iteration 8760: Achieved Loss=> 22.8468308052
Gradients: w=>0.0038177924, b=>-0.0658672229
Iteration 8761: Achieved Loss=> 22.8468264533
Gradients: w=>0.0038157628, b=>-0.0658322059
Iteration 8762: Achieved Loss=> 22.8468221061
Gradients: w=>0.0038137342, b=>-0.0657972075
Iteration 8763: Achieved Loss=> 22.8468177634
Gradients: w=>0.0038117067, b=>-0.0657622277
Iteration 8764: Achieved Loss=> 22.8468134253
Gradients: w=>0.0038096803, b=>-0.0657272665
Iteration 8765: Achieved Loss=> 22.8468090919
Gradients: w=>0.0038076549, b=>-0.0656923238
Iteration 8766: Achieved Loss=> 22.8468047631
Gradients: w=>0.0038056307, b=>-0.0656573998
Iteration 8767: Achieved Loss=> 22.8468004389
Gradients: w=>0.0038036075, b=>-0.0656224943
Iteration 8768: Achieved Loss=> 22.8467961192
Gradients: w=>0.0038015854, b=>-0.0655876073
Iteration 8769: Achieved Loss=> 22.8467918042
Gradients: w=>0.0037995643, b=>-0.0655527390
Iteration 8770: Achieved Loss=> 22.8467874937
Gradients: w=>0.0037975444, b=>-0.0655178891
Iteration 8771: Achieved Loss=> 22.8467831879
Gradients: w=>0.0037955255, b=>-0.0654830578
Iteration 8772: Achieved Loss=> 22.8467788866
Gradients: w=>0.0037935077, b=>-0.0654482450
Iteration 8773: Achieved Loss=> 22.8467745899
Gradients: w=>0.0037914909, b=>-0.0654134507
Iteration 8774: Achieved Loss=> 22.8467702977
Gradients: w=>0.0037894752, b=>-0.0653786749
Iteration 8775: Achieved Loss=> 22.8467660101
Gradients: w=>0.0037874606, b=>-0.0653439176
Iteration 8776: Achieved Loss=> 22.8467617271
Gradients: w=>0.0037854471, b=>-0.0653091787
Iteration 8777: Achieved Loss=> 22.8467574486
Gradients: w=>0.0037834346, b=>-0.0652744584
Iteration 8778: Achieved Loss=> 22.8467531747
Gradients: w=>0.0037814233, b=>-0.0652397565
Iteration 8779: Achieved Loss=> 22.8467489053
Gradients: w=>0.0037794129, b=>-0.0652050730
Iteration 8780: Achieved Loss=> 22.8467446404
Gradients: w=>0.0037774037, b=>-0.0651704080
Iteration 8781: Achieved Loss=> 22.8467403801
Gradients: w=>0.0037753955, b=>-0.0651357614
Iteration 8782: Achieved Loss=> 22.8467361243
Gradients: w=>0.0037733884, b=>-0.0651011332
Iteration 8783: Achieved Loss=> 22.8467318730
Gradients: w=>0.0037713823, b=>-0.0650665235
Iteration 8784: Achieved Loss=> 22.8467276263
Gradients: w=>0.0037693774, b=>-0.0650319321
Iteration 8785: Achieved Loss=> 22.8467233841
Gradients: w=>0.0037673734, b=>-0.0649973591
Iteration 8786: Achieved Loss=> 22.8467191463
Gradients: w=>0.0037653706, b=>-0.0649628046
Iteration 8787: Achieved Loss=> 22.8467149131
Gradients: w=>0.0037633688, b=>-0.0649282683
Iteration 8788: Achieved Loss=> 22.8467106844
Gradients: w=>0.0037613681, b=>-0.0648937505
Iteration 8789: Achieved Loss=> 22.8467064602
Gradients: w=>0.0037593684, b=>-0.0648592510
Iteration 8790: Achieved Loss=> 22.8467022404
Gradients: w=>0.0037573698, b=>-0.0648247698
Iteration 8791: Achieved Loss=> 22.8466980252
Gradients: w=>0.0037553723, b=>-0.0647903070
Iteration 8792: Achieved Loss=> 22.8466938144
Gradients: w=>0.0037533758, b=>-0.0647558624
Iteration 8793: Achieved Loss=> 22.8466896081
Gradients: w=>0.0037513804, b=>-0.0647214362
Iteration 8794: Achieved Loss=> 22.8466854063
Gradients: w=>0.0037493861, b=>-0.0646870283
Iteration 8795: Achieved Loss=> 22.8466812090
Gradients: w=>0.0037473928, b=>-0.0646526387
Iteration 8796: Achieved Loss=> 22.8466770161
Gradients: w=>0.0037454005, b=>-0.0646182674
Iteration 8797: Achieved Loss=> 22.8466728276
Gradients: w=>0.0037434094, b=>-0.0645839143
Iteration 8798: Achieved Loss=> 22.8466686437
Gradients: w=>0.0037414193, b=>-0.0645495796
Iteration 8799: Achieved Loss=> 22.8466644641
Gradients: w=>0.0037394302, b=>-0.0645152630
Iteration 8800: Achieved Loss=> 22.8466602890
Gradients: w=>0.0037374422, b=>-0.0644809647
Iteration 8801: Achieved Loss=> 22.8466561184
Gradients: w=>0.0037354553, b=>-0.0644466847
Iteration 8802: Achieved Loss=> 22.8466519522
Gradients: w=>0.0037334694, b=>-0.0644124228
Iteration 8803: Achieved Loss=> 22.8466477904
Gradients: w=>0.0037314846, b=>-0.0643781792
Iteration 8804: Achieved Loss=> 22.8466436330
Gradients: w=>0.0037295008, b=>-0.0643439538
Iteration 8805: Achieved Loss=> 22.8466394800
Gradients: w=>0.0037275181, b=>-0.0643097466
Iteration 8806: Achieved Loss=> 22.8466353315
Gradients: w=>0.0037255364, b=>-0.0642755575
Iteration 8807: Achieved Loss=> 22.8466311874
Gradients: w=>0.0037235558, b=>-0.0642413867
Iteration 8808: Achieved Loss=> 22.8466270477
Gradients: w=>0.0037215762, b=>-0.0642072340
Iteration 8809: Achieved Loss=> 22.8466229123
Gradients: w=>0.0037195977, b=>-0.0641730994
Iteration 8810: Achieved Loss=> 22.8466187814
Gradients: w=>0.0037176203, b=>-0.0641389831
Iteration 8811: Achieved Loss=> 22.8466146549
Gradients: w=>0.0037156439, b=>-0.0641048848
Iteration 8812: Achieved Loss=> 22.8466105327
Gradients: w=>0.0037136685, b=>-0.0640708047
Iteration 8813: Achieved Loss=> 22.8466064150
Gradients: w=>0.0037116942, b=>-0.0640367427
Iteration 8814: Achieved Loss=> 22.8466023016
Gradients: w=>0.0037097210, b=>-0.0640026988
Iteration 8815: Achieved Loss=> 22.8465981926
Gradients: w=>0.0037077488, b=>-0.0639686730
Iteration 8816: Achieved Loss=> 22.8465940879
Gradients: w=>0.0037057776, b=>-0.0639346653
Iteration 8817: Achieved Loss=> 22.8465899876
Gradients: w=>0.0037038075, b=>-0.0639006756
Iteration 8818: Achieved Loss=> 22.8465858917
Gradients: w=>0.0037018385, b=>-0.0638667041
Iteration 8819: Achieved Loss=> 22.8465818001
Gradients: w=>0.0036998705, b=>-0.0638327506
Iteration 8820: Achieved Loss=> 22.8465777129
Gradients: w=>0.0036979035, b=>-0.0637988151
Iteration 8821: Achieved Loss=> 22.8465736300
Gradients: w=>0.0036959376, b=>-0.0637648977
Iteration 8822: Achieved Loss=> 22.8465695515
Gradients: w=>0.0036939727, b=>-0.0637309983
Iteration 8823: Achieved Loss=> 22.8465654773
Gradients: w=>0.0036920089, b=>-0.0636971170
Iteration 8824: Achieved Loss=> 22.8465614074
Gradients: w=>0.0036900461, b=>-0.0636632536
Iteration 8825: Achieved Loss=> 22.8465573419
Gradients: w=>0.0036880843, b=>-0.0636294083
Iteration 8826: Achieved Loss=> 22.8465532807
Gradients: w=>0.0036861236, b=>-0.0635955810
Iteration 8827: Achieved Loss=> 22.8465492238
Gradients: w=>0.0036841640, b=>-0.0635617716
Iteration 8828: Achieved Loss=> 22.8465451712
Gradients: w=>0.0036822054, b=>-0.0635279802
Iteration 8829: Achieved Loss=> 22.8465411229
Gradients: w=>0.0036802478, b=>-0.0634942068
Iteration 8830: Achieved Loss=> 22.8465370789
Gradients: w=>0.0036782913, b=>-0.0634604513
Iteration 8831: Achieved Loss=> 22.8465330392
Gradients: w=>0.0036763358, b=>-0.0634267138
Iteration 8832: Achieved Loss=> 22.8465290038
Gradients: w=>0.0036743813, b=>-0.0633929942
Iteration 8833: Achieved Loss=> 22.8465249727
Gradients: w=>0.0036724279, b=>-0.0633592925
Iteration 8834: Achieved Loss=> 22.8465209459
Gradients: w=>0.0036704755, b=>-0.0633256088
Iteration 8835: Achieved Loss=> 22.8465169234
Gradients: w=>0.0036685242, b=>-0.0632919430
Iteration 8836: Achieved Loss=> 22.8465129051
Gradients: w=>0.0036665739, b=>-0.0632582950
Iteration 8837: Achieved Loss=> 22.8465088911
Gradients: w=>0.0036646246, b=>-0.0632246650
Iteration 8838: Achieved Loss=> 22.8465048814
Gradients: w=>0.0036626764, b=>-0.0631910528
Iteration 8839: Achieved Loss=> 22.8465008759
Gradients: w=>0.0036607292, b=>-0.0631574585
Iteration 8840: Achieved Loss=> 22.8464968747
Gradients: w=>0.0036587831, b=>-0.0631238820
Iteration 8841: Achieved Loss=> 22.8464928778
Gradients: w=>0.0036568379, b=>-0.0630903235
Iteration 8842: Achieved Loss=> 22.8464888851
Gradients: w=>0.0036548939, b=>-0.0630567827
Iteration 8843: Achieved Loss=> 22.8464848966
Gradients: w=>0.0036529508, b=>-0.0630232598
Iteration 8844: Achieved Loss=> 22.8464809124
Gradients: w=>0.0036510088, b=>-0.0629897547
Iteration 8845: Achieved Loss=> 22.8464769324
Gradients: w=>0.0036490678, b=>-0.0629562674
Iteration 8846: Achieved Loss=> 22.8464729567
Gradients: w=>0.0036471278, b=>-0.0629227979
Iteration 8847: Achieved Loss=> 22.8464689852
Gradients: w=>0.0036451889, b=>-0.0628893462
Iteration 8848: Achieved Loss=> 22.8464650179
Gradients: w=>0.0036432510, b=>-0.0628559123
Iteration 8849: Achieved Loss=> 22.8464610548
Gradients: w=>0.0036413141, b=>-0.0628224962
Iteration 8850: Achieved Loss=> 22.8464570959
Gradients: w=>0.0036393783, b=>-0.0627890978
Iteration 8851: Achieved Loss=> 22.8464531412
Gradients: w=>0.0036374435, b=>-0.0627557172
Iteration 8852: Achieved Loss=> 22.8464491908
Gradients: w=>0.0036355097, b=>-0.0627223543
Iteration 8853: Achieved Loss=> 22.8464452445
Gradients: w=>0.0036335770, b=>-0.0626890092
Iteration 8854: Achieved Loss=> 22.8464413025
Gradients: w=>0.0036316453, b=>-0.0626556818
Iteration 8855: Achieved Loss=> 22.8464373646
Gradients: w=>0.0036297146, b=>-0.0626223721
Iteration 8856: Achieved Loss=> 22.8464334309
Gradients: w=>0.0036277849, b=>-0.0625890801
Iteration 8857: Achieved Loss=> 22.8464295014
Gradients: w=>0.0036258563, b=>-0.0625558059
Iteration 8858: Achieved Loss=> 22.8464255760
Gradients: w=>0.0036239286, b=>-0.0625225493
Iteration 8859: Achieved Loss=> 22.8464216549
Gradients: w=>0.0036220020, b=>-0.0624893104
Iteration 8860: Achieved Loss=> 22.8464177379
Gradients: w=>0.0036200765, b=>-0.0624560891
Iteration 8861: Achieved Loss=> 22.8464138251
Gradients: w=>0.0036181519, b=>-0.0624228856
Iteration 8862: Achieved Loss=> 22.8464099164
Gradients: w=>0.0036162284, b=>-0.0623896996
Iteration 8863: Achieved Loss=> 22.8464060119
Gradients: w=>0.0036143059, b=>-0.0623565314
Iteration 8864: Achieved Loss=> 22.8464021115
Gradients: w=>0.0036123844, b=>-0.0623233807
Iteration 8865: Achieved Loss=> 22.8463982153
Gradients: w=>0.0036104640, b=>-0.0622902477
Iteration 8866: Achieved Loss=> 22.8463943232
Gradients: w=>0.0036085446, b=>-0.0622571323
Iteration 8867: Achieved Loss=> 22.8463904353
Gradients: w=>0.0036066261, b=>-0.0622240345
Iteration 8868: Achieved Loss=> 22.8463865515
Gradients: w=>0.0036047087, b=>-0.0621909543
Iteration 8869: Achieved Loss=> 22.8463826718
Gradients: w=>0.0036027924, b=>-0.0621578917
Iteration 8870: Achieved Loss=> 22.8463787963
Gradients: w=>0.0036008770, b=>-0.0621248466
Iteration 8871: Achieved Loss=> 22.8463749248
Gradients: w=>0.0035989627, b=>-0.0620918191
Iteration 8872: Achieved Loss=> 22.8463710575
Gradients: w=>0.0035970494, b=>-0.0620588092
Iteration 8873: Achieved Loss=> 22.8463671943
Gradients: w=>0.0035951371, b=>-0.0620258169
Iteration 8874: Achieved Loss=> 22.8463633352
Gradients: w=>0.0035932258, b=>-0.0619928420
Iteration 8875: Achieved Loss=> 22.8463594802
Gradients: w=>0.0035913155, b=>-0.0619598847
Iteration 8876: Achieved Loss=> 22.8463556293
Gradients: w=>0.0035894063, b=>-0.0619269450
Iteration 8877: Achieved Loss=> 22.8463517825
Gradients: w=>0.0035874980, b=>-0.0618940227
Iteration 8878: Achieved Loss=> 22.8463479398
Gradients: w=>0.0035855908, b=>-0.0618611179
Iteration 8879: Achieved Loss=> 22.8463441011
Gradients: w=>0.0035836846, b=>-0.0618282307
Iteration 8880: Achieved Loss=> 22.8463402666
Gradients: w=>0.0035817794, b=>-0.0617953609
Iteration 8881: Achieved Loss=> 22.8463364361
Gradients: w=>0.0035798752, b=>-0.0617625086
Iteration 8882: Achieved Loss=> 22.8463326097
Gradients: w=>0.0035779720, b=>-0.0617296737
Iteration 8883: Achieved Loss=> 22.8463287874
Gradients: w=>0.0035760699, b=>-0.0616968563
Iteration 8884: Achieved Loss=> 22.8463249691
Gradients: w=>0.0035741687, b=>-0.0616640564
Iteration 8885: Achieved Loss=> 22.8463211549
Gradients: w=>0.0035722686, b=>-0.0616312739
Iteration 8886: Achieved Loss=> 22.8463173447
Gradients: w=>0.0035703695, b=>-0.0615985088
Iteration 8887: Achieved Loss=> 22.8463135386
Gradients: w=>0.0035684713, b=>-0.0615657611
Iteration 8888: Achieved Loss=> 22.8463097365
Gradients: w=>0.0035665742, b=>-0.0615330309
Iteration 8889: Achieved Loss=> 22.8463059385
Gradients: w=>0.0035646781, b=>-0.0615003180
Iteration 8890: Achieved Loss=> 22.8463021445
Gradients: w=>0.0035627830, b=>-0.0614676226
Iteration 8891: Achieved Loss=> 22.8462983546
Gradients: w=>0.0035608890, b=>-0.0614349445
Iteration 8892: Achieved Loss=> 22.8462945686
Gradients: w=>0.0035589959, b=>-0.0614022838
Iteration 8893: Achieved Loss=> 22.8462907867
Gradients: w=>0.0035571038, b=>-0.0613696405
Iteration 8894: Achieved Loss=> 22.8462870089
Gradients: w=>0.0035552127, b=>-0.0613370145
Iteration 8895: Achieved Loss=> 22.8462832350
Gradients: w=>0.0035533227, b=>-0.0613044059
Iteration 8896: Achieved Loss=> 22.8462794651
Gradients: w=>0.0035514336, b=>-0.0612718145
Iteration 8897: Achieved Loss=> 22.8462756993
Gradients: w=>0.0035495456, b=>-0.0612392406
Iteration 8898: Achieved Loss=> 22.8462719374
Gradients: w=>0.0035476585, b=>-0.0612066839
Iteration 8899: Achieved Loss=> 22.8462681796
Gradients: w=>0.0035457725, b=>-0.0611741446
Iteration 8900: Achieved Loss=> 22.8462644257
Gradients: w=>0.0035438874, b=>-0.0611416225
Iteration 8901: Achieved Loss=> 22.8462606759
Gradients: w=>0.0035420034, b=>-0.0611091177
Iteration 8902: Achieved Loss=> 22.8462569300
Gradients: w=>0.0035401204, b=>-0.0610766303
Iteration 8903: Achieved Loss=> 22.8462531881
Gradients: w=>0.0035382383, b=>-0.0610441600
Iteration 8904: Achieved Loss=> 22.8462494502
Gradients: w=>0.0035363573, b=>-0.0610117071
Iteration 8905: Achieved Loss=> 22.8462457163
Gradients: w=>0.0035344773, b=>-0.0609792714
Iteration 8906: Achieved Loss=> 22.8462419863
Gradients: w=>0.0035325982, b=>-0.0609468529
Iteration 8907: Achieved Loss=> 22.8462382603
Gradients: w=>0.0035307202, b=>-0.0609144517
Iteration 8908: Achieved Loss=> 22.8462345382
Gradients: w=>0.0035288431, b=>-0.0608820677
Iteration 8909: Achieved Loss=> 22.8462308201
Gradients: w=>0.0035269671, b=>-0.0608497010
Iteration 8910: Achieved Loss=> 22.8462271060
Gradients: w=>0.0035250921, b=>-0.0608173514
Iteration 8911: Achieved Loss=> 22.8462233958
Gradients: w=>0.0035232180, b=>-0.0607850190
Iteration 8912: Achieved Loss=> 22.8462196896
Gradients: w=>0.0035213450, b=>-0.0607527038
Iteration 8913: Achieved Loss=> 22.8462159873
Gradients: w=>0.0035194729, b=>-0.0607204058
Iteration 8914: Achieved Loss=> 22.8462122889
Gradients: w=>0.0035176018, b=>-0.0606881250
Iteration 8915: Achieved Loss=> 22.8462085945
Gradients: w=>0.0035157318, b=>-0.0606558613
Iteration 8916: Achieved Loss=> 22.8462049039
Gradients: w=>0.0035138627, b=>-0.0606236148
Iteration 8917: Achieved Loss=> 22.8462012174
Gradients: w=>0.0035119946, b=>-0.0605913854
Iteration 8918: Achieved Loss=> 22.8461975347
Gradients: w=>0.0035101275, b=>-0.0605591732
Iteration 8919: Achieved Loss=> 22.8461938559
Gradients: w=>0.0035082615, b=>-0.0605269781
Iteration 8920: Achieved Loss=> 22.8461901811
Gradients: w=>0.0035063964, b=>-0.0604948001
Iteration 8921: Achieved Loss=> 22.8461865101
Gradients: w=>0.0035045322, b=>-0.0604626392
Iteration 8922: Achieved Loss=> 22.8461828431
Gradients: w=>0.0035026691, b=>-0.0604304954
Iteration 8923: Achieved Loss=> 22.8461791800
Gradients: w=>0.0035008070, b=>-0.0603983687
Iteration 8924: Achieved Loss=> 22.8461755207
Gradients: w=>0.0034989459, b=>-0.0603662591
Iteration 8925: Achieved Loss=> 22.8461718654
Gradients: w=>0.0034970857, b=>-0.0603341665
Iteration 8926: Achieved Loss=> 22.8461682139
Gradients: w=>0.0034952266, b=>-0.0603020910
Iteration 8927: Achieved Loss=> 22.8461645663
Gradients: w=>0.0034933684, b=>-0.0602700326
Iteration 8928: Achieved Loss=> 22.8461609226
Gradients: w=>0.0034915112, b=>-0.0602379912
Iteration 8929: Achieved Loss=> 22.8461572828
Gradients: w=>0.0034896550, b=>-0.0602059668
Iteration 8930: Achieved Loss=> 22.8461536468
Gradients: w=>0.0034877998, b=>-0.0601739595
Iteration 8931: Achieved Loss=> 22.8461500147
Gradients: w=>0.0034859456, b=>-0.0601419691
Iteration 8932: Achieved Loss=> 22.8461463864
Gradients: w=>0.0034840923, b=>-0.0601099958
Iteration 8933: Achieved Loss=> 22.8461427620
Gradients: w=>0.0034822401, b=>-0.0600780395
Iteration 8934: Achieved Loss=> 22.8461391415
Gradients: w=>0.0034803888, b=>-0.0600461002
Iteration 8935: Achieved Loss=> 22.8461355248
Gradients: w=>0.0034785386, b=>-0.0600141778
Iteration 8936: Achieved Loss=> 22.8461319120
Gradients: w=>0.0034766893, b=>-0.0599822725
Iteration 8937: Achieved Loss=> 22.8461283030
Gradients: w=>0.0034748409, b=>-0.0599503840
Iteration 8938: Achieved Loss=> 22.8461246978
Gradients: w=>0.0034729936, b=>-0.0599185126
Iteration 8939: Achieved Loss=> 22.8461210965
Gradients: w=>0.0034711473, b=>-0.0598866581
Iteration 8940: Achieved Loss=> 22.8461174990
Gradients: w=>0.0034693019, b=>-0.0598548205
Iteration 8941: Achieved Loss=> 22.8461139053
Gradients: w=>0.0034674575, b=>-0.0598229998
Iteration 8942: Achieved Loss=> 22.8461103154
Gradients: w=>0.0034656141, b=>-0.0597911961
Iteration 8943: Achieved Loss=> 22.8461067294
Gradients: w=>0.0034637717, b=>-0.0597594092
Iteration 8944: Achieved Loss=> 22.8461031472
Gradients: w=>0.0034619302, b=>-0.0597276393
Iteration 8945: Achieved Loss=> 22.8460995687
Gradients: w=>0.0034600898, b=>-0.0596958862
Iteration 8946: Achieved Loss=> 22.8460959941
Gradients: w=>0.0034582503, b=>-0.0596641501
Iteration 8947: Achieved Loss=> 22.8460924233
Gradients: w=>0.0034564117, b=>-0.0596324308
Iteration 8948: Achieved Loss=> 22.8460888563
Gradients: w=>0.0034545742, b=>-0.0596007284
Iteration 8949: Achieved Loss=> 22.8460852930
Gradients: w=>0.0034527377, b=>-0.0595690428
Iteration 8950: Achieved Loss=> 22.8460817336
Gradients: w=>0.0034509021, b=>-0.0595373740
Iteration 8951: Achieved Loss=> 22.8460781779
Gradients: w=>0.0034490675, b=>-0.0595057222
Iteration 8952: Achieved Loss=> 22.8460746261
Gradients: w=>0.0034472338, b=>-0.0594740871
Iteration 8953: Achieved Loss=> 22.8460710779
Gradients: w=>0.0034454012, b=>-0.0594424688
Iteration 8954: Achieved Loss=> 22.8460675336
Gradients: w=>0.0034435695, b=>-0.0594108674
Iteration 8955: Achieved Loss=> 22.8460639930
Gradients: w=>0.0034417388, b=>-0.0593792828
Iteration 8956: Achieved Loss=> 22.8460604562
Gradients: w=>0.0034399091, b=>-0.0593477149
Iteration 8957: Achieved Loss=> 22.8460569232
Gradients: w=>0.0034380803, b=>-0.0593161638
Iteration 8958: Achieved Loss=> 22.8460533939
Gradients: w=>0.0034362525, b=>-0.0592846295
Iteration 8959: Achieved Loss=> 22.8460498684
Gradients: w=>0.0034344257, b=>-0.0592531120
Iteration 8960: Achieved Loss=> 22.8460463466
Gradients: w=>0.0034325998, b=>-0.0592216112
Iteration 8961: Achieved Loss=> 22.8460428285
Gradients: w=>0.0034307750, b=>-0.0591901272
Iteration 8962: Achieved Loss=> 22.8460393142
Gradients: w=>0.0034289511, b=>-0.0591586599
Iteration 8963: Achieved Loss=> 22.8460358037
Gradients: w=>0.0034271281, b=>-0.0591272094
Iteration 8964: Achieved Loss=> 22.8460322968
Gradients: w=>0.0034253062, b=>-0.0590957755
Iteration 8965: Achieved Loss=> 22.8460287937
Gradients: w=>0.0034234852, b=>-0.0590643584
Iteration 8966: Achieved Loss=> 22.8460252943
Gradients: w=>0.0034216651, b=>-0.0590329580
Iteration 8967: Achieved Loss=> 22.8460217986
Gradients: w=>0.0034198461, b=>-0.0590015742
Iteration 8968: Achieved Loss=> 22.8460183067
Gradients: w=>0.0034180280, b=>-0.0589702072
Iteration 8969: Achieved Loss=> 22.8460148185
Gradients: w=>0.0034162109, b=>-0.0589388568
Iteration 8970: Achieved Loss=> 22.8460113339
Gradients: w=>0.0034143947, b=>-0.0589075231
Iteration 8971: Achieved Loss=> 22.8460078531
Gradients: w=>0.0034125795, b=>-0.0588762061
Iteration 8972: Achieved Loss=> 22.8460043760
Gradients: w=>0.0034107653, b=>-0.0588449057
Iteration 8973: Achieved Loss=> 22.8460009025
Gradients: w=>0.0034089520, b=>-0.0588136219
Iteration 8974: Achieved Loss=> 22.8459974328
Gradients: w=>0.0034071397, b=>-0.0587823548
Iteration 8975: Achieved Loss=> 22.8459939667
Gradients: w=>0.0034053283, b=>-0.0587511043
Iteration 8976: Achieved Loss=> 22.8459905044
Gradients: w=>0.0034035180, b=>-0.0587198704
Iteration 8977: Achieved Loss=> 22.8459870457
Gradients: w=>0.0034017086, b=>-0.0586886531
Iteration 8978: Achieved Loss=> 22.8459835907
Gradients: w=>0.0033999001, b=>-0.0586574524
Iteration 8979: Achieved Loss=> 22.8459801393
Gradients: w=>0.0033980926, b=>-0.0586262683
Iteration 8980: Achieved Loss=> 22.8459766917
Gradients: w=>0.0033962861, b=>-0.0585951008
Iteration 8981: Achieved Loss=> 22.8459732477
Gradients: w=>0.0033944805, b=>-0.0585639498
Iteration 8982: Achieved Loss=> 22.8459698073
Gradients: w=>0.0033926759, b=>-0.0585328154
Iteration 8983: Achieved Loss=> 22.8459663706
Gradients: w=>0.0033908723, b=>-0.0585016976
Iteration 8984: Achieved Loss=> 22.8459629376
Gradients: w=>0.0033890696, b=>-0.0584705963
Iteration 8985: Achieved Loss=> 22.8459595082
Gradients: w=>0.0033872678, b=>-0.0584395115
Iteration 8986: Achieved Loss=> 22.8459560825
Gradients: w=>0.0033854671, b=>-0.0584084433
Iteration 8987: Achieved Loss=> 22.8459526604
Gradients: w=>0.0033836672, b=>-0.0583773916
Iteration 8988: Achieved Loss=> 22.8459492419
Gradients: w=>0.0033818684, b=>-0.0583463564
Iteration 8989: Achieved Loss=> 22.8459458271
Gradients: w=>0.0033800705, b=>-0.0583153377
Iteration 8990: Achieved Loss=> 22.8459424159
Gradients: w=>0.0033782735, b=>-0.0582843354
Iteration 8991: Achieved Loss=> 22.8459390083
Gradients: w=>0.0033764775, b=>-0.0582533497
Iteration 8992: Achieved Loss=> 22.8459356044
Gradients: w=>0.0033746825, b=>-0.0582223804
Iteration 8993: Achieved Loss=> 22.8459322040
Gradients: w=>0.0033728884, b=>-0.0581914276
Iteration 8994: Achieved Loss=> 22.8459288073
Gradients: w=>0.0033710953, b=>-0.0581604913
Iteration 8995: Achieved Loss=> 22.8459254142
Gradients: w=>0.0033693031, b=>-0.0581295714
Iteration 8996: Achieved Loss=> 22.8459220247
Gradients: w=>0.0033675119, b=>-0.0580986679
Iteration 8997: Achieved Loss=> 22.8459186388
Gradients: w=>0.0033657216, b=>-0.0580677809
Iteration 8998: Achieved Loss=> 22.8459152565
Gradients: w=>0.0033639323, b=>-0.0580369102
Iteration 8999: Achieved Loss=> 22.8459118778
Gradients: w=>0.0033621439, b=>-0.0580060560
Iteration 9000: Achieved Loss=> 22.8459085027
Gradients: w=>0.0033603565, b=>-0.0579752182
Iteration 9001: Achieved Loss=> 22.8459051312
Gradients: w=>0.0033585700, b=>-0.0579443968
Iteration 9002: Achieved Loss=> 22.8459017633
Gradients: w=>0.0033567845, b=>-0.0579135918
Iteration 9003: Achieved Loss=> 22.8458983989
Gradients: w=>0.0033549999, b=>-0.0578828032
Iteration 9004: Achieved Loss=> 22.8458950381
Gradients: w=>0.0033532163, b=>-0.0578520309
Iteration 9005: Achieved Loss=> 22.8458916809
Gradients: w=>0.0033514336, b=>-0.0578212750
Iteration 9006: Achieved Loss=> 22.8458883273
Gradients: w=>0.0033496519, b=>-0.0577905354
Iteration 9007: Achieved Loss=> 22.8458849772
Gradients: w=>0.0033478711, b=>-0.0577598122
Iteration 9008: Achieved Loss=> 22.8458816307
Gradients: w=>0.0033460913, b=>-0.0577291053
Iteration 9009: Achieved Loss=> 22.8458782877
Gradients: w=>0.0033443124, b=>-0.0576984147
Iteration 9010: Achieved Loss=> 22.8458749483
Gradients: w=>0.0033425345, b=>-0.0576677405
Iteration 9011: Achieved Loss=> 22.8458716125
Gradients: w=>0.0033407575, b=>-0.0576370826
Iteration 9012: Achieved Loss=> 22.8458682802
Gradients: w=>0.0033389814, b=>-0.0576064409
Iteration 9013: Achieved Loss=> 22.8458649514
Gradients: w=>0.0033372063, b=>-0.0575758156
Iteration 9014: Achieved Loss=> 22.8458616262
Gradients: w=>0.0033354322, b=>-0.0575452065
Iteration 9015: Achieved Loss=> 22.8458583045
Gradients: w=>0.0033336589, b=>-0.0575146137
Iteration 9016: Achieved Loss=> 22.8458549863
Gradients: w=>0.0033318867, b=>-0.0574840372
Iteration 9017: Achieved Loss=> 22.8458516717
Gradients: w=>0.0033301153, b=>-0.0574534769
Iteration 9018: Achieved Loss=> 22.8458483606
Gradients: w=>0.0033283449, b=>-0.0574229328
Iteration 9019: Achieved Loss=> 22.8458450530
Gradients: w=>0.0033265755, b=>-0.0573924051
Iteration 9020: Achieved Loss=> 22.8458417489
Gradients: w=>0.0033248070, b=>-0.0573618935
Iteration 9021: Achieved Loss=> 22.8458384483
Gradients: w=>0.0033230394, b=>-0.0573313981
Iteration 9022: Achieved Loss=> 22.8458351513
Gradients: w=>0.0033212728, b=>-0.0573009190
Iteration 9023: Achieved Loss=> 22.8458318577
Gradients: w=>0.0033195071, b=>-0.0572704561
Iteration 9024: Achieved Loss=> 22.8458285677
Gradients: w=>0.0033177423, b=>-0.0572400094
Iteration 9025: Achieved Loss=> 22.8458252811
Gradients: w=>0.0033159785, b=>-0.0572095788
Iteration 9026: Achieved Loss=> 22.8458219981
Gradients: w=>0.0033142157, b=>-0.0571791644
Iteration 9027: Achieved Loss=> 22.8458187185
Gradients: w=>0.0033124537, b=>-0.0571487662
Iteration 9028: Achieved Loss=> 22.8458154424
Gradients: w=>0.0033106927, b=>-0.0571183842
Iteration 9029: Achieved Loss=> 22.8458121698
Gradients: w=>0.0033089326, b=>-0.0570880183
Iteration 9030: Achieved Loss=> 22.8458089007
Gradients: w=>0.0033071735, b=>-0.0570576686
Iteration 9031: Achieved Loss=> 22.8458056350
Gradients: w=>0.0033054153, b=>-0.0570273350
Iteration 9032: Achieved Loss=> 22.8458023729
Gradients: w=>0.0033036581, b=>-0.0569970175
Iteration 9033: Achieved Loss=> 22.8457991142
Gradients: w=>0.0033019017, b=>-0.0569667161
Iteration 9034: Achieved Loss=> 22.8457958589
Gradients: w=>0.0033001463, b=>-0.0569364309
Iteration 9035: Achieved Loss=> 22.8457926071
Gradients: w=>0.0032983919, b=>-0.0569061617
Iteration 9036: Achieved Loss=> 22.8457893588
Gradients: w=>0.0032966384, b=>-0.0568759087
Iteration 9037: Achieved Loss=> 22.8457861139
Gradients: w=>0.0032948858, b=>-0.0568456717
Iteration 9038: Achieved Loss=> 22.8457828725
Gradients: w=>0.0032931341, b=>-0.0568154508
Iteration 9039: Achieved Loss=> 22.8457796345
Gradients: w=>0.0032913834, b=>-0.0567852459
Iteration 9040: Achieved Loss=> 22.8457764000
Gradients: w=>0.0032896336, b=>-0.0567550572
Iteration 9041: Achieved Loss=> 22.8457731689
Gradients: w=>0.0032878847, b=>-0.0567248844
Iteration 9042: Achieved Loss=> 22.8457699412
Gradients: w=>0.0032861368, b=>-0.0566947277
Iteration 9043: Achieved Loss=> 22.8457667170
Gradients: w=>0.0032843898, b=>-0.0566645871
Iteration 9044: Achieved Loss=> 22.8457634962
Gradients: w=>0.0032826437, b=>-0.0566344624
Iteration 9045: Achieved Loss=> 22.8457602788
Gradients: w=>0.0032808985, b=>-0.0566043538
Iteration 9046: Achieved Loss=> 22.8457570648
Gradients: w=>0.0032791543, b=>-0.0565742612
Iteration 9047: Achieved Loss=> 22.8457538543
Gradients: w=>0.0032774110, b=>-0.0565441846
Iteration 9048: Achieved Loss=> 22.8457506472
Gradients: w=>0.0032756686, b=>-0.0565141240
Iteration 9049: Achieved Loss=> 22.8457474434
Gradients: w=>0.0032739272, b=>-0.0564840793
Iteration 9050: Achieved Loss=> 22.8457442431
Gradients: w=>0.0032721867, b=>-0.0564540507
Iteration 9051: Achieved Loss=> 22.8457410462
Gradients: w=>0.0032704471, b=>-0.0564240380
Iteration 9052: Achieved Loss=> 22.8457378527
Gradients: w=>0.0032687084, b=>-0.0563940412
Iteration 9053: Achieved Loss=> 22.8457346626
Gradients: w=>0.0032669706, b=>-0.0563640604
Iteration 9054: Achieved Loss=> 22.8457314758
Gradients: w=>0.0032652338, b=>-0.0563340955
Iteration 9055: Achieved Loss=> 22.8457282925
Gradients: w=>0.0032634979, b=>-0.0563041466
Iteration 9056: Achieved Loss=> 22.8457251125
Gradients: w=>0.0032617629, b=>-0.0562742136
Iteration 9057: Achieved Loss=> 22.8457219359
Gradients: w=>0.0032600289, b=>-0.0562442965
Iteration 9058: Achieved Loss=> 22.8457187627
Gradients: w=>0.0032582958, b=>-0.0562143953
Iteration 9059: Achieved Loss=> 22.8457155929
Gradients: w=>0.0032565635, b=>-0.0561845100
Iteration 9060: Achieved Loss=> 22.8457124264
Gradients: w=>0.0032548323, b=>-0.0561546406
Iteration 9061: Achieved Loss=> 22.8457092633
Gradients: w=>0.0032531019, b=>-0.0561247871
Iteration 9062: Achieved Loss=> 22.8457061036
Gradients: w=>0.0032513724, b=>-0.0560949494
Iteration 9063: Achieved Loss=> 22.8457029472
Gradients: w=>0.0032496439, b=>-0.0560651276
Iteration 9064: Achieved Loss=> 22.8456997942
Gradients: w=>0.0032479163, b=>-0.0560353217
Iteration 9065: Achieved Loss=> 22.8456966445
Gradients: w=>0.0032461896, b=>-0.0560055316
Iteration 9066: Achieved Loss=> 22.8456934982
Gradients: w=>0.0032444638, b=>-0.0559757573
Iteration 9067: Achieved Loss=> 22.8456903552
Gradients: w=>0.0032427390, b=>-0.0559459989
Iteration 9068: Achieved Loss=> 22.8456872156
Gradients: w=>0.0032410150, b=>-0.0559162563
Iteration 9069: Achieved Loss=> 22.8456840793
Gradients: w=>0.0032392920, b=>-0.0558865295
Iteration 9070: Achieved Loss=> 22.8456809464
Gradients: w=>0.0032375699, b=>-0.0558568185
Iteration 9071: Achieved Loss=> 22.8456778167
Gradients: w=>0.0032358487, b=>-0.0558271233
Iteration 9072: Achieved Loss=> 22.8456746904
Gradients: w=>0.0032341284, b=>-0.0557974439
Iteration 9073: Achieved Loss=> 22.8456715674
Gradients: w=>0.0032324091, b=>-0.0557677802
Iteration 9074: Achieved Loss=> 22.8456684478
Gradients: w=>0.0032306906, b=>-0.0557381324
Iteration 9075: Achieved Loss=> 22.8456653314
Gradients: w=>0.0032289731, b=>-0.0557085003
Iteration 9076: Achieved Loss=> 22.8456622184
Gradients: w=>0.0032272565, b=>-0.0556788839
Iteration 9077: Achieved Loss=> 22.8456591087
Gradients: w=>0.0032255408, b=>-0.0556492833
Iteration 9078: Achieved Loss=> 22.8456560022
Gradients: w=>0.0032238260, b=>-0.0556196984
Iteration 9079: Achieved Loss=> 22.8456528991
Gradients: w=>0.0032221121, b=>-0.0555901293
Iteration 9080: Achieved Loss=> 22.8456497993
Gradients: w=>0.0032203991, b=>-0.0555605759
Iteration 9081: Achieved Loss=> 22.8456467028
Gradients: w=>0.0032186871, b=>-0.0555310382
Iteration 9082: Achieved Loss=> 22.8456436095
Gradients: w=>0.0032169759, b=>-0.0555015162
Iteration 9083: Achieved Loss=> 22.8456405196
Gradients: w=>0.0032152657, b=>-0.0554720099
Iteration 9084: Achieved Loss=> 22.8456374329
Gradients: w=>0.0032135563, b=>-0.0554425192
Iteration 9085: Achieved Loss=> 22.8456343495
Gradients: w=>0.0032118479, b=>-0.0554130443
Iteration 9086: Achieved Loss=> 22.8456312694
Gradients: w=>0.0032101404, b=>-0.0553835850
Iteration 9087: Achieved Loss=> 22.8456281926
Gradients: w=>0.0032084338, b=>-0.0553541414
Iteration 9088: Achieved Loss=> 22.8456251191
Gradients: w=>0.0032067281, b=>-0.0553247134
Iteration 9089: Achieved Loss=> 22.8456220488
Gradients: w=>0.0032050233, b=>-0.0552953011
Iteration 9090: Achieved Loss=> 22.8456189817
Gradients: w=>0.0032033194, b=>-0.0552659044
Iteration 9091: Achieved Loss=> 22.8456159180
Gradients: w=>0.0032016164, b=>-0.0552365234
Iteration 9092: Achieved Loss=> 22.8456128575
Gradients: w=>0.0031999143, b=>-0.0552071580
Iteration 9093: Achieved Loss=> 22.8456098002
Gradients: w=>0.0031982132, b=>-0.0551778081
Iteration 9094: Achieved Loss=> 22.8456067462
Gradients: w=>0.0031965129, b=>-0.0551484739
Iteration 9095: Achieved Loss=> 22.8456036954
Gradients: w=>0.0031948135, b=>-0.0551191553
Iteration 9096: Achieved Loss=> 22.8456006479
Gradients: w=>0.0031931151, b=>-0.0550898523
Iteration 9097: Achieved Loss=> 22.8455976036
Gradients: w=>0.0031914175, b=>-0.0550605648
Iteration 9098: Achieved Loss=> 22.8455945626
Gradients: w=>0.0031897208, b=>-0.0550312929
Iteration 9099: Achieved Loss=> 22.8455915248
Gradients: w=>0.0031880251, b=>-0.0550020366
Iteration 9100: Achieved Loss=> 22.8455884902
Gradients: w=>0.0031863302, b=>-0.0549727958
Iteration 9101: Achieved Loss=> 22.8455854589
Gradients: w=>0.0031846363, b=>-0.0549435706
Iteration 9102: Achieved Loss=> 22.8455824307
Gradients: w=>0.0031829432, b=>-0.0549143609
Iteration 9103: Achieved Loss=> 22.8455794058
Gradients: w=>0.0031812511, b=>-0.0548851668
Iteration 9104: Achieved Loss=> 22.8455763841
Gradients: w=>0.0031795598, b=>-0.0548559881
Iteration 9105: Achieved Loss=> 22.8455733656
Gradients: w=>0.0031778695, b=>-0.0548268250
Iteration 9106: Achieved Loss=> 22.8455703503
Gradients: w=>0.0031761800, b=>-0.0547976774
Iteration 9107: Achieved Loss=> 22.8455673383
Gradients: w=>0.0031744915, b=>-0.0547685452
Iteration 9108: Achieved Loss=> 22.8455643294
Gradients: w=>0.0031728038, b=>-0.0547394286
Iteration 9109: Achieved Loss=> 22.8455613237
Gradients: w=>0.0031711171, b=>-0.0547103274
Iteration 9110: Achieved Loss=> 22.8455583213
Gradients: w=>0.0031694312, b=>-0.0546812417
Iteration 9111: Achieved Loss=> 22.8455553220
Gradients: w=>0.0031677462, b=>-0.0546521715
Iteration 9112: Achieved Loss=> 22.8455523259
Gradients: w=>0.0031660622, b=>-0.0546231167
Iteration 9113: Achieved Loss=> 22.8455493330
Gradients: w=>0.0031643790, b=>-0.0545940774
Iteration 9114: Achieved Loss=> 22.8455463432
Gradients: w=>0.0031626967, b=>-0.0545650535
Iteration 9115: Achieved Loss=> 22.8455433567
Gradients: w=>0.0031610153, b=>-0.0545360451
Iteration 9116: Achieved Loss=> 22.8455403733
Gradients: w=>0.0031593348, b=>-0.0545070520
Iteration 9117: Achieved Loss=> 22.8455373931
Gradients: w=>0.0031576552, b=>-0.0544780744
Iteration 9118: Achieved Loss=> 22.8455344160
Gradients: w=>0.0031559765, b=>-0.0544491122
Iteration 9119: Achieved Loss=> 22.8455314422
Gradients: w=>0.0031542987, b=>-0.0544201654
Iteration 9120: Achieved Loss=> 22.8455284715
Gradients: w=>0.0031526218, b=>-0.0543912340
Iteration 9121: Achieved Loss=> 22.8455255039
Gradients: w=>0.0031509457, b=>-0.0543623179
Iteration 9122: Achieved Loss=> 22.8455225395
Gradients: w=>0.0031492706, b=>-0.0543334172
Iteration 9123: Achieved Loss=> 22.8455195782
Gradients: w=>0.0031475964, b=>-0.0543045319
Iteration 9124: Achieved Loss=> 22.8455166201
Gradients: w=>0.0031459230, b=>-0.0542756620
Iteration 9125: Achieved Loss=> 22.8455136652
Gradients: w=>0.0031442505, b=>-0.0542468074
Iteration 9126: Achieved Loss=> 22.8455107134
Gradients: w=>0.0031425790, b=>-0.0542179681
Iteration 9127: Achieved Loss=> 22.8455077647
Gradients: w=>0.0031409083, b=>-0.0541891441
Iteration 9128: Achieved Loss=> 22.8455048191
Gradients: w=>0.0031392385, b=>-0.0541603355
Iteration 9129: Achieved Loss=> 22.8455018767
Gradients: w=>0.0031375695, b=>-0.0541315422
Iteration 9130: Achieved Loss=> 22.8454989374
Gradients: w=>0.0031359015, b=>-0.0541027642
Iteration 9131: Achieved Loss=> 22.8454960013
Gradients: w=>0.0031342344, b=>-0.0540740016
Iteration 9132: Achieved Loss=> 22.8454930682
Gradients: w=>0.0031325681, b=>-0.0540452542
Iteration 9133: Achieved Loss=> 22.8454901383
Gradients: w=>0.0031309027, b=>-0.0540165220
Iteration 9134: Achieved Loss=> 22.8454872115
Gradients: w=>0.0031292383, b=>-0.0539878052
Iteration 9135: Achieved Loss=> 22.8454842878
Gradients: w=>0.0031275747, b=>-0.0539591036
Iteration 9136: Achieved Loss=> 22.8454813672
Gradients: w=>0.0031259120, b=>-0.0539304173
Iteration 9137: Achieved Loss=> 22.8454784497
Gradients: w=>0.0031242501, b=>-0.0539017462
Iteration 9138: Achieved Loss=> 22.8454755353
Gradients: w=>0.0031225892, b=>-0.0538730904
Iteration 9139: Achieved Loss=> 22.8454726241
Gradients: w=>0.0031209291, b=>-0.0538444498
Iteration 9140: Achieved Loss=> 22.8454697159
Gradients: w=>0.0031192699, b=>-0.0538158245
Iteration 9141: Achieved Loss=> 22.8454668108
Gradients: w=>0.0031176116, b=>-0.0537872143
Iteration 9142: Achieved Loss=> 22.8454639088
Gradients: w=>0.0031159542, b=>-0.0537586194
Iteration 9143: Achieved Loss=> 22.8454610098
Gradients: w=>0.0031142977, b=>-0.0537300397
Iteration 9144: Achieved Loss=> 22.8454581140
Gradients: w=>0.0031126420, b=>-0.0537014751
Iteration 9145: Achieved Loss=> 22.8454552212
Gradients: w=>0.0031109872, b=>-0.0536729258
Iteration 9146: Achieved Loss=> 22.8454523315
Gradients: w=>0.0031093333, b=>-0.0536443916
Iteration 9147: Achieved Loss=> 22.8454494449
Gradients: w=>0.0031076803, b=>-0.0536158726
Iteration 9148: Achieved Loss=> 22.8454465613
Gradients: w=>0.0031060282, b=>-0.0535873687
Iteration 9149: Achieved Loss=> 22.8454436809
Gradients: w=>0.0031043769, b=>-0.0535588801
Iteration 9150: Achieved Loss=> 22.8454408034
Gradients: w=>0.0031027265, b=>-0.0535304065
Iteration 9151: Achieved Loss=> 22.8454379291
Gradients: w=>0.0031010770, b=>-0.0535019481
Iteration 9152: Achieved Loss=> 22.8454350578
Gradients: w=>0.0030994284, b=>-0.0534735048
Iteration 9153: Achieved Loss=> 22.8454321895
Gradients: w=>0.0030977807, b=>-0.0534450767
Iteration 9154: Achieved Loss=> 22.8454293243
Gradients: w=>0.0030961338, b=>-0.0534166636
Iteration 9155: Achieved Loss=> 22.8454264621
Gradients: w=>0.0030944878, b=>-0.0533882657
Iteration 9156: Achieved Loss=> 22.8454236030
Gradients: w=>0.0030928427, b=>-0.0533598829
Iteration 9157: Achieved Loss=> 22.8454207469
Gradients: w=>0.0030911984, b=>-0.0533315151
Iteration 9158: Achieved Loss=> 22.8454178939
Gradients: w=>0.0030895550, b=>-0.0533031624
Iteration 9159: Achieved Loss=> 22.8454150439
Gradients: w=>0.0030879125, b=>-0.0532748248
Iteration 9160: Achieved Loss=> 22.8454121969
Gradients: w=>0.0030862709, b=>-0.0532465023
Iteration 9161: Achieved Loss=> 22.8454093529
Gradients: w=>0.0030846301, b=>-0.0532181948
Iteration 9162: Achieved Loss=> 22.8454065120
Gradients: w=>0.0030829903, b=>-0.0531899024
Iteration 9163: Achieved Loss=> 22.8454036741
Gradients: w=>0.0030813513, b=>-0.0531616250
Iteration 9164: Achieved Loss=> 22.8454008392
Gradients: w=>0.0030797131, b=>-0.0531333627
Iteration 9165: Achieved Loss=> 22.8453980073
Gradients: w=>0.0030780758, b=>-0.0531051153
Iteration 9166: Achieved Loss=> 22.8453951784
Gradients: w=>0.0030764394, b=>-0.0530768830
Iteration 9167: Achieved Loss=> 22.8453923525
Gradients: w=>0.0030748039, b=>-0.0530486657
Iteration 9168: Achieved Loss=> 22.8453895297
Gradients: w=>0.0030731693, b=>-0.0530204634
Iteration 9169: Achieved Loss=> 22.8453867098
Gradients: w=>0.0030715355, b=>-0.0529922761
Iteration 9170: Achieved Loss=> 22.8453838929
Gradients: w=>0.0030699025, b=>-0.0529641038
Iteration 9171: Achieved Loss=> 22.8453810791
Gradients: w=>0.0030682705, b=>-0.0529359465
Iteration 9172: Achieved Loss=> 22.8453782682
Gradients: w=>0.0030666393, b=>-0.0529078041
Iteration 9173: Achieved Loss=> 22.8453754603
Gradients: w=>0.0030650090, b=>-0.0528796767
Iteration 9174: Achieved Loss=> 22.8453726554
Gradients: w=>0.0030633795, b=>-0.0528515642
Iteration 9175: Achieved Loss=> 22.8453698534
Gradients: w=>0.0030617509, b=>-0.0528234667
Iteration 9176: Achieved Loss=> 22.8453670545
Gradients: w=>0.0030601232, b=>-0.0527953841
Iteration 9177: Achieved Loss=> 22.8453642585
Gradients: w=>0.0030584964, b=>-0.0527673165
Iteration 9178: Achieved Loss=> 22.8453614655
Gradients: w=>0.0030568704, b=>-0.0527392637
Iteration 9179: Achieved Loss=> 22.8453586755
Gradients: w=>0.0030552452, b=>-0.0527112259
Iteration 9180: Achieved Loss=> 22.8453558884
Gradients: w=>0.0030536210, b=>-0.0526832030
Iteration 9181: Achieved Loss=> 22.8453531043
Gradients: w=>0.0030519976, b=>-0.0526551950
Iteration 9182: Achieved Loss=> 22.8453503232
Gradients: w=>0.0030503750, b=>-0.0526272019
Iteration 9183: Achieved Loss=> 22.8453475450
Gradients: w=>0.0030487534, b=>-0.0525992237
Iteration 9184: Achieved Loss=> 22.8453447698
Gradients: w=>0.0030471326, b=>-0.0525712603
Iteration 9185: Achieved Loss=> 22.8453419975
Gradients: w=>0.0030455126, b=>-0.0525433118
Iteration 9186: Achieved Loss=> 22.8453392281
Gradients: w=>0.0030438935, b=>-0.0525153782
Iteration 9187: Achieved Loss=> 22.8453364617
Gradients: w=>0.0030422753, b=>-0.0524874594
Iteration 9188: Achieved Loss=> 22.8453336983
Gradients: w=>0.0030406579, b=>-0.0524595554
Iteration 9189: Achieved Loss=> 22.8453309378
Gradients: w=>0.0030390414, b=>-0.0524316663
Iteration 9190: Achieved Loss=> 22.8453281802
Gradients: w=>0.0030374258, b=>-0.0524037921
Iteration 9191: Achieved Loss=> 22.8453254255
Gradients: w=>0.0030358110, b=>-0.0523759326
Iteration 9192: Achieved Loss=> 22.8453226738
Gradients: w=>0.0030341971, b=>-0.0523480879
Iteration 9193: Achieved Loss=> 22.8453199250
Gradients: w=>0.0030325840, b=>-0.0523202581
Iteration 9194: Achieved Loss=> 22.8453171791
Gradients: w=>0.0030309718, b=>-0.0522924430
Iteration 9195: Achieved Loss=> 22.8453144362
Gradients: w=>0.0030293604, b=>-0.0522646428
Iteration 9196: Achieved Loss=> 22.8453116961
Gradients: w=>0.0030277499, b=>-0.0522368573
Iteration 9197: Achieved Loss=> 22.8453089590
Gradients: w=>0.0030261403, b=>-0.0522090866
Iteration 9198: Achieved Loss=> 22.8453062248
Gradients: w=>0.0030245315, b=>-0.0521813306
Iteration 9199: Achieved Loss=> 22.8453034935
Gradients: w=>0.0030229235, b=>-0.0521535894
Iteration 9200: Achieved Loss=> 22.8453007651
Gradients: w=>0.0030213165, b=>-0.0521258630
Iteration 9201: Achieved Loss=> 22.8452980396
Gradients: w=>0.0030197102, b=>-0.0520981512
Iteration 9202: Achieved Loss=> 22.8452953169
Gradients: w=>0.0030181049, b=>-0.0520704543
Iteration 9203: Achieved Loss=> 22.8452925972
Gradients: w=>0.0030165004, b=>-0.0520427720
Iteration 9204: Achieved Loss=> 22.8452898804
Gradients: w=>0.0030148967, b=>-0.0520151045
Iteration 9205: Achieved Loss=> 22.8452871665
Gradients: w=>0.0030132939, b=>-0.0519874517
Iteration 9206: Achieved Loss=> 22.8452844554
Gradients: w=>0.0030116919, b=>-0.0519598135
Iteration 9207: Achieved Loss=> 22.8452817472
Gradients: w=>0.0030100908, b=>-0.0519321901
Iteration 9208: Achieved Loss=> 22.8452790419
Gradients: w=>0.0030084906, b=>-0.0519045814
Iteration 9209: Achieved Loss=> 22.8452763395
Gradients: w=>0.0030068911, b=>-0.0518769873
Iteration 9210: Achieved Loss=> 22.8452736400
Gradients: w=>0.0030052926, b=>-0.0518494079
Iteration 9211: Achieved Loss=> 22.8452709433
Gradients: w=>0.0030036949, b=>-0.0518218432
Iteration 9212: Achieved Loss=> 22.8452682495
Gradients: w=>0.0030020980, b=>-0.0517942931
Iteration 9213: Achieved Loss=> 22.8452655585
Gradients: w=>0.0030005020, b=>-0.0517667576
Iteration 9214: Achieved Loss=> 22.8452628705
Gradients: w=>0.0029989069, b=>-0.0517392368
Iteration 9215: Achieved Loss=> 22.8452601852
Gradients: w=>0.0029973126, b=>-0.0517117307
Iteration 9216: Achieved Loss=> 22.8452575029
Gradients: w=>0.0029957191, b=>-0.0516842391
Iteration 9217: Achieved Loss=> 22.8452548233
Gradients: w=>0.0029941265, b=>-0.0516567622
Iteration 9218: Achieved Loss=> 22.8452521467
Gradients: w=>0.0029925347, b=>-0.0516292999
Iteration 9219: Achieved Loss=> 22.8452494728
Gradients: w=>0.0029909438, b=>-0.0516018522
Iteration 9220: Achieved Loss=> 22.8452468018
Gradients: w=>0.0029893537, b=>-0.0515744190
Iteration 9221: Achieved Loss=> 22.8452441337
Gradients: w=>0.0029877645, b=>-0.0515470005
Iteration 9222: Achieved Loss=> 22.8452414684
Gradients: w=>0.0029861761, b=>-0.0515195965
Iteration 9223: Achieved Loss=> 22.8452388059
Gradients: w=>0.0029845885, b=>-0.0514922071
Iteration 9224: Achieved Loss=> 22.8452361463
Gradients: w=>0.0029830018, b=>-0.0514648323
Iteration 9225: Achieved Loss=> 22.8452334894
Gradients: w=>0.0029814160, b=>-0.0514374720
Iteration 9226: Achieved Loss=> 22.8452308354
Gradients: w=>0.0029798310, b=>-0.0514101263
Iteration 9227: Achieved Loss=> 22.8452281843
Gradients: w=>0.0029782468, b=>-0.0513827951
Iteration 9228: Achieved Loss=> 22.8452255359
Gradients: w=>0.0029766635, b=>-0.0513554784
Iteration 9229: Achieved Loss=> 22.8452228904
Gradients: w=>0.0029750810, b=>-0.0513281762
Iteration 9230: Achieved Loss=> 22.8452202476
Gradients: w=>0.0029734993, b=>-0.0513008886
Iteration 9231: Achieved Loss=> 22.8452176077
Gradients: w=>0.0029719185, b=>-0.0512736155
Iteration 9232: Achieved Loss=> 22.8452149706
Gradients: w=>0.0029703386, b=>-0.0512463568
Iteration 9233: Achieved Loss=> 22.8452123363
Gradients: w=>0.0029687595, b=>-0.0512191127
Iteration 9234: Achieved Loss=> 22.8452097048
Gradients: w=>0.0029671812, b=>-0.0511918831
Iteration 9235: Achieved Loss=> 22.8452070761
Gradients: w=>0.0029656037, b=>-0.0511646679
Iteration 9236: Achieved Loss=> 22.8452044501
Gradients: w=>0.0029640271, b=>-0.0511374672
Iteration 9237: Achieved Loss=> 22.8452018270
Gradients: w=>0.0029624514, b=>-0.0511102809
Iteration 9238: Achieved Loss=> 22.8451992067
Gradients: w=>0.0029608764, b=>-0.0510831091
Iteration 9239: Achieved Loss=> 22.8451965891
Gradients: w=>0.0029593023, b=>-0.0510559518
Iteration 9240: Achieved Loss=> 22.8451939743
Gradients: w=>0.0029577291, b=>-0.0510288089
Iteration 9241: Achieved Loss=> 22.8451913624
Gradients: w=>0.0029561567, b=>-0.0510016804
Iteration 9242: Achieved Loss=> 22.8451887531
Gradients: w=>0.0029545851, b=>-0.0509745663
Iteration 9243: Achieved Loss=> 22.8451861467
Gradients: w=>0.0029530143, b=>-0.0509474667
Iteration 9244: Achieved Loss=> 22.8451835430
Gradients: w=>0.0029514444, b=>-0.0509203815
Iteration 9245: Achieved Loss=> 22.8451809421
Gradients: w=>0.0029498753, b=>-0.0508933106
Iteration 9246: Achieved Loss=> 22.8451783440
Gradients: w=>0.0029483071, b=>-0.0508662542
Iteration 9247: Achieved Loss=> 22.8451757486
Gradients: w=>0.0029467397, b=>-0.0508392121
Iteration 9248: Achieved Loss=> 22.8451731560
Gradients: w=>0.0029451731, b=>-0.0508121844
Iteration 9249: Achieved Loss=> 22.8451705661
Gradients: w=>0.0029436074, b=>-0.0507851711
Iteration 9250: Achieved Loss=> 22.8451679790
Gradients: w=>0.0029420424, b=>-0.0507581721
Iteration 9251: Achieved Loss=> 22.8451653946
Gradients: w=>0.0029404784, b=>-0.0507311875
Iteration 9252: Achieved Loss=> 22.8451628130
Gradients: w=>0.0029389151, b=>-0.0507042173
Iteration 9253: Achieved Loss=> 22.8451602342
Gradients: w=>0.0029373527, b=>-0.0506772614
Iteration 9254: Achieved Loss=> 22.8451576580
Gradients: w=>0.0029357911, b=>-0.0506503198
Iteration 9255: Achieved Loss=> 22.8451550846
Gradients: w=>0.0029342304, b=>-0.0506233925
Iteration 9256: Achieved Loss=> 22.8451525140
Gradients: w=>0.0029326704, b=>-0.0505964796
Iteration 9257: Achieved Loss=> 22.8451499461
Gradients: w=>0.0029311113, b=>-0.0505695809
Iteration 9258: Achieved Loss=> 22.8451473809
Gradients: w=>0.0029295531, b=>-0.0505426966
Iteration 9259: Achieved Loss=> 22.8451448184
Gradients: w=>0.0029279956, b=>-0.0505158265
Iteration 9260: Achieved Loss=> 22.8451422587
Gradients: w=>0.0029264390, b=>-0.0504889708
Iteration 9261: Achieved Loss=> 22.8451397017
Gradients: w=>0.0029248832, b=>-0.0504621293
Iteration 9262: Achieved Loss=> 22.8451371473
Gradients: w=>0.0029233283, b=>-0.0504353021
Iteration 9263: Achieved Loss=> 22.8451345958
Gradients: w=>0.0029217741, b=>-0.0504084891
Iteration 9264: Achieved Loss=> 22.8451320469
Gradients: w=>0.0029202208, b=>-0.0503816904
Iteration 9265: Achieved Loss=> 22.8451295007
Gradients: w=>0.0029186684, b=>-0.0503549060
Iteration 9266: Achieved Loss=> 22.8451269573
Gradients: w=>0.0029171167, b=>-0.0503281357
Iteration 9267: Achieved Loss=> 22.8451244165
Gradients: w=>0.0029155659, b=>-0.0503013798
Iteration 9268: Achieved Loss=> 22.8451218785
Gradients: w=>0.0029140159, b=>-0.0502746380
Iteration 9269: Achieved Loss=> 22.8451193431
Gradients: w=>0.0029124667, b=>-0.0502479105
Iteration 9270: Achieved Loss=> 22.8451168104
Gradients: w=>0.0029109183, b=>-0.0502211971
Iteration 9271: Achieved Loss=> 22.8451142805
Gradients: w=>0.0029093708, b=>-0.0501944980
Iteration 9272: Achieved Loss=> 22.8451117532
Gradients: w=>0.0029078241, b=>-0.0501678131
Iteration 9273: Achieved Loss=> 22.8451092286
Gradients: w=>0.0029062782, b=>-0.0501411423
Iteration 9274: Achieved Loss=> 22.8451067067
Gradients: w=>0.0029047331, b=>-0.0501144857
Iteration 9275: Achieved Loss=> 22.8451041875
Gradients: w=>0.0029031889, b=>-0.0500878434
Iteration 9276: Achieved Loss=> 22.8451016709
Gradients: w=>0.0029016455, b=>-0.0500612151
Iteration 9277: Achieved Loss=> 22.8450991570
Gradients: w=>0.0029001029, b=>-0.0500346010
Iteration 9278: Achieved Loss=> 22.8450966458
Gradients: w=>0.0028985611, b=>-0.0500080011
Iteration 9279: Achieved Loss=> 22.8450941373
Gradients: w=>0.0028970201, b=>-0.0499814153
Iteration 9280: Achieved Loss=> 22.8450916314
Gradients: w=>0.0028954800, b=>-0.0499548437
Iteration 9281: Achieved Loss=> 22.8450891282
Gradients: w=>0.0028939406, b=>-0.0499282861
Iteration 9282: Achieved Loss=> 22.8450866277
Gradients: w=>0.0028924021, b=>-0.0499017427
Iteration 9283: Achieved Loss=> 22.8450841298
Gradients: w=>0.0028908644, b=>-0.0498752134
Iteration 9284: Achieved Loss=> 22.8450816346
Gradients: w=>0.0028893276, b=>-0.0498486982
Iteration 9285: Achieved Loss=> 22.8450791420
Gradients: w=>0.0028877915, b=>-0.0498221971
Iteration 9286: Achieved Loss=> 22.8450766520
Gradients: w=>0.0028862563, b=>-0.0497957101
Iteration 9287: Achieved Loss=> 22.8450741648
Gradients: w=>0.0028847219, b=>-0.0497692372
Iteration 9288: Achieved Loss=> 22.8450716801
Gradients: w=>0.0028831882, b=>-0.0497427783
Iteration 9289: Achieved Loss=> 22.8450691981
Gradients: w=>0.0028816555, b=>-0.0497163336
Iteration 9290: Achieved Loss=> 22.8450667188
Gradients: w=>0.0028801235, b=>-0.0496899028
Iteration 9291: Achieved Loss=> 22.8450642420
Gradients: w=>0.0028785923, b=>-0.0496634862
Iteration 9292: Achieved Loss=> 22.8450617680
Gradients: w=>0.0028770620, b=>-0.0496370835
Iteration 9293: Achieved Loss=> 22.8450592965
Gradients: w=>0.0028755324, b=>-0.0496106949
Iteration 9294: Achieved Loss=> 22.8450568277
Gradients: w=>0.0028740037, b=>-0.0495843204
Iteration 9295: Achieved Loss=> 22.8450543615
Gradients: w=>0.0028724758, b=>-0.0495579598
Iteration 9296: Achieved Loss=> 22.8450518979
Gradients: w=>0.0028709487, b=>-0.0495316133
Iteration 9297: Achieved Loss=> 22.8450494369
Gradients: w=>0.0028694224, b=>-0.0495052807
Iteration 9298: Achieved Loss=> 22.8450469785
Gradients: w=>0.0028678969, b=>-0.0494789622
Iteration 9299: Achieved Loss=> 22.8450445228
Gradients: w=>0.0028663723, b=>-0.0494526577
Iteration 9300: Achieved Loss=> 22.8450420697
Gradients: w=>0.0028648484, b=>-0.0494263671
Iteration 9301: Achieved Loss=> 22.8450396192
Gradients: w=>0.0028633254, b=>-0.0494000906
Iteration 9302: Achieved Loss=> 22.8450371712
Gradients: w=>0.0028618032, b=>-0.0493738280
Iteration 9303: Achieved Loss=> 22.8450347259
Gradients: w=>0.0028602817, b=>-0.0493475793
Iteration 9304: Achieved Loss=> 22.8450322832
Gradients: w=>0.0028587611, b=>-0.0493213446
Iteration 9305: Achieved Loss=> 22.8450298431
Gradients: w=>0.0028572413, b=>-0.0492951239
Iteration 9306: Achieved Loss=> 22.8450274056
Gradients: w=>0.0028557223, b=>-0.0492689171
Iteration 9307: Achieved Loss=> 22.8450249706
Gradients: w=>0.0028542041, b=>-0.0492427242
Iteration 9308: Achieved Loss=> 22.8450225383
Gradients: w=>0.0028526868, b=>-0.0492165453
Iteration 9309: Achieved Loss=> 22.8450201085
Gradients: w=>0.0028511702, b=>-0.0491903802
Iteration 9310: Achieved Loss=> 22.8450176813
Gradients: w=>0.0028496544, b=>-0.0491642291
Iteration 9311: Achieved Loss=> 22.8450152567
Gradients: w=>0.0028481394, b=>-0.0491380919
Iteration 9312: Achieved Loss=> 22.8450128347
Gradients: w=>0.0028466253, b=>-0.0491119686
Iteration 9313: Achieved Loss=> 22.8450104153
Gradients: w=>0.0028451119, b=>-0.0490858592
Iteration 9314: Achieved Loss=> 22.8450079984
Gradients: w=>0.0028435994, b=>-0.0490597636
Iteration 9315: Achieved Loss=> 22.8450055841
Gradients: w=>0.0028420876, b=>-0.0490336819
Iteration 9316: Achieved Loss=> 22.8450031724
Gradients: w=>0.0028405767, b=>-0.0490076141
Iteration 9317: Achieved Loss=> 22.8450007632
Gradients: w=>0.0028390666, b=>-0.0489815602
Iteration 9318: Achieved Loss=> 22.8449983566
Gradients: w=>0.0028375572, b=>-0.0489555201
Iteration 9319: Achieved Loss=> 22.8449959525
Gradients: w=>0.0028360487, b=>-0.0489294938
Iteration 9320: Achieved Loss=> 22.8449935510
Gradients: w=>0.0028345410, b=>-0.0489034814
Iteration 9321: Achieved Loss=> 22.8449911521
Gradients: w=>0.0028330340, b=>-0.0488774828
Iteration 9322: Achieved Loss=> 22.8449887557
Gradients: w=>0.0028315279, b=>-0.0488514980
Iteration 9323: Achieved Loss=> 22.8449863618
Gradients: w=>0.0028300226, b=>-0.0488255271
Iteration 9324: Achieved Loss=> 22.8449839705
Gradients: w=>0.0028285181, b=>-0.0487995699
Iteration 9325: Achieved Loss=> 22.8449815818
Gradients: w=>0.0028270143, b=>-0.0487736266
Iteration 9326: Achieved Loss=> 22.8449791955
Gradients: w=>0.0028255114, b=>-0.0487476970
Iteration 9327: Achieved Loss=> 22.8449768118
Gradients: w=>0.0028240093, b=>-0.0487217813
Iteration 9328: Achieved Loss=> 22.8449744307
Gradients: w=>0.0028225079, b=>-0.0486958793
Iteration 9329: Achieved Loss=> 22.8449720521
Gradients: w=>0.0028210074, b=>-0.0486699910
Iteration 9330: Achieved Loss=> 22.8449696760
Gradients: w=>0.0028195077, b=>-0.0486441166
Iteration 9331: Achieved Loss=> 22.8449673024
Gradients: w=>0.0028180087, b=>-0.0486182559
Iteration 9332: Achieved Loss=> 22.8449649314
Gradients: w=>0.0028165106, b=>-0.0485924089
Iteration 9333: Achieved Loss=> 22.8449625628
Gradients: w=>0.0028150133, b=>-0.0485665757
Iteration 9334: Achieved Loss=> 22.8449601968
Gradients: w=>0.0028135167, b=>-0.0485407562
Iteration 9335: Achieved Loss=> 22.8449578333
Gradients: w=>0.0028120210, b=>-0.0485149505
Iteration 9336: Achieved Loss=> 22.8449554724
Gradients: w=>0.0028105260, b=>-0.0484891584
Iteration 9337: Achieved Loss=> 22.8449531139
Gradients: w=>0.0028090318, b=>-0.0484633801
Iteration 9338: Achieved Loss=> 22.8449507579
Gradients: w=>0.0028075385, b=>-0.0484376155
Iteration 9339: Achieved Loss=> 22.8449484045
Gradients: w=>0.0028060459, b=>-0.0484118646
Iteration 9340: Achieved Loss=> 22.8449460535
Gradients: w=>0.0028045541, b=>-0.0483861273
Iteration 9341: Achieved Loss=> 22.8449437050
Gradients: w=>0.0028030631, b=>-0.0483604038
Iteration 9342: Achieved Loss=> 22.8449413591
Gradients: w=>0.0028015729, b=>-0.0483346939
Iteration 9343: Achieved Loss=> 22.8449390156
Gradients: w=>0.0028000835, b=>-0.0483089977
Iteration 9344: Achieved Loss=> 22.8449366746
Gradients: w=>0.0027985949, b=>-0.0482833151
Iteration 9345: Achieved Loss=> 22.8449343362
Gradients: w=>0.0027971071, b=>-0.0482576462
Iteration 9346: Achieved Loss=> 22.8449320001
Gradients: w=>0.0027956201, b=>-0.0482319910
Iteration 9347: Achieved Loss=> 22.8449296666
Gradients: w=>0.0027941338, b=>-0.0482063494
Iteration 9348: Achieved Loss=> 22.8449273356
Gradients: w=>0.0027926484, b=>-0.0481807214
Iteration 9349: Achieved Loss=> 22.8449250070
Gradients: w=>0.0027911637, b=>-0.0481551071
Iteration 9350: Achieved Loss=> 22.8449226809
Gradients: w=>0.0027896799, b=>-0.0481295063
Iteration 9351: Achieved Loss=> 22.8449203573
Gradients: w=>0.0027881968, b=>-0.0481039192
Iteration 9352: Achieved Loss=> 22.8449180362
Gradients: w=>0.0027867145, b=>-0.0480783457
Iteration 9353: Achieved Loss=> 22.8449157175
Gradients: w=>0.0027852330, b=>-0.0480527858
Iteration 9354: Achieved Loss=> 22.8449134013
Gradients: w=>0.0027837523, b=>-0.0480272394
Iteration 9355: Achieved Loss=> 22.8449110875
Gradients: w=>0.0027822723, b=>-0.0480017067
Iteration 9356: Achieved Loss=> 22.8449087763
Gradients: w=>0.0027807932, b=>-0.0479761875
Iteration 9357: Achieved Loss=> 22.8449064674
Gradients: w=>0.0027793148, b=>-0.0479506819
Iteration 9358: Achieved Loss=> 22.8449041610
Gradients: w=>0.0027778373, b=>-0.0479251898
Iteration 9359: Achieved Loss=> 22.8449018571
Gradients: w=>0.0027763605, b=>-0.0478997113
Iteration 9360: Achieved Loss=> 22.8448995556
Gradients: w=>0.0027748845, b=>-0.0478742464
Iteration 9361: Achieved Loss=> 22.8448972566
Gradients: w=>0.0027734093, b=>-0.0478487950
Iteration 9362: Achieved Loss=> 22.8448949600
Gradients: w=>0.0027719349, b=>-0.0478233571
Iteration 9363: Achieved Loss=> 22.8448926659
Gradients: w=>0.0027704612, b=>-0.0477979327
Iteration 9364: Achieved Loss=> 22.8448903742
Gradients: w=>0.0027689883, b=>-0.0477725219
Iteration 9365: Achieved Loss=> 22.8448880849
Gradients: w=>0.0027675163, b=>-0.0477471245
Iteration 9366: Achieved Loss=> 22.8448857980
Gradients: w=>0.0027660450, b=>-0.0477217407
Iteration 9367: Achieved Loss=> 22.8448835136
Gradients: w=>0.0027645745, b=>-0.0476963703
Iteration 9368: Achieved Loss=> 22.8448812317
Gradients: w=>0.0027631047, b=>-0.0476710135
Iteration 9369: Achieved Loss=> 22.8448789521
Gradients: w=>0.0027616358, b=>-0.0476456701
Iteration 9370: Achieved Loss=> 22.8448766750
Gradients: w=>0.0027601676, b=>-0.0476203402
Iteration 9371: Achieved Loss=> 22.8448744003
Gradients: w=>0.0027587002, b=>-0.0475950238
Iteration 9372: Achieved Loss=> 22.8448721280
Gradients: w=>0.0027572336, b=>-0.0475697208
Iteration 9373: Achieved Loss=> 22.8448698581
Gradients: w=>0.0027557678, b=>-0.0475444313
Iteration 9374: Achieved Loss=> 22.8448675906
Gradients: w=>0.0027543027, b=>-0.0475191552
Iteration 9375: Achieved Loss=> 22.8448653256
Gradients: w=>0.0027528384, b=>-0.0474938926
Iteration 9376: Achieved Loss=> 22.8448630629
Gradients: w=>0.0027513750, b=>-0.0474686433
Iteration 9377: Achieved Loss=> 22.8448608027
Gradients: w=>0.0027499122, b=>-0.0474434076
Iteration 9378: Achieved Loss=> 22.8448585449
Gradients: w=>0.0027484503, b=>-0.0474181852
Iteration 9379: Achieved Loss=> 22.8448562894
Gradients: w=>0.0027469891, b=>-0.0473929762
Iteration 9380: Achieved Loss=> 22.8448540364
Gradients: w=>0.0027455288, b=>-0.0473677807
Iteration 9381: Achieved Loss=> 22.8448517857
Gradients: w=>0.0027440691, b=>-0.0473425985
Iteration 9382: Achieved Loss=> 22.8448495375
Gradients: w=>0.0027426103, b=>-0.0473174297
Iteration 9383: Achieved Loss=> 22.8448472916
Gradients: w=>0.0027411523, b=>-0.0472922743
Iteration 9384: Achieved Loss=> 22.8448450481
Gradients: w=>0.0027396950, b=>-0.0472671323
Iteration 9385: Achieved Loss=> 22.8448428070
Gradients: w=>0.0027382385, b=>-0.0472420036
Iteration 9386: Achieved Loss=> 22.8448405683
Gradients: w=>0.0027367827, b=>-0.0472168883
Iteration 9387: Achieved Loss=> 22.8448383320
Gradients: w=>0.0027353278, b=>-0.0471917864
Iteration 9388: Achieved Loss=> 22.8448360980
Gradients: w=>0.0027338736, b=>-0.0471666978
Iteration 9389: Achieved Loss=> 22.8448338665
Gradients: w=>0.0027324202, b=>-0.0471416225
Iteration 9390: Achieved Loss=> 22.8448316373
Gradients: w=>0.0027309676, b=>-0.0471165606
Iteration 9391: Achieved Loss=> 22.8448294104
Gradients: w=>0.0027295157, b=>-0.0470915120
Iteration 9392: Achieved Loss=> 22.8448271860
Gradients: w=>0.0027280646, b=>-0.0470664767
Iteration 9393: Achieved Loss=> 22.8448249639
Gradients: w=>0.0027266143, b=>-0.0470414547
Iteration 9394: Achieved Loss=> 22.8448227441
Gradients: w=>0.0027251647, b=>-0.0470164460
Iteration 9395: Achieved Loss=> 22.8448205267
Gradients: w=>0.0027237159, b=>-0.0469914506
Iteration 9396: Achieved Loss=> 22.8448183117
Gradients: w=>0.0027222679, b=>-0.0469664685
Iteration 9397: Achieved Loss=> 22.8448160990
Gradients: w=>0.0027208207, b=>-0.0469414997
Iteration 9398: Achieved Loss=> 22.8448138887
Gradients: w=>0.0027193742, b=>-0.0469165442
Iteration 9399: Achieved Loss=> 22.8448116807
Gradients: w=>0.0027179285, b=>-0.0468916019
Iteration 9400: Achieved Loss=> 22.8448094751
Gradients: w=>0.0027164836, b=>-0.0468666729
Iteration 9401: Achieved Loss=> 22.8448072718
Gradients: w=>0.0027150394, b=>-0.0468417571
Iteration 9402: Achieved Loss=> 22.8448050709
Gradients: w=>0.0027135960, b=>-0.0468168546
Iteration 9403: Achieved Loss=> 22.8448028723
Gradients: w=>0.0027121534, b=>-0.0467919653
Iteration 9404: Achieved Loss=> 22.8448006760
Gradients: w=>0.0027107115, b=>-0.0467670893
Iteration 9405: Achieved Loss=> 22.8447984821
Gradients: w=>0.0027092704, b=>-0.0467422265
Iteration 9406: Achieved Loss=> 22.8447962905
Gradients: w=>0.0027078301, b=>-0.0467173769
Iteration 9407: Achieved Loss=> 22.8447941013
Gradients: w=>0.0027063905, b=>-0.0466925405
Iteration 9408: Achieved Loss=> 22.8447919143
Gradients: w=>0.0027049517, b=>-0.0466677173
Iteration 9409: Achieved Loss=> 22.8447897297
Gradients: w=>0.0027035137, b=>-0.0466429073
Iteration 9410: Achieved Loss=> 22.8447875474
Gradients: w=>0.0027020764, b=>-0.0466181105
Iteration 9411: Achieved Loss=> 22.8447853675
Gradients: w=>0.0027006399, b=>-0.0465933269
Iteration 9412: Achieved Loss=> 22.8447831898
Gradients: w=>0.0026992042, b=>-0.0465685564
Iteration 9413: Achieved Loss=> 22.8447810145
Gradients: w=>0.0026977692, b=>-0.0465437992
Iteration 9414: Achieved Loss=> 22.8447788414
Gradients: w=>0.0026963350, b=>-0.0465190550
Iteration 9415: Achieved Loss=> 22.8447766707
Gradients: w=>0.0026949015, b=>-0.0464943241
Iteration 9416: Achieved Loss=> 22.8447745023
Gradients: w=>0.0026934688, b=>-0.0464696063
Iteration 9417: Achieved Loss=> 22.8447723362
Gradients: w=>0.0026920369, b=>-0.0464449016
Iteration 9418: Achieved Loss=> 22.8447701724
Gradients: w=>0.0026906057, b=>-0.0464202101
Iteration 9419: Achieved Loss=> 22.8447680109
Gradients: w=>0.0026891753, b=>-0.0463955317
Iteration 9420: Achieved Loss=> 22.8447658517
Gradients: w=>0.0026877457, b=>-0.0463708664
Iteration 9421: Achieved Loss=> 22.8447636948
Gradients: w=>0.0026863168, b=>-0.0463462142
Iteration 9422: Achieved Loss=> 22.8447615402
Gradients: w=>0.0026848886, b=>-0.0463215751
Iteration 9423: Achieved Loss=> 22.8447593879
Gradients: w=>0.0026834613, b=>-0.0462969492
Iteration 9424: Achieved Loss=> 22.8447572378
Gradients: w=>0.0026820347, b=>-0.0462723363
Iteration 9425: Achieved Loss=> 22.8447550901
Gradients: w=>0.0026806088, b=>-0.0462477365
Iteration 9426: Achieved Loss=> 22.8447529446
Gradients: w=>0.0026791837, b=>-0.0462231498
Iteration 9427: Achieved Loss=> 22.8447508014
Gradients: w=>0.0026777594, b=>-0.0461985761
Iteration 9428: Achieved Loss=> 22.8447486605
Gradients: w=>0.0026763358, b=>-0.0461740155
Iteration 9429: Achieved Loss=> 22.8447465219
Gradients: w=>0.0026749130, b=>-0.0461494680
Iteration 9430: Achieved Loss=> 22.8447443855
Gradients: w=>0.0026734909, b=>-0.0461249335
Iteration 9431: Achieved Loss=> 22.8447422514
Gradients: w=>0.0026720696, b=>-0.0461004121
Iteration 9432: Achieved Loss=> 22.8447401196
Gradients: w=>0.0026706491, b=>-0.0460759037
Iteration 9433: Achieved Loss=> 22.8447379900
Gradients: w=>0.0026692293, b=>-0.0460514084
Iteration 9434: Achieved Loss=> 22.8447358628
Gradients: w=>0.0026678102, b=>-0.0460269260
Iteration 9435: Achieved Loss=> 22.8447337377
Gradients: w=>0.0026663919, b=>-0.0460024567
Iteration 9436: Achieved Loss=> 22.8447316150
Gradients: w=>0.0026649744, b=>-0.0459780004
Iteration 9437: Achieved Loss=> 22.8447294944
Gradients: w=>0.0026635576, b=>-0.0459535571
Iteration 9438: Achieved Loss=> 22.8447273762
Gradients: w=>0.0026621416, b=>-0.0459291267
Iteration 9439: Achieved Loss=> 22.8447252602
Gradients: w=>0.0026607263, b=>-0.0459047094
Iteration 9440: Achieved Loss=> 22.8447231464
Gradients: w=>0.0026593118, b=>-0.0458803050
Iteration 9441: Achieved Loss=> 22.8447210349
Gradients: w=>0.0026578980, b=>-0.0458559137
Iteration 9442: Achieved Loss=> 22.8447189256
Gradients: w=>0.0026564850, b=>-0.0458315353
Iteration 9443: Achieved Loss=> 22.8447168186
Gradients: w=>0.0026550727, b=>-0.0458071698
Iteration 9444: Achieved Loss=> 22.8447147138
Gradients: w=>0.0026536612, b=>-0.0457828173
Iteration 9445: Achieved Loss=> 22.8447126113
Gradients: w=>0.0026522504, b=>-0.0457584778
Iteration 9446: Achieved Loss=> 22.8447105110
Gradients: w=>0.0026508404, b=>-0.0457341511
Iteration 9447: Achieved Loss=> 22.8447084129
Gradients: w=>0.0026494311, b=>-0.0457098375
Iteration 9448: Achieved Loss=> 22.8447063170
Gradients: w=>0.0026480226, b=>-0.0456855367
Iteration 9449: Achieved Loss=> 22.8447042234
Gradients: w=>0.0026466149, b=>-0.0456612489
Iteration 9450: Achieved Loss=> 22.8447021320
Gradients: w=>0.0026452078, b=>-0.0456369740
Iteration 9451: Achieved Loss=> 22.8447000428
Gradients: w=>0.0026438016, b=>-0.0456127119
Iteration 9452: Achieved Loss=> 22.8446979559
Gradients: w=>0.0026423960, b=>-0.0455884628
Iteration 9453: Achieved Loss=> 22.8446958711
Gradients: w=>0.0026409913, b=>-0.0455642266
Iteration 9454: Achieved Loss=> 22.8446937886
Gradients: w=>0.0026395872, b=>-0.0455400033
Iteration 9455: Achieved Loss=> 22.8446917083
Gradients: w=>0.0026381839, b=>-0.0455157928
Iteration 9456: Achieved Loss=> 22.8446896302
Gradients: w=>0.0026367814, b=>-0.0454915952
Iteration 9457: Achieved Loss=> 22.8446875543
Gradients: w=>0.0026353796, b=>-0.0454674105
Iteration 9458: Achieved Loss=> 22.8446854806
Gradients: w=>0.0026339786, b=>-0.0454432386
Iteration 9459: Achieved Loss=> 22.8446834092
Gradients: w=>0.0026325782, b=>-0.0454190796
Iteration 9460: Achieved Loss=> 22.8446813399
Gradients: w=>0.0026311787, b=>-0.0453949334
Iteration 9461: Achieved Loss=> 22.8446792728
Gradients: w=>0.0026297799, b=>-0.0453708001
Iteration 9462: Achieved Loss=> 22.8446772079
Gradients: w=>0.0026283818, b=>-0.0453466796
Iteration 9463: Achieved Loss=> 22.8446751453
Gradients: w=>0.0026269845, b=>-0.0453225719
Iteration 9464: Achieved Loss=> 22.8446730848
Gradients: w=>0.0026255879, b=>-0.0452984770
Iteration 9465: Achieved Loss=> 22.8446710265
Gradients: w=>0.0026241920, b=>-0.0452743949
Iteration 9466: Achieved Loss=> 22.8446689704
Gradients: w=>0.0026227969, b=>-0.0452503257
Iteration 9467: Achieved Loss=> 22.8446669164
Gradients: w=>0.0026214026, b=>-0.0452262692
Iteration 9468: Achieved Loss=> 22.8446648647
Gradients: w=>0.0026200090, b=>-0.0452022255
Iteration 9469: Achieved Loss=> 22.8446628151
Gradients: w=>0.0026186161, b=>-0.0451781947
Iteration 9470: Achieved Loss=> 22.8446607678
Gradients: w=>0.0026172240, b=>-0.0451541765
Iteration 9471: Achieved Loss=> 22.8446587226
Gradients: w=>0.0026158326, b=>-0.0451301712
Iteration 9472: Achieved Loss=> 22.8446566795
Gradients: w=>0.0026144419, b=>-0.0451061786
Iteration 9473: Achieved Loss=> 22.8446546387
Gradients: w=>0.0026130520, b=>-0.0450821988
Iteration 9474: Achieved Loss=> 22.8446526000
Gradients: w=>0.0026116628, b=>-0.0450582317
Iteration 9475: Achieved Loss=> 22.8446505634
Gradients: w=>0.0026102744, b=>-0.0450342774
Iteration 9476: Achieved Loss=> 22.8446485291
Gradients: w=>0.0026088867, b=>-0.0450103358
Iteration 9477: Achieved Loss=> 22.8446464969
Gradients: w=>0.0026074997, b=>-0.0449864069
Iteration 9478: Achieved Loss=> 22.8446444669
Gradients: w=>0.0026061135, b=>-0.0449624907
Iteration 9479: Achieved Loss=> 22.8446424390
Gradients: w=>0.0026047280, b=>-0.0449385873
Iteration 9480: Achieved Loss=> 22.8446404133
Gradients: w=>0.0026033432, b=>-0.0449146966
Iteration 9481: Achieved Loss=> 22.8446383897
Gradients: w=>0.0026019592, b=>-0.0448908185
Iteration 9482: Achieved Loss=> 22.8446363683
Gradients: w=>0.0026005759, b=>-0.0448669532
Iteration 9483: Achieved Loss=> 22.8446343490
Gradients: w=>0.0025991934, b=>-0.0448431005
Iteration 9484: Achieved Loss=> 22.8446323319
Gradients: w=>0.0025978116, b=>-0.0448192606
Iteration 9485: Achieved Loss=> 22.8446303169
Gradients: w=>0.0025964305, b=>-0.0447954333
Iteration 9486: Achieved Loss=> 22.8446283041
Gradients: w=>0.0025950502, b=>-0.0447716186
Iteration 9487: Achieved Loss=> 22.8446262934
Gradients: w=>0.0025936705, b=>-0.0447478167
Iteration 9488: Achieved Loss=> 22.8446242848
Gradients: w=>0.0025922917, b=>-0.0447240274
Iteration 9489: Achieved Loss=> 22.8446222784
Gradients: w=>0.0025909135, b=>-0.0447002507
Iteration 9490: Achieved Loss=> 22.8446202741
Gradients: w=>0.0025895361, b=>-0.0446764867
Iteration 9491: Achieved Loss=> 22.8446182719
Gradients: w=>0.0025881594, b=>-0.0446527353
Iteration 9492: Achieved Loss=> 22.8446162719
Gradients: w=>0.0025867835, b=>-0.0446289965
Iteration 9493: Achieved Loss=> 22.8446142740
Gradients: w=>0.0025854083, b=>-0.0446052704
Iteration 9494: Achieved Loss=> 22.8446122782
Gradients: w=>0.0025840338, b=>-0.0445815568
Iteration 9495: Achieved Loss=> 22.8446102845
Gradients: w=>0.0025826600, b=>-0.0445578559
Iteration 9496: Achieved Loss=> 22.8446082930
Gradients: w=>0.0025812870, b=>-0.0445341676
Iteration 9497: Achieved Loss=> 22.8446063036
Gradients: w=>0.0025799147, b=>-0.0445104919
Iteration 9498: Achieved Loss=> 22.8446043163
Gradients: w=>0.0025785432, b=>-0.0444868287
Iteration 9499: Achieved Loss=> 22.8446023311
Gradients: w=>0.0025771723, b=>-0.0444631782
Iteration 9500: Achieved Loss=> 22.8446003480
Gradients: w=>0.0025758022, b=>-0.0444395402
Iteration 9501: Achieved Loss=> 22.8445983670
Gradients: w=>0.0025744329, b=>-0.0444159147
Iteration 9502: Achieved Loss=> 22.8445963881
Gradients: w=>0.0025730642, b=>-0.0443923019
Iteration 9503: Achieved Loss=> 22.8445944113
Gradients: w=>0.0025716963, b=>-0.0443687016
Iteration 9504: Achieved Loss=> 22.8445924367
Gradients: w=>0.0025703291, b=>-0.0443451138
Iteration 9505: Achieved Loss=> 22.8445904641
Gradients: w=>0.0025689626, b=>-0.0443215386
Iteration 9506: Achieved Loss=> 22.8445884936
Gradients: w=>0.0025675969, b=>-0.0442979759
Iteration 9507: Achieved Loss=> 22.8445865252
Gradients: w=>0.0025662319, b=>-0.0442744257
Iteration 9508: Achieved Loss=> 22.8445845590
Gradients: w=>0.0025648676, b=>-0.0442508881
Iteration 9509: Achieved Loss=> 22.8445825948
Gradients: w=>0.0025635040, b=>-0.0442273630
Iteration 9510: Achieved Loss=> 22.8445806326
Gradients: w=>0.0025621412, b=>-0.0442038503
Iteration 9511: Achieved Loss=> 22.8445786726
Gradients: w=>0.0025607791, b=>-0.0441803502
Iteration 9512: Achieved Loss=> 22.8445767147
Gradients: w=>0.0025594177, b=>-0.0441568626
Iteration 9513: Achieved Loss=> 22.8445747588
Gradients: w=>0.0025580570, b=>-0.0441333874
Iteration 9514: Achieved Loss=> 22.8445728050
Gradients: w=>0.0025566971, b=>-0.0441099248
Iteration 9515: Achieved Loss=> 22.8445708533
Gradients: w=>0.0025553379, b=>-0.0440864746
Iteration 9516: Achieved Loss=> 22.8445689037
Gradients: w=>0.0025539794, b=>-0.0440630369
Iteration 9517: Achieved Loss=> 22.8445669562
Gradients: w=>0.0025526216, b=>-0.0440396116
Iteration 9518: Achieved Loss=> 22.8445650107
Gradients: w=>0.0025512645, b=>-0.0440161988
Iteration 9519: Achieved Loss=> 22.8445630673
Gradients: w=>0.0025499082, b=>-0.0439927984
Iteration 9520: Achieved Loss=> 22.8445611259
Gradients: w=>0.0025485526, b=>-0.0439694105
Iteration 9521: Achieved Loss=> 22.8445591866
Gradients: w=>0.0025471977, b=>-0.0439460350
Iteration 9522: Achieved Loss=> 22.8445572494
Gradients: w=>0.0025458436, b=>-0.0439226719
Iteration 9523: Achieved Loss=> 22.8445553142
Gradients: w=>0.0025444901, b=>-0.0438993213
Iteration 9524: Achieved Loss=> 22.8445533811
Gradients: w=>0.0025431374, b=>-0.0438759831
Iteration 9525: Achieved Loss=> 22.8445514501
Gradients: w=>0.0025417854, b=>-0.0438526573
Iteration 9526: Achieved Loss=> 22.8445495210
Gradients: w=>0.0025404341, b=>-0.0438293438
Iteration 9527: Achieved Loss=> 22.8445475941
Gradients: w=>0.0025390835, b=>-0.0438060428
Iteration 9528: Achieved Loss=> 22.8445456692
Gradients: w=>0.0025377336, b=>-0.0437827542
Iteration 9529: Achieved Loss=> 22.8445437463
Gradients: w=>0.0025363845, b=>-0.0437594779
Iteration 9530: Achieved Loss=> 22.8445418255
Gradients: w=>0.0025350361, b=>-0.0437362140
Iteration 9531: Achieved Loss=> 22.8445399067
Gradients: w=>0.0025336884, b=>-0.0437129625
Iteration 9532: Achieved Loss=> 22.8445379900
Gradients: w=>0.0025323414, b=>-0.0436897234
Iteration 9533: Achieved Loss=> 22.8445360753
Gradients: w=>0.0025309951, b=>-0.0436664966
Iteration 9534: Achieved Loss=> 22.8445341627
Gradients: w=>0.0025296496, b=>-0.0436432821
Iteration 9535: Achieved Loss=> 22.8445322520
Gradients: w=>0.0025283047, b=>-0.0436200800
Iteration 9536: Achieved Loss=> 22.8445303434
Gradients: w=>0.0025269606, b=>-0.0435968902
Iteration 9537: Achieved Loss=> 22.8445284369
Gradients: w=>0.0025256172, b=>-0.0435737128
Iteration 9538: Achieved Loss=> 22.8445265323
Gradients: w=>0.0025242745, b=>-0.0435505477
Iteration 9539: Achieved Loss=> 22.8445246298
Gradients: w=>0.0025229325, b=>-0.0435273949
Iteration 9540: Achieved Loss=> 22.8445227293
Gradients: w=>0.0025215913, b=>-0.0435042544
Iteration 9541: Achieved Loss=> 22.8445208308
Gradients: w=>0.0025202507, b=>-0.0434811262
Iteration 9542: Achieved Loss=> 22.8445189344
Gradients: w=>0.0025189109, b=>-0.0434580103
Iteration 9543: Achieved Loss=> 22.8445170399
Gradients: w=>0.0025175717, b=>-0.0434349067
Iteration 9544: Achieved Loss=> 22.8445151475
Gradients: w=>0.0025162333, b=>-0.0434118153
Iteration 9545: Achieved Loss=> 22.8445132571
Gradients: w=>0.0025148956, b=>-0.0433887363
Iteration 9546: Achieved Loss=> 22.8445113687
Gradients: w=>0.0025135586, b=>-0.0433656695
Iteration 9547: Achieved Loss=> 22.8445094823
Gradients: w=>0.0025122223, b=>-0.0433426150
Iteration 9548: Achieved Loss=> 22.8445075979
Gradients: w=>0.0025108867, b=>-0.0433195727
Iteration 9549: Achieved Loss=> 22.8445057155
Gradients: w=>0.0025095519, b=>-0.0432965427
Iteration 9550: Achieved Loss=> 22.8445038351
Gradients: w=>0.0025082177, b=>-0.0432735249
Iteration 9551: Achieved Loss=> 22.8445019567
Gradients: w=>0.0025068843, b=>-0.0432505194
Iteration 9552: Achieved Loss=> 22.8445000803
Gradients: w=>0.0025055515, b=>-0.0432275261
Iteration 9553: Achieved Loss=> 22.8444982059
Gradients: w=>0.0025042195, b=>-0.0432045450
Iteration 9554: Achieved Loss=> 22.8444963335
Gradients: w=>0.0025028882, b=>-0.0431815761
Iteration 9555: Achieved Loss=> 22.8444944631
Gradients: w=>0.0025015576, b=>-0.0431586195
Iteration 9556: Achieved Loss=> 22.8444925947
Gradients: w=>0.0025002277, b=>-0.0431356750
Iteration 9557: Achieved Loss=> 22.8444907283
Gradients: w=>0.0024988985, b=>-0.0431127428
Iteration 9558: Achieved Loss=> 22.8444888638
Gradients: w=>0.0024975700, b=>-0.0430898227
Iteration 9559: Achieved Loss=> 22.8444870013
Gradients: w=>0.0024962422, b=>-0.0430669149
Iteration 9560: Achieved Loss=> 22.8444851408
Gradients: w=>0.0024949151, b=>-0.0430440192
Iteration 9561: Achieved Loss=> 22.8444832823
Gradients: w=>0.0024935888, b=>-0.0430211356
Iteration 9562: Achieved Loss=> 22.8444814258
Gradients: w=>0.0024922631, b=>-0.0429982643
Iteration 9563: Achieved Loss=> 22.8444795712
Gradients: w=>0.0024909381, b=>-0.0429754051
Iteration 9564: Achieved Loss=> 22.8444777186
Gradients: w=>0.0024896139, b=>-0.0429525581
Iteration 9565: Achieved Loss=> 22.8444758680
Gradients: w=>0.0024882903, b=>-0.0429297232
Iteration 9566: Achieved Loss=> 22.8444740193
Gradients: w=>0.0024869675, b=>-0.0429069004
Iteration 9567: Achieved Loss=> 22.8444721726
Gradients: w=>0.0024856453, b=>-0.0428840898
Iteration 9568: Achieved Loss=> 22.8444703279
Gradients: w=>0.0024843239, b=>-0.0428612913
Iteration 9569: Achieved Loss=> 22.8444684851
Gradients: w=>0.0024830031, b=>-0.0428385049
Iteration 9570: Achieved Loss=> 22.8444666443
Gradients: w=>0.0024816831, b=>-0.0428157306
Iteration 9571: Achieved Loss=> 22.8444648054
Gradients: w=>0.0024803637, b=>-0.0427929685
Iteration 9572: Achieved Loss=> 22.8444629685
Gradients: w=>0.0024790451, b=>-0.0427702184
Iteration 9573: Achieved Loss=> 22.8444611336
Gradients: w=>0.0024777272, b=>-0.0427474805
Iteration 9574: Achieved Loss=> 22.8444593006
Gradients: w=>0.0024764099, b=>-0.0427247546
Iteration 9575: Achieved Loss=> 22.8444574695
Gradients: w=>0.0024750934, b=>-0.0427020408
Iteration 9576: Achieved Loss=> 22.8444556404
Gradients: w=>0.0024737776, b=>-0.0426793391
Iteration 9577: Achieved Loss=> 22.8444538133
Gradients: w=>0.0024724624, b=>-0.0426566494
Iteration 9578: Achieved Loss=> 22.8444519881
Gradients: w=>0.0024711480, b=>-0.0426339718
Iteration 9579: Achieved Loss=> 22.8444501648
Gradients: w=>0.0024698342, b=>-0.0426113063
Iteration 9580: Achieved Loss=> 22.8444483434
Gradients: w=>0.0024685212, b=>-0.0425886528
Iteration 9581: Achieved Loss=> 22.8444465240
Gradients: w=>0.0024672089, b=>-0.0425660114
Iteration 9582: Achieved Loss=> 22.8444447066
Gradients: w=>0.0024658972, b=>-0.0425433820
Iteration 9583: Achieved Loss=> 22.8444428910
Gradients: w=>0.0024645863, b=>-0.0425207646
Iteration 9584: Achieved Loss=> 22.8444410774
Gradients: w=>0.0024632760, b=>-0.0424981593
Iteration 9585: Achieved Loss=> 22.8444392657
Gradients: w=>0.0024619665, b=>-0.0424755660
Iteration 9586: Achieved Loss=> 22.8444374560
Gradients: w=>0.0024606576, b=>-0.0424529847
Iteration 9587: Achieved Loss=> 22.8444356482
Gradients: w=>0.0024593495, b=>-0.0424304153
Iteration 9588: Achieved Loss=> 22.8444338423
Gradients: w=>0.0024580420, b=>-0.0424078580
Iteration 9589: Achieved Loss=> 22.8444320383
Gradients: w=>0.0024567352, b=>-0.0423853127
Iteration 9590: Achieved Loss=> 22.8444302362
Gradients: w=>0.0024554291, b=>-0.0423627794
Iteration 9591: Achieved Loss=> 22.8444284360
Gradients: w=>0.0024541238, b=>-0.0423402580
Iteration 9592: Achieved Loss=> 22.8444266378
Gradients: w=>0.0024528191, b=>-0.0423177486
Iteration 9593: Achieved Loss=> 22.8444248415
Gradients: w=>0.0024515151, b=>-0.0422952512
Iteration 9594: Achieved Loss=> 22.8444230470
Gradients: w=>0.0024502118, b=>-0.0422727658
Iteration 9595: Achieved Loss=> 22.8444212545
Gradients: w=>0.0024489092, b=>-0.0422502923
Iteration 9596: Achieved Loss=> 22.8444194639
Gradients: w=>0.0024476073, b=>-0.0422278307
Iteration 9597: Achieved Loss=> 22.8444176752
Gradients: w=>0.0024463060, b=>-0.0422053811
Iteration 9598: Achieved Loss=> 22.8444158884
Gradients: w=>0.0024450055, b=>-0.0421829434
Iteration 9599: Achieved Loss=> 22.8444141035
Gradients: w=>0.0024437057, b=>-0.0421605177
Iteration 9600: Achieved Loss=> 22.8444123205
Gradients: w=>0.0024424065, b=>-0.0421381039
Iteration 9601: Achieved Loss=> 22.8444105394
Gradients: w=>0.0024411081, b=>-0.0421157020
Iteration 9602: Achieved Loss=> 22.8444087602
Gradients: w=>0.0024398103, b=>-0.0420933120
Iteration 9603: Achieved Loss=> 22.8444069828
Gradients: w=>0.0024385132, b=>-0.0420709339
Iteration 9604: Achieved Loss=> 22.8444052074
Gradients: w=>0.0024372168, b=>-0.0420485677
Iteration 9605: Achieved Loss=> 22.8444034339
Gradients: w=>0.0024359211, b=>-0.0420262134
Iteration 9606: Achieved Loss=> 22.8444016622
Gradients: w=>0.0024346261, b=>-0.0420038709
Iteration 9607: Achieved Loss=> 22.8443998924
Gradients: w=>0.0024333318, b=>-0.0419815404
Iteration 9608: Achieved Loss=> 22.8443981245
Gradients: w=>0.0024320382, b=>-0.0419592217
Iteration 9609: Achieved Loss=> 22.8443963585
Gradients: w=>0.0024307452, b=>-0.0419369149
Iteration 9610: Achieved Loss=> 22.8443945943
Gradients: w=>0.0024294529, b=>-0.0419146199
Iteration 9611: Achieved Loss=> 22.8443928321
Gradients: w=>0.0024281614, b=>-0.0418923368
Iteration 9612: Achieved Loss=> 22.8443910717
Gradients: w=>0.0024268705, b=>-0.0418700656
Iteration 9613: Achieved Loss=> 22.8443893132
Gradients: w=>0.0024255803, b=>-0.0418478062
Iteration 9614: Achieved Loss=> 22.8443875565
Gradients: w=>0.0024242908, b=>-0.0418255586
Iteration 9615: Achieved Loss=> 22.8443858017
Gradients: w=>0.0024230020, b=>-0.0418033229
Iteration 9616: Achieved Loss=> 22.8443840488
Gradients: w=>0.0024217138, b=>-0.0417810989
Iteration 9617: Achieved Loss=> 22.8443822977
Gradients: w=>0.0024204264, b=>-0.0417588868
Iteration 9618: Achieved Loss=> 22.8443805485
Gradients: w=>0.0024191396, b=>-0.0417366865
Iteration 9619: Achieved Loss=> 22.8443788012
Gradients: w=>0.0024178535, b=>-0.0417144980
Iteration 9620: Achieved Loss=> 22.8443770557
Gradients: w=>0.0024165681, b=>-0.0416923213
Iteration 9621: Achieved Loss=> 22.8443753121
Gradients: w=>0.0024152834, b=>-0.0416701564
Iteration 9622: Achieved Loss=> 22.8443735703
Gradients: w=>0.0024139993, b=>-0.0416480032
Iteration 9623: Achieved Loss=> 22.8443718304
Gradients: w=>0.0024127160, b=>-0.0416258619
Iteration 9624: Achieved Loss=> 22.8443700923
Gradients: w=>0.0024114333, b=>-0.0416037323
Iteration 9625: Achieved Loss=> 22.8443683561
Gradients: w=>0.0024101513, b=>-0.0415816145
Iteration 9626: Achieved Loss=> 22.8443666217
Gradients: w=>0.0024088700, b=>-0.0415595084
Iteration 9627: Achieved Loss=> 22.8443648892
Gradients: w=>0.0024075894, b=>-0.0415374141
Iteration 9628: Achieved Loss=> 22.8443631585
Gradients: w=>0.0024063094, b=>-0.0415153315
Iteration 9629: Achieved Loss=> 22.8443614296
Gradients: w=>0.0024050301, b=>-0.0414932607
Iteration 9630: Achieved Loss=> 22.8443597026
Gradients: w=>0.0024037516, b=>-0.0414712016
Iteration 9631: Achieved Loss=> 22.8443579774
Gradients: w=>0.0024024737, b=>-0.0414491543
Iteration 9632: Achieved Loss=> 22.8443562541
Gradients: w=>0.0024011964, b=>-0.0414271186
Iteration 9633: Achieved Loss=> 22.8443545326
Gradients: w=>0.0023999199, b=>-0.0414050947
Iteration 9634: Achieved Loss=> 22.8443528129
Gradients: w=>0.0023986440, b=>-0.0413830825
Iteration 9635: Achieved Loss=> 22.8443510950
Gradients: w=>0.0023973688, b=>-0.0413610820
Iteration 9636: Achieved Loss=> 22.8443493790
Gradients: w=>0.0023960943, b=>-0.0413390931
Iteration 9637: Achieved Loss=> 22.8443476648
Gradients: w=>0.0023948205, b=>-0.0413171160
Iteration 9638: Achieved Loss=> 22.8443459524
Gradients: w=>0.0023935473, b=>-0.0412951506
Iteration 9639: Achieved Loss=> 22.8443442419
Gradients: w=>0.0023922748, b=>-0.0412731968
Iteration 9640: Achieved Loss=> 22.8443425331
Gradients: w=>0.0023910030, b=>-0.0412512547
Iteration 9641: Achieved Loss=> 22.8443408262
Gradients: w=>0.0023897319, b=>-0.0412293243
Iteration 9642: Achieved Loss=> 22.8443391211
Gradients: w=>0.0023884614, b=>-0.0412074055
Iteration 9643: Achieved Loss=> 22.8443374178
Gradients: w=>0.0023871916, b=>-0.0411854984
Iteration 9644: Achieved Loss=> 22.8443357163
Gradients: w=>0.0023859225, b=>-0.0411636029
Iteration 9645: Achieved Loss=> 22.8443340166
Gradients: w=>0.0023846541, b=>-0.0411417190
Iteration 9646: Achieved Loss=> 22.8443323187
Gradients: w=>0.0023833864, b=>-0.0411198468
Iteration 9647: Achieved Loss=> 22.8443306226
Gradients: w=>0.0023821193, b=>-0.0410979863
Iteration 9648: Achieved Loss=> 22.8443289284
Gradients: w=>0.0023808529, b=>-0.0410761373
Iteration 9649: Achieved Loss=> 22.8443272359
Gradients: w=>0.0023795871, b=>-0.0410543000
Iteration 9650: Achieved Loss=> 22.8443255452
Gradients: w=>0.0023783221, b=>-0.0410324743
Iteration 9651: Achieved Loss=> 22.8443238564
Gradients: w=>0.0023770577, b=>-0.0410106601
Iteration 9652: Achieved Loss=> 22.8443221693
Gradients: w=>0.0023757940, b=>-0.0409888576
Iteration 9653: Achieved Loss=> 22.8443204840
Gradients: w=>0.0023745309, b=>-0.0409670667
Iteration 9654: Achieved Loss=> 22.8443188005
Gradients: w=>0.0023732685, b=>-0.0409452873
Iteration 9655: Achieved Loss=> 22.8443171188
Gradients: w=>0.0023720068, b=>-0.0409235196
Iteration 9656: Achieved Loss=> 22.8443154389
Gradients: w=>0.0023707458, b=>-0.0409017634
Iteration 9657: Achieved Loss=> 22.8443137608
Gradients: w=>0.0023694854, b=>-0.0408800187
Iteration 9658: Achieved Loss=> 22.8443120844
Gradients: w=>0.0023682258, b=>-0.0408582856
Iteration 9659: Achieved Loss=> 22.8443104099
Gradients: w=>0.0023669667, b=>-0.0408365641
Iteration 9660: Achieved Loss=> 22.8443087371
Gradients: w=>0.0023657084, b=>-0.0408148542
Iteration 9661: Achieved Loss=> 22.8443070661
Gradients: w=>0.0023644507, b=>-0.0407931557
Iteration 9662: Achieved Loss=> 22.8443053969
Gradients: w=>0.0023631937, b=>-0.0407714688
Iteration 9663: Achieved Loss=> 22.8443037294
Gradients: w=>0.0023619373, b=>-0.0407497935
Iteration 9664: Achieved Loss=> 22.8443020637
Gradients: w=>0.0023606817, b=>-0.0407281296
Iteration 9665: Achieved Loss=> 22.8443003998
Gradients: w=>0.0023594267, b=>-0.0407064773
Iteration 9666: Achieved Loss=> 22.8442987377
Gradients: w=>0.0023581723, b=>-0.0406848365
Iteration 9667: Achieved Loss=> 22.8442970773
Gradients: w=>0.0023569186, b=>-0.0406632072
Iteration 9668: Achieved Loss=> 22.8442954187
Gradients: w=>0.0023556656, b=>-0.0406415894
Iteration 9669: Achieved Loss=> 22.8442937618
Gradients: w=>0.0023544133, b=>-0.0406199831
Iteration 9670: Achieved Loss=> 22.8442921067
Gradients: w=>0.0023531616, b=>-0.0405983882
Iteration 9671: Achieved Loss=> 22.8442904534
Gradients: w=>0.0023519106, b=>-0.0405768049
Iteration 9672: Achieved Loss=> 22.8442888018
Gradients: w=>0.0023506602, b=>-0.0405552330
Iteration 9673: Achieved Loss=> 22.8442871520
Gradients: w=>0.0023494106, b=>-0.0405336726
Iteration 9674: Achieved Loss=> 22.8442855040
Gradients: w=>0.0023481615, b=>-0.0405121237
Iteration 9675: Achieved Loss=> 22.8442838577
Gradients: w=>0.0023469132, b=>-0.0404905862
Iteration 9676: Achieved Loss=> 22.8442822131
Gradients: w=>0.0023456655, b=>-0.0404690601
Iteration 9677: Achieved Loss=> 22.8442805703
Gradients: w=>0.0023444185, b=>-0.0404475455
Iteration 9678: Achieved Loss=> 22.8442789292
Gradients: w=>0.0023431721, b=>-0.0404260424
Iteration 9679: Achieved Loss=> 22.8442772899
Gradients: w=>0.0023419264, b=>-0.0404045507
Iteration 9680: Achieved Loss=> 22.8442756523
Gradients: w=>0.0023406814, b=>-0.0403830704
Iteration 9681: Achieved Loss=> 22.8442740165
Gradients: w=>0.0023394370, b=>-0.0403616015
Iteration 9682: Achieved Loss=> 22.8442723824
Gradients: w=>0.0023381933, b=>-0.0403401440
Iteration 9683: Achieved Loss=> 22.8442707500
Gradients: w=>0.0023369502, b=>-0.0403186979
Iteration 9684: Achieved Loss=> 22.8442691194
Gradients: w=>0.0023357078, b=>-0.0402972633
Iteration 9685: Achieved Loss=> 22.8442674905
Gradients: w=>0.0023344661, b=>-0.0402758400
Iteration 9686: Achieved Loss=> 22.8442658634
Gradients: w=>0.0023332250, b=>-0.0402544282
Iteration 9687: Achieved Loss=> 22.8442642379
Gradients: w=>0.0023319846, b=>-0.0402330277
Iteration 9688: Achieved Loss=> 22.8442626142
Gradients: w=>0.0023307448, b=>-0.0402116386
Iteration 9689: Achieved Loss=> 22.8442609923
Gradients: w=>0.0023295057, b=>-0.0401902608
Iteration 9690: Achieved Loss=> 22.8442593720
Gradients: w=>0.0023282673, b=>-0.0401688944
Iteration 9691: Achieved Loss=> 22.8442577535
Gradients: w=>0.0023270295, b=>-0.0401475394
Iteration 9692: Achieved Loss=> 22.8442561367
Gradients: w=>0.0023257924, b=>-0.0401261958
Iteration 9693: Achieved Loss=> 22.8442545216
Gradients: w=>0.0023245559, b=>-0.0401048634
Iteration 9694: Achieved Loss=> 22.8442529082
Gradients: w=>0.0023233201, b=>-0.0400835425
Iteration 9695: Achieved Loss=> 22.8442512965
Gradients: w=>0.0023220850, b=>-0.0400622328
Iteration 9696: Achieved Loss=> 22.8442496866
Gradients: w=>0.0023208505, b=>-0.0400409345
Iteration 9697: Achieved Loss=> 22.8442480784
Gradients: w=>0.0023196167, b=>-0.0400196475
Iteration 9698: Achieved Loss=> 22.8442464718
Gradients: w=>0.0023183835, b=>-0.0399983718
Iteration 9699: Achieved Loss=> 22.8442448670
Gradients: w=>0.0023171510, b=>-0.0399771075
Iteration 9700: Achieved Loss=> 22.8442432639
Gradients: w=>0.0023159191, b=>-0.0399558544
Iteration 9701: Achieved Loss=> 22.8442416625
Gradients: w=>0.0023146879, b=>-0.0399346127
Iteration 9702: Achieved Loss=> 22.8442400628
Gradients: w=>0.0023134573, b=>-0.0399133822
Iteration 9703: Achieved Loss=> 22.8442384648
Gradients: w=>0.0023122274, b=>-0.0398921630
Iteration 9704: Achieved Loss=> 22.8442368685
Gradients: w=>0.0023109982, b=>-0.0398709551
Iteration 9705: Achieved Loss=> 22.8442352739
Gradients: w=>0.0023097696, b=>-0.0398497585
Iteration 9706: Achieved Loss=> 22.8442336809
Gradients: w=>0.0023085416, b=>-0.0398285731
Iteration 9707: Achieved Loss=> 22.8442320897
Gradients: w=>0.0023073143, b=>-0.0398073991
Iteration 9708: Achieved Loss=> 22.8442305002
Gradients: w=>0.0023060877, b=>-0.0397862362
Iteration 9709: Achieved Loss=> 22.8442289124
Gradients: w=>0.0023048617, b=>-0.0397650846
Iteration 9710: Achieved Loss=> 22.8442273262
Gradients: w=>0.0023036364, b=>-0.0397439443
Iteration 9711: Achieved Loss=> 22.8442257417
Gradients: w=>0.0023024117, b=>-0.0397228152
Iteration 9712: Achieved Loss=> 22.8442241590
Gradients: w=>0.0023011876, b=>-0.0397016973
Iteration 9713: Achieved Loss=> 22.8442225779
Gradients: w=>0.0022999643, b=>-0.0396805907
Iteration 9714: Achieved Loss=> 22.8442209984
Gradients: w=>0.0022987415, b=>-0.0396594953
Iteration 9715: Achieved Loss=> 22.8442194207
Gradients: w=>0.0022975195, b=>-0.0396384111
Iteration 9716: Achieved Loss=> 22.8442178446
Gradients: w=>0.0022962980, b=>-0.0396173381
Iteration 9717: Achieved Loss=> 22.8442162702
Gradients: w=>0.0022950772, b=>-0.0395962763
Iteration 9718: Achieved Loss=> 22.8442146975
Gradients: w=>0.0022938571, b=>-0.0395752257
Iteration 9719: Achieved Loss=> 22.8442131265
Gradients: w=>0.0022926376, b=>-0.0395541863
Iteration 9720: Achieved Loss=> 22.8442115571
Gradients: w=>0.0022914188, b=>-0.0395331580
Iteration 9721: Achieved Loss=> 22.8442099894
Gradients: w=>0.0022902006, b=>-0.0395121410
Iteration 9722: Achieved Loss=> 22.8442084234
Gradients: w=>0.0022889831, b=>-0.0394911351
Iteration 9723: Achieved Loss=> 22.8442068590
Gradients: w=>0.0022877662, b=>-0.0394701404
Iteration 9724: Achieved Loss=> 22.8442052963
Gradients: w=>0.0022865499, b=>-0.0394491569
Iteration 9725: Achieved Loss=> 22.8442037352
Gradients: w=>0.0022853343, b=>-0.0394281845
Iteration 9726: Achieved Loss=> 22.8442021758
Gradients: w=>0.0022841194, b=>-0.0394072233
Iteration 9727: Achieved Loss=> 22.8442006181
Gradients: w=>0.0022829051, b=>-0.0393862732
Iteration 9728: Achieved Loss=> 22.8441990620
Gradients: w=>0.0022816914, b=>-0.0393653342
Iteration 9729: Achieved Loss=> 22.8441975076
Gradients: w=>0.0022804784, b=>-0.0393444064
Iteration 9730: Achieved Loss=> 22.8441959548
Gradients: w=>0.0022792660, b=>-0.0393234897
Iteration 9731: Achieved Loss=> 22.8441944037
Gradients: w=>0.0022780543, b=>-0.0393025842
Iteration 9732: Achieved Loss=> 22.8441928543
Gradients: w=>0.0022768432, b=>-0.0392816897
Iteration 9733: Achieved Loss=> 22.8441913064
Gradients: w=>0.0022756327, b=>-0.0392608063
Iteration 9734: Achieved Loss=> 22.8441897603
Gradients: w=>0.0022744230, b=>-0.0392399341
Iteration 9735: Achieved Loss=> 22.8441882157
Gradients: w=>0.0022732138, b=>-0.0392190729
Iteration 9736: Achieved Loss=> 22.8441866728
Gradients: w=>0.0022720053, b=>-0.0391982229
Iteration 9737: Achieved Loss=> 22.8441851316
Gradients: w=>0.0022707974, b=>-0.0391773839
Iteration 9738: Achieved Loss=> 22.8441835920
Gradients: w=>0.0022695902, b=>-0.0391565560
Iteration 9739: Achieved Loss=> 22.8441820540
Gradients: w=>0.0022683836, b=>-0.0391357392
Iteration 9740: Achieved Loss=> 22.8441805176
Gradients: w=>0.0022671777, b=>-0.0391149334
Iteration 9741: Achieved Loss=> 22.8441789829
Gradients: w=>0.0022659724, b=>-0.0390941387
Iteration 9742: Achieved Loss=> 22.8441774499
Gradients: w=>0.0022647677, b=>-0.0390733551
Iteration 9743: Achieved Loss=> 22.8441759184
Gradients: w=>0.0022635637, b=>-0.0390525825
Iteration 9744: Achieved Loss=> 22.8441743886
Gradients: w=>0.0022623603, b=>-0.0390318209
Iteration 9745: Achieved Loss=> 22.8441728604
Gradients: w=>0.0022611576, b=>-0.0390110704
Iteration 9746: Achieved Loss=> 22.8441713338
Gradients: w=>0.0022599555, b=>-0.0389903309
Iteration 9747: Achieved Loss=> 22.8441698089
Gradients: w=>0.0022587540, b=>-0.0389696025
Iteration 9748: Achieved Loss=> 22.8441682855
Gradients: w=>0.0022575532, b=>-0.0389488850
Iteration 9749: Achieved Loss=> 22.8441667638
Gradients: w=>0.0022563530, b=>-0.0389281786
Iteration 9750: Achieved Loss=> 22.8441652437
Gradients: w=>0.0022551535, b=>-0.0389074832
Iteration 9751: Achieved Loss=> 22.8441637253
Gradients: w=>0.0022539545, b=>-0.0388867988
Iteration 9752: Achieved Loss=> 22.8441622084
Gradients: w=>0.0022527563, b=>-0.0388661254
Iteration 9753: Achieved Loss=> 22.8441606932
Gradients: w=>0.0022515586, b=>-0.0388454629
Iteration 9754: Achieved Loss=> 22.8441591795
Gradients: w=>0.0022503616, b=>-0.0388248115
Iteration 9755: Achieved Loss=> 22.8441576675
Gradients: w=>0.0022491653, b=>-0.0388041710
Iteration 9756: Achieved Loss=> 22.8441561571
Gradients: w=>0.0022479696, b=>-0.0387835416
Iteration 9757: Achieved Loss=> 22.8441546483
Gradients: w=>0.0022467745, b=>-0.0387629230
Iteration 9758: Achieved Loss=> 22.8441531410
Gradients: w=>0.0022455800, b=>-0.0387423155
Iteration 9759: Achieved Loss=> 22.8441516354
Gradients: w=>0.0022443862, b=>-0.0387217189
Iteration 9760: Achieved Loss=> 22.8441501314
Gradients: w=>0.0022431930, b=>-0.0387011332
Iteration 9761: Achieved Loss=> 22.8441486290
Gradients: w=>0.0022420005, b=>-0.0386805585
Iteration 9762: Achieved Loss=> 22.8441471282
Gradients: w=>0.0022408085, b=>-0.0386599947
Iteration 9763: Achieved Loss=> 22.8441456290
Gradients: w=>0.0022396173, b=>-0.0386394419
Iteration 9764: Achieved Loss=> 22.8441441314
Gradients: w=>0.0022384266, b=>-0.0386189000
Iteration 9765: Achieved Loss=> 22.8441426353
Gradients: w=>0.0022372366, b=>-0.0385983690
Iteration 9766: Achieved Loss=> 22.8441411409
Gradients: w=>0.0022360472, b=>-0.0385778489
Iteration 9767: Achieved Loss=> 22.8441396480
Gradients: w=>0.0022348585, b=>-0.0385573397
Iteration 9768: Achieved Loss=> 22.8441381568
Gradients: w=>0.0022336703, b=>-0.0385368415
Iteration 9769: Achieved Loss=> 22.8441366671
Gradients: w=>0.0022324828, b=>-0.0385163541
Iteration 9770: Achieved Loss=> 22.8441351790
Gradients: w=>0.0022312960, b=>-0.0384958776
Iteration 9771: Achieved Loss=> 22.8441336925
Gradients: w=>0.0022301098, b=>-0.0384754120
Iteration 9772: Achieved Loss=> 22.8441322075
Gradients: w=>0.0022289242, b=>-0.0384549573
Iteration 9773: Achieved Loss=> 22.8441307242
Gradients: w=>0.0022277392, b=>-0.0384345135
Iteration 9774: Achieved Loss=> 22.8441292424
Gradients: w=>0.0022265549, b=>-0.0384140805
Iteration 9775: Achieved Loss=> 22.8441277622
Gradients: w=>0.0022253712, b=>-0.0383936584
Iteration 9776: Achieved Loss=> 22.8441262836
Gradients: w=>0.0022241881, b=>-0.0383732472
Iteration 9777: Achieved Loss=> 22.8441248065
Gradients: w=>0.0022230056, b=>-0.0383528468
Iteration 9778: Achieved Loss=> 22.8441233310
Gradients: w=>0.0022218238, b=>-0.0383324572
Iteration 9779: Achieved Loss=> 22.8441218571
Gradients: w=>0.0022206426, b=>-0.0383120785
Iteration 9780: Achieved Loss=> 22.8441203847
Gradients: w=>0.0022194621, b=>-0.0382917106
Iteration 9781: Achieved Loss=> 22.8441189139
Gradients: w=>0.0022182821, b=>-0.0382713536
Iteration 9782: Achieved Loss=> 22.8441174447
Gradients: w=>0.0022171028, b=>-0.0382510074
Iteration 9783: Achieved Loss=> 22.8441159771
Gradients: w=>0.0022159242, b=>-0.0382306720
Iteration 9784: Achieved Loss=> 22.8441145109
Gradients: w=>0.0022147461, b=>-0.0382103474
Iteration 9785: Achieved Loss=> 22.8441130464
Gradients: w=>0.0022135687, b=>-0.0381900336
Iteration 9786: Achieved Loss=> 22.8441115834
Gradients: w=>0.0022123919, b=>-0.0381697306
Iteration 9787: Achieved Loss=> 22.8441101220
Gradients: w=>0.0022112157, b=>-0.0381494384
Iteration 9788: Achieved Loss=> 22.8441086621
Gradients: w=>0.0022100401, b=>-0.0381291570
Iteration 9789: Achieved Loss=> 22.8441072038
Gradients: w=>0.0022088652, b=>-0.0381088863
Iteration 9790: Achieved Loss=> 22.8441057470
Gradients: w=>0.0022076909, b=>-0.0380886265
Iteration 9791: Achieved Loss=> 22.8441042918
Gradients: w=>0.0022065172, b=>-0.0380683774
Iteration 9792: Achieved Loss=> 22.8441028381
Gradients: w=>0.0022053442, b=>-0.0380481391
Iteration 9793: Achieved Loss=> 22.8441013859
Gradients: w=>0.0022041718, b=>-0.0380279115
Iteration 9794: Achieved Loss=> 22.8440999353
Gradients: w=>0.0022030000, b=>-0.0380076947
Iteration 9795: Achieved Loss=> 22.8440984863
Gradients: w=>0.0022018288, b=>-0.0379874887
Iteration 9796: Achieved Loss=> 22.8440970388
Gradients: w=>0.0022006582, b=>-0.0379672933
Iteration 9797: Achieved Loss=> 22.8440955928
Gradients: w=>0.0021994883, b=>-0.0379471088
Iteration 9798: Achieved Loss=> 22.8440941484
Gradients: w=>0.0021983190, b=>-0.0379269349
Iteration 9799: Achieved Loss=> 22.8440927055
Gradients: w=>0.0021971503, b=>-0.0379067718
Iteration 9800: Achieved Loss=> 22.8440912641
Gradients: w=>0.0021959822, b=>-0.0378866194
Iteration 9801: Achieved Loss=> 22.8440898243
Gradients: w=>0.0021948147, b=>-0.0378664777
Iteration 9802: Achieved Loss=> 22.8440883860
Gradients: w=>0.0021936479, b=>-0.0378463467
Iteration 9803: Achieved Loss=> 22.8440869492
Gradients: w=>0.0021924817, b=>-0.0378262264
Iteration 9804: Achieved Loss=> 22.8440855139
Gradients: w=>0.0021913161, b=>-0.0378061169
Iteration 9805: Achieved Loss=> 22.8440840802
Gradients: w=>0.0021901511, b=>-0.0377860180
Iteration 9806: Achieved Loss=> 22.8440826480
Gradients: w=>0.0021889868, b=>-0.0377659298
Iteration 9807: Achieved Loss=> 22.8440812173
Gradients: w=>0.0021878231, b=>-0.0377458522
Iteration 9808: Achieved Loss=> 22.8440797882
Gradients: w=>0.0021866599, b=>-0.0377257854
Iteration 9809: Achieved Loss=> 22.8440783605
Gradients: w=>0.0021854974, b=>-0.0377057292
Iteration 9810: Achieved Loss=> 22.8440769344
Gradients: w=>0.0021843356, b=>-0.0376856837
Iteration 9811: Achieved Loss=> 22.8440755098
Gradients: w=>0.0021831743, b=>-0.0376656488
Iteration 9812: Achieved Loss=> 22.8440740867
Gradients: w=>0.0021820137, b=>-0.0376456246
Iteration 9813: Achieved Loss=> 22.8440726652
Gradients: w=>0.0021808536, b=>-0.0376256110
Iteration 9814: Achieved Loss=> 22.8440712451
Gradients: w=>0.0021796942, b=>-0.0376056081
Iteration 9815: Achieved Loss=> 22.8440698265
Gradients: w=>0.0021785354, b=>-0.0375856158
Iteration 9816: Achieved Loss=> 22.8440684095
Gradients: w=>0.0021773773, b=>-0.0375656341
Iteration 9817: Achieved Loss=> 22.8440669939
Gradients: w=>0.0021762197, b=>-0.0375456631
Iteration 9818: Achieved Loss=> 22.8440655799
Gradients: w=>0.0021750628, b=>-0.0375257026
Iteration 9819: Achieved Loss=> 22.8440641674
Gradients: w=>0.0021739064, b=>-0.0375057528
Iteration 9820: Achieved Loss=> 22.8440627563
Gradients: w=>0.0021727507, b=>-0.0374858136
Iteration 9821: Achieved Loss=> 22.8440613468
Gradients: w=>0.0021715956, b=>-0.0374658850
Iteration 9822: Achieved Loss=> 22.8440599388
Gradients: w=>0.0021704411, b=>-0.0374459670
Iteration 9823: Achieved Loss=> 22.8440585322
Gradients: w=>0.0021692873, b=>-0.0374260596
Iteration 9824: Achieved Loss=> 22.8440571272
Gradients: w=>0.0021681340, b=>-0.0374061627
Iteration 9825: Achieved Loss=> 22.8440557236
Gradients: w=>0.0021669813, b=>-0.0373862765
Iteration 9826: Achieved Loss=> 22.8440543216
Gradients: w=>0.0021658293, b=>-0.0373664008
Iteration 9827: Achieved Loss=> 22.8440529210
Gradients: w=>0.0021646779, b=>-0.0373465356
Iteration 9828: Achieved Loss=> 22.8440515219
Gradients: w=>0.0021635271, b=>-0.0373266811
Iteration 9829: Achieved Loss=> 22.8440501244
Gradients: w=>0.0021623769, b=>-0.0373068371
Iteration 9830: Achieved Loss=> 22.8440487283
Gradients: w=>0.0021612273, b=>-0.0372870036
Iteration 9831: Achieved Loss=> 22.8440473336
Gradients: w=>0.0021600783, b=>-0.0372671807
Iteration 9832: Achieved Loss=> 22.8440459405
Gradients: w=>0.0021589300, b=>-0.0372473683
Iteration 9833: Achieved Loss=> 22.8440445488
Gradients: w=>0.0021577822, b=>-0.0372275665
Iteration 9834: Achieved Loss=> 22.8440431587
Gradients: w=>0.0021566351, b=>-0.0372077752
Iteration 9835: Achieved Loss=> 22.8440417700
Gradients: w=>0.0021554885, b=>-0.0371879944
Iteration 9836: Achieved Loss=> 22.8440403827
Gradients: w=>0.0021543426, b=>-0.0371682241
Iteration 9837: Achieved Loss=> 22.8440389970
Gradients: w=>0.0021531973, b=>-0.0371484643
Iteration 9838: Achieved Loss=> 22.8440376127
Gradients: w=>0.0021520526, b=>-0.0371287151
Iteration 9839: Achieved Loss=> 22.8440362299
Gradients: w=>0.0021509085, b=>-0.0371089763
Iteration 9840: Achieved Loss=> 22.8440348486
Gradients: w=>0.0021497650, b=>-0.0370892480
Iteration 9841: Achieved Loss=> 22.8440334687
Gradients: w=>0.0021486221, b=>-0.0370695302
Iteration 9842: Achieved Loss=> 22.8440320903
Gradients: w=>0.0021474798, b=>-0.0370498229
Iteration 9843: Achieved Loss=> 22.8440307134
Gradients: w=>0.0021463382, b=>-0.0370301261
Iteration 9844: Achieved Loss=> 22.8440293379
Gradients: w=>0.0021451971, b=>-0.0370104398
Iteration 9845: Achieved Loss=> 22.8440279639
Gradients: w=>0.0021440567, b=>-0.0369907639
Iteration 9846: Achieved Loss=> 22.8440265913
Gradients: w=>0.0021429168, b=>-0.0369710984
Iteration 9847: Achieved Loss=> 22.8440252202
Gradients: w=>0.0021417776, b=>-0.0369514435
Iteration 9848: Achieved Loss=> 22.8440238506
Gradients: w=>0.0021406389, b=>-0.0369317990
Iteration 9849: Achieved Loss=> 22.8440224824
Gradients: w=>0.0021395009, b=>-0.0369121649
Iteration 9850: Achieved Loss=> 22.8440211157
Gradients: w=>0.0021383635, b=>-0.0368925412
Iteration 9851: Achieved Loss=> 22.8440197504
Gradients: w=>0.0021372267, b=>-0.0368729280
Iteration 9852: Achieved Loss=> 22.8440183866
Gradients: w=>0.0021360905, b=>-0.0368533253
Iteration 9853: Achieved Loss=> 22.8440170243
Gradients: w=>0.0021349548, b=>-0.0368337329
Iteration 9854: Achieved Loss=> 22.8440156633
Gradients: w=>0.0021338198, b=>-0.0368141510
Iteration 9855: Achieved Loss=> 22.8440143039
Gradients: w=>0.0021326854, b=>-0.0367945794
Iteration 9856: Achieved Loss=> 22.8440129458
Gradients: w=>0.0021315516, b=>-0.0367750183
Iteration 9857: Achieved Loss=> 22.8440115893
Gradients: w=>0.0021304184, b=>-0.0367554676
Iteration 9858: Achieved Loss=> 22.8440102341
Gradients: w=>0.0021292858, b=>-0.0367359272
Iteration 9859: Achieved Loss=> 22.8440088804
Gradients: w=>0.0021281538, b=>-0.0367163973
Iteration 9860: Achieved Loss=> 22.8440075281
Gradients: w=>0.0021270225, b=>-0.0366968777
Iteration 9861: Achieved Loss=> 22.8440061773
Gradients: w=>0.0021258917, b=>-0.0366773685
Iteration 9862: Achieved Loss=> 22.8440048279
Gradients: w=>0.0021247615, b=>-0.0366578697
Iteration 9863: Achieved Loss=> 22.8440034800
Gradients: w=>0.0021236319, b=>-0.0366383813
Iteration 9864: Achieved Loss=> 22.8440021334
Gradients: w=>0.0021225029, b=>-0.0366189032
Iteration 9865: Achieved Loss=> 22.8440007884
Gradients: w=>0.0021213745, b=>-0.0365994354
Iteration 9866: Achieved Loss=> 22.8439994447
Gradients: w=>0.0021202467, b=>-0.0365799781
Iteration 9867: Achieved Loss=> 22.8439981025
Gradients: w=>0.0021191195, b=>-0.0365605310
Iteration 9868: Achieved Loss=> 22.8439967617
Gradients: w=>0.0021179929, b=>-0.0365410943
Iteration 9869: Achieved Loss=> 22.8439954223
Gradients: w=>0.0021168670, b=>-0.0365216680
Iteration 9870: Achieved Loss=> 22.8439940843
Gradients: w=>0.0021157416, b=>-0.0365022519
Iteration 9871: Achieved Loss=> 22.8439927478
Gradients: w=>0.0021146168, b=>-0.0364828462
Iteration 9872: Achieved Loss=> 22.8439914127
Gradients: w=>0.0021134926, b=>-0.0364634508
Iteration 9873: Achieved Loss=> 22.8439900790
Gradients: w=>0.0021123690, b=>-0.0364440657
Iteration 9874: Achieved Loss=> 22.8439887467
Gradients: w=>0.0021112460, b=>-0.0364246909
Iteration 9875: Achieved Loss=> 22.8439874158
Gradients: w=>0.0021101236, b=>-0.0364053264
Iteration 9876: Achieved Loss=> 22.8439860864
Gradients: w=>0.0021090018, b=>-0.0363859723
Iteration 9877: Achieved Loss=> 22.8439847584
Gradients: w=>0.0021078806, b=>-0.0363666284
Iteration 9878: Achieved Loss=> 22.8439834317
Gradients: w=>0.0021067599, b=>-0.0363472947
Iteration 9879: Achieved Loss=> 22.8439821065
Gradients: w=>0.0021056399, b=>-0.0363279714
Iteration 9880: Achieved Loss=> 22.8439807827
Gradients: w=>0.0021045205, b=>-0.0363086583
Iteration 9881: Achieved Loss=> 22.8439794603
Gradients: w=>0.0021034017, b=>-0.0362893555
Iteration 9882: Achieved Loss=> 22.8439781393
Gradients: w=>0.0021022834, b=>-0.0362700630
Iteration 9883: Achieved Loss=> 22.8439768197
Gradients: w=>0.0021011658, b=>-0.0362507807
Iteration 9884: Achieved Loss=> 22.8439755016
Gradients: w=>0.0021000488, b=>-0.0362315087
Iteration 9885: Achieved Loss=> 22.8439741848
Gradients: w=>0.0020989323, b=>-0.0362122469
Iteration 9886: Achieved Loss=> 22.8439728694
Gradients: w=>0.0020978165, b=>-0.0361929954
Iteration 9887: Achieved Loss=> 22.8439715554
Gradients: w=>0.0020967012, b=>-0.0361737541
Iteration 9888: Achieved Loss=> 22.8439702428
Gradients: w=>0.0020955865, b=>-0.0361545230
Iteration 9889: Achieved Loss=> 22.8439689316
Gradients: w=>0.0020944724, b=>-0.0361353021
Iteration 9890: Achieved Loss=> 22.8439676218
Gradients: w=>0.0020933590, b=>-0.0361160915
Iteration 9891: Achieved Loss=> 22.8439663134
Gradients: w=>0.0020922461, b=>-0.0360968911
Iteration 9892: Achieved Loss=> 22.8439650064
Gradients: w=>0.0020911338, b=>-0.0360777009
Iteration 9893: Achieved Loss=> 22.8439637008
Gradients: w=>0.0020900220, b=>-0.0360585208
Iteration 9894: Achieved Loss=> 22.8439623965
Gradients: w=>0.0020889109, b=>-0.0360393510
Iteration 9895: Achieved Loss=> 22.8439610937
Gradients: w=>0.0020878004, b=>-0.0360201914
Iteration 9896: Achieved Loss=> 22.8439597922
Gradients: w=>0.0020866905, b=>-0.0360010420
Iteration 9897: Achieved Loss=> 22.8439584921
Gradients: w=>0.0020855811, b=>-0.0359819027
Iteration 9898: Achieved Loss=> 22.8439571934
Gradients: w=>0.0020844724, b=>-0.0359627736
Iteration 9899: Achieved Loss=> 22.8439558961
Gradients: w=>0.0020833642, b=>-0.0359436547
Iteration 9900: Achieved Loss=> 22.8439546002
Gradients: w=>0.0020822566, b=>-0.0359245460
Iteration 9901: Achieved Loss=> 22.8439533056
Gradients: w=>0.0020811496, b=>-0.0359054474
Iteration 9902: Achieved Loss=> 22.8439520124
Gradients: w=>0.0020800432, b=>-0.0358863589
Iteration 9903: Achieved Loss=> 22.8439507206
Gradients: w=>0.0020789374, b=>-0.0358672806
Iteration 9904: Achieved Loss=> 22.8439494302
Gradients: w=>0.0020778322, b=>-0.0358482125
Iteration 9905: Achieved Loss=> 22.8439481411
Gradients: w=>0.0020767275, b=>-0.0358291545
Iteration 9906: Achieved Loss=> 22.8439468534
Gradients: w=>0.0020756235, b=>-0.0358101066
Iteration 9907: Achieved Loss=> 22.8439455671
Gradients: w=>0.0020745200, b=>-0.0357910688
Iteration 9908: Achieved Loss=> 22.8439442821
Gradients: w=>0.0020734171, b=>-0.0357720412
Iteration 9909: Achieved Loss=> 22.8439429985
Gradients: w=>0.0020723148, b=>-0.0357530237
Iteration 9910: Achieved Loss=> 22.8439417163
Gradients: w=>0.0020712131, b=>-0.0357340163
Iteration 9911: Achieved Loss=> 22.8439404354
Gradients: w=>0.0020701120, b=>-0.0357150190
Iteration 9912: Achieved Loss=> 22.8439391559
Gradients: w=>0.0020690115, b=>-0.0356960318
Iteration 9913: Achieved Loss=> 22.8439378778
Gradients: w=>0.0020679115, b=>-0.0356770547
Iteration 9914: Achieved Loss=> 22.8439366010
Gradients: w=>0.0020668122, b=>-0.0356580877
Iteration 9915: Achieved Loss=> 22.8439353255
Gradients: w=>0.0020657134, b=>-0.0356391307
Iteration 9916: Achieved Loss=> 22.8439340515
Gradients: w=>0.0020646152, b=>-0.0356201839
Iteration 9917: Achieved Loss=> 22.8439327787
Gradients: w=>0.0020635176, b=>-0.0356012471
Iteration 9918: Achieved Loss=> 22.8439315074
Gradients: w=>0.0020624205, b=>-0.0355823204
Iteration 9919: Achieved Loss=> 22.8439302374
Gradients: w=>0.0020613241, b=>-0.0355634037
Iteration 9920: Achieved Loss=> 22.8439289687
Gradients: w=>0.0020602282, b=>-0.0355444971
Iteration 9921: Achieved Loss=> 22.8439277014
Gradients: w=>0.0020591330, b=>-0.0355256006
Iteration 9922: Achieved Loss=> 22.8439264354
Gradients: w=>0.0020580383, b=>-0.0355067141
Iteration 9923: Achieved Loss=> 22.8439251708
Gradients: w=>0.0020569441, b=>-0.0354878376
Iteration 9924: Achieved Loss=> 22.8439239075
Gradients: w=>0.0020558506, b=>-0.0354689712
Iteration 9925: Achieved Loss=> 22.8439226455
Gradients: w=>0.0020547576, b=>-0.0354501148
Iteration 9926: Achieved Loss=> 22.8439213849
Gradients: w=>0.0020536653, b=>-0.0354312684
Iteration 9927: Achieved Loss=> 22.8439201257
Gradients: w=>0.0020525735, b=>-0.0354124321
Iteration 9928: Achieved Loss=> 22.8439188678
Gradients: w=>0.0020514823, b=>-0.0353936058
Iteration 9929: Achieved Loss=> 22.8439176112
Gradients: w=>0.0020503916, b=>-0.0353747894
Iteration 9930: Achieved Loss=> 22.8439163559
Gradients: w=>0.0020493016, b=>-0.0353559831
Iteration 9931: Achieved Loss=> 22.8439151020
Gradients: w=>0.0020482121, b=>-0.0353371868
Iteration 9932: Achieved Loss=> 22.8439138495
Gradients: w=>0.0020471232, b=>-0.0353184004
Iteration 9933: Achieved Loss=> 22.8439125982
Gradients: w=>0.0020460349, b=>-0.0352996241
Iteration 9934: Achieved Loss=> 22.8439113483
Gradients: w=>0.0020449472, b=>-0.0352808577
Iteration 9935: Achieved Loss=> 22.8439100997
Gradients: w=>0.0020438600, b=>-0.0352621014
Iteration 9936: Achieved Loss=> 22.8439088524
Gradients: w=>0.0020427734, b=>-0.0352433549
Iteration 9937: Achieved Loss=> 22.8439076065
Gradients: w=>0.0020416874, b=>-0.0352246185
Iteration 9938: Achieved Loss=> 22.8439063619
Gradients: w=>0.0020406020, b=>-0.0352058920
Iteration 9939: Achieved Loss=> 22.8439051186
Gradients: w=>0.0020395172, b=>-0.0351871755
Iteration 9940: Achieved Loss=> 22.8439038766
Gradients: w=>0.0020384329, b=>-0.0351684689
Iteration 9941: Achieved Loss=> 22.8439026360
Gradients: w=>0.0020373492, b=>-0.0351497722
Iteration 9942: Achieved Loss=> 22.8439013967
Gradients: w=>0.0020362661, b=>-0.0351310855
Iteration 9943: Achieved Loss=> 22.8439001587
Gradients: w=>0.0020351836, b=>-0.0351124088
Iteration 9944: Achieved Loss=> 22.8438989220
Gradients: w=>0.0020341016, b=>-0.0350937419
Iteration 9945: Achieved Loss=> 22.8438976866
Gradients: w=>0.0020330202, b=>-0.0350750850
Iteration 9946: Achieved Loss=> 22.8438964525
Gradients: w=>0.0020319394, b=>-0.0350564380
Iteration 9947: Achieved Loss=> 22.8438952198
Gradients: w=>0.0020308591, b=>-0.0350378010
Iteration 9948: Achieved Loss=> 22.8438939883
Gradients: w=>0.0020297795, b=>-0.0350191738
Iteration 9949: Achieved Loss=> 22.8438927582
Gradients: w=>0.0020287004, b=>-0.0350005565
Iteration 9950: Achieved Loss=> 22.8438915293
Gradients: w=>0.0020276219, b=>-0.0349819492
Iteration 9951: Achieved Loss=> 22.8438903018
Gradients: w=>0.0020265439, b=>-0.0349633517
Iteration 9952: Achieved Loss=> 22.8438890756
Gradients: w=>0.0020254665, b=>-0.0349447641
Iteration 9953: Achieved Loss=> 22.8438878507
Gradients: w=>0.0020243897, b=>-0.0349261864
Iteration 9954: Achieved Loss=> 22.8438866271
Gradients: w=>0.0020233135, b=>-0.0349076185
Iteration 9955: Achieved Loss=> 22.8438854048
Gradients: w=>0.0020222379, b=>-0.0348890606
Iteration 9956: Achieved Loss=> 22.8438841838
Gradients: w=>0.0020211628, b=>-0.0348705125
Iteration 9957: Achieved Loss=> 22.8438829640
Gradients: w=>0.0020200883, b=>-0.0348519743
Iteration 9958: Achieved Loss=> 22.8438817456
Gradients: w=>0.0020190143, b=>-0.0348334459
Iteration 9959: Achieved Loss=> 22.8438805285
Gradients: w=>0.0020179409, b=>-0.0348149273
Iteration 9960: Achieved Loss=> 22.8438793127
Gradients: w=>0.0020168681, b=>-0.0347964187
Iteration 9961: Achieved Loss=> 22.8438780981
Gradients: w=>0.0020157959, b=>-0.0347779198
Iteration 9962: Achieved Loss=> 22.8438768849
Gradients: w=>0.0020147243, b=>-0.0347594308
Iteration 9963: Achieved Loss=> 22.8438756729
Gradients: w=>0.0020136532, b=>-0.0347409516
Iteration 9964: Achieved Loss=> 22.8438744623
Gradients: w=>0.0020125827, b=>-0.0347224823
Iteration 9965: Achieved Loss=> 22.8438732529
Gradients: w=>0.0020115127, b=>-0.0347040227
Iteration 9966: Achieved Loss=> 22.8438720448
Gradients: w=>0.0020104433, b=>-0.0346855730
Iteration 9967: Achieved Loss=> 22.8438708380
Gradients: w=>0.0020093745, b=>-0.0346671331
Iteration 9968: Achieved Loss=> 22.8438696325
Gradients: w=>0.0020083063, b=>-0.0346487030
Iteration 9969: Achieved Loss=> 22.8438684282
Gradients: w=>0.0020072386, b=>-0.0346302827
Iteration 9970: Achieved Loss=> 22.8438672253
Gradients: w=>0.0020061715, b=>-0.0346118722
Iteration 9971: Achieved Loss=> 22.8438660236
Gradients: w=>0.0020051049, b=>-0.0345934714
Iteration 9972: Achieved Loss=> 22.8438648232
Gradients: w=>0.0020040390, b=>-0.0345750805
Iteration 9973: Achieved Loss=> 22.8438636240
Gradients: w=>0.0020029735, b=>-0.0345566993
Iteration 9974: Achieved Loss=> 22.8438624262
Gradients: w=>0.0020019087, b=>-0.0345383279
Iteration 9975: Achieved Loss=> 22.8438612296
Gradients: w=>0.0020008444, b=>-0.0345199663
Iteration 9976: Achieved Loss=> 22.8438600343
Gradients: w=>0.0019997807, b=>-0.0345016144
Iteration 9977: Achieved Loss=> 22.8438588402
Gradients: w=>0.0019987176, b=>-0.0344832723
Iteration 9978: Achieved Loss=> 22.8438576475
Gradients: w=>0.0019976550, b=>-0.0344649399
Iteration 9979: Achieved Loss=> 22.8438564559
Gradients: w=>0.0019965930, b=>-0.0344466173
Iteration 9980: Achieved Loss=> 22.8438552657
Gradients: w=>0.0019955315, b=>-0.0344283044
Iteration 9981: Achieved Loss=> 22.8438540767
Gradients: w=>0.0019944706, b=>-0.0344100013
Iteration 9982: Achieved Loss=> 22.8438528890
Gradients: w=>0.0019934103, b=>-0.0343917078
Iteration 9983: Achieved Loss=> 22.8438517026
Gradients: w=>0.0019923506, b=>-0.0343734242
Iteration 9984: Achieved Loss=> 22.8438505174
Gradients: w=>0.0019912914, b=>-0.0343551502
Iteration 9985: Achieved Loss=> 22.8438493335
Gradients: w=>0.0019902327, b=>-0.0343368859
Iteration 9986: Achieved Loss=> 22.8438481508
Gradients: w=>0.0019891747, b=>-0.0343186314
Iteration 9987: Achieved Loss=> 22.8438469694
Gradients: w=>0.0019881172, b=>-0.0343003866
Iteration 9988: Achieved Loss=> 22.8438457892
Gradients: w=>0.0019870602, b=>-0.0342821514
Iteration 9989: Achieved Loss=> 22.8438446103
Gradients: w=>0.0019860038, b=>-0.0342639260
Iteration 9990: Achieved Loss=> 22.8438434327
Gradients: w=>0.0019849480, b=>-0.0342457102
Iteration 9991: Achieved Loss=> 22.8438422563
Gradients: w=>0.0019838928, b=>-0.0342275041
Iteration 9992: Achieved Loss=> 22.8438410811
Gradients: w=>0.0019828381, b=>-0.0342093078
Iteration 9993: Achieved Loss=> 22.8438399072
Gradients: w=>0.0019817839, b=>-0.0341911210
Iteration 9994: Achieved Loss=> 22.8438387346
Gradients: w=>0.0019807303, b=>-0.0341729440
Iteration 9995: Achieved Loss=> 22.8438375632
Gradients: w=>0.0019796773, b=>-0.0341547766
Iteration 9996: Achieved Loss=> 22.8438363930
Gradients: w=>0.0019786249, b=>-0.0341366189
Iteration 9997: Achieved Loss=> 22.8438352241
Gradients: w=>0.0019775730, b=>-0.0341184708
Iteration 9998: Achieved Loss=> 22.8438340565
Gradients: w=>0.0019765216, b=>-0.0341003324
Iteration 9999: Achieved Loss=> 22.8438328900
Gradients: w=>0.0019754709, b=>-0.0340822036
Parameters are w=>1.0848258524 and b=>13.1085071601
Prediction: x= 20 y=> 34.8050242091
